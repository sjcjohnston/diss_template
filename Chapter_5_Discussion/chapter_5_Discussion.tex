% %set_parent(â€˜/Users/mwilli/Documents/Spring_2017/Dissertation_Document/Dissertation_Working_Directory_Draft/Dissertation_Main.Rnw')
% 
% <<chunk_options, echo=FALSE>>=
% # This is where we set basic knitr options.
% opts_chunk$set(echo=TRUE, message=FALSE, warning = FALSE, cache = FALSE, comment="")
% options(width=75) # This sets how wide the R printout can be.
% @
% 
%  <<setup-child, include=FALSE>>=
%  set_parent('/home/sam/Dissertation/Dissertation_Template/Dissertation_Main_Template/Dissertation_Main.Rnw')
% @
% 
% <<load_libraries, echo=FALSE>>=
% library(tidyr)
% library(dplyr)
% library(ggplot2)
% library(lme4)
% library(lsmeans)
% library(car)
% library(pbkrtest)
% library(xtable)
% library(cowplot)
% library(plyr)
% @

\chapter{Overall Discussion\label{chapter5}}


The primary goal of this project is to record a small corpus of ear-recorded speech, and test the ability of humans and ASR systems to accurately recognize the speech content.  More specifically, the project aims to determine a) if ear-recorded speech, using an earplug and ear muffs, is able to significantly filter out background noise, b) if ear-recorded speech contains enough information to be intelligible, c) if human listeners find ear-recorded speech more intelligible than the exact same signal recorded at the mouth in a noisy environment, and d) if an ASR system finds ear-recorded speech more intelligible than speech recorded at the mouth in a noisy environment.

The following three sections will briefly review the results of the three experiments discussed in this report: a) the data collection experiment which gathered the ear-recorded speech, b) the human speech perception experiment testing ear-recorded and noisy mouth-recorded speech, and c) the ASR experiement testing the ability of an ASR system to recognize ear-recorded speech compared with noisy mouth-recorded speech.


\section{Review of Ear-Recorded Speech}\label{sec:chap2-review}

Ear-recorded speech was collected and analyzed from 20 participants.  The microphone was placed into a silicone earplug and into the participants' right ear (facing the tympanic membrane), and noise reduction earmuff were placed over top of their ears.  A rod extending out from the side of the earmuffs was used to place a microphone recording the speech as it came from their mouth.  A loudspeaker was placed off to the side, which emitted background noise.  The participants read aloud stimulus sentences, and were recorded, while various kinds of background noises (bus, caf\'{e}, pedestrian area, street, factory) were produced by the loudspeaker.  For more details concerning the experimental methods, refer to Chapter 2\ref{chapter2}, Section \ref{expt1}.

These recordings were then observed.  Overall, the ear-recorded speech was highly lowpass filtered, and, subjectively, it had a very `muffled' quality, which can be visualized with the spectra and spectrograms in Figs. \ref{} and \ref{}, each from a different utterance spoken by a different participant.  To test the similarity between the mouth-recorded signals and their ear-recorded counterparts, the maximum value of the cross-correlation matrix was obtained and normalized \textbf{(ie. to obtain values between 0 and 1, where 1 indicates identical signals)}, and this value was averaged over all clean-speech utterances by all speakers.  Only utterances with no background noise were included in this calculation, in order to only test the similarity of the speech and not the noise.  This test yielded a maximum normalized cross-correlation value of 0.358 as an average across all included utterances.

To increase the amplitude of the speech information in the higher frequencies, the author pre-emphasized the signal (Fig. \ref{x}).  As there was no speech information found above approximately 2.7 kHz, and since the pre-emphasis resulted in the presence of white noise in the higher frequencies, the signal was then low-pass filtered at 2500 Hz with a slope of 500 Hz (Fig. \ref{x}).  The lower frequencies in this signal were still much more prominent than the upper frequencies, and so pre-emphasis was applied a second time (Fig. \ref{}).  The average maximum normalized cross-correlation metric was obtained using the newly transformed ear-recorded speech, and each signal was compared to its simultaneously mouth-recorded counterpart.  As before, only clean speech, with no background noise, was included in this comparison.  This gave a nearly doubled cross-correlation value of 0.674.

The spectra and spectrograms for the sentence ``He said the same phrase thirty times'', spoken by a male participant, are provided for better visualization of speech intelligibility and noise reduction.  A comparison between speech recorded at the mouth and at the ear, with transformations, and with no background noise can be seen in Figures \ref{x} and \ref{y}); the same utterance is displayed, spoken at the mouth with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{} and \ref{}) and at the ear (after transformations) with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{}, and \ref{}).  Each mouth- and ear-recorded pair is the exact same utterance; they were recorded simultaneously (e.g. the already mentioned Figures \ref{x} at the ear and \ref{y} at the mouth).

The figures containing noisy speech recorded at the mouth (ie. Figs. \ref{}, \ref{}, and \ref{}) demonstrate one of the limitations of the collected data - that the SNR of the signal recorded by the mouth microphone was very high for all noise categories.  The noise did not drown out the speech in the high noise (80 dB) condition as originally intended.  The SNR needed to be high to compare the recognition performance of humans and computers on the ear-recorded speech, compared with the noisy mouth speech.  If the noise wasn't sufficiently high enough to interfere with the recognition of speech, then the mouth-recorded speech would be too easy to understand, and an adequate comparison between ear-recorded speech and noisy mouth-recorded speech could not be made.

Despite the high SNRs in the mouth-recorded signals, noise is still present.  When, by comparison, one looks at the ear-recorded signals that were also recorded in the noisy environment (ie. Figs. \ref{}, \ref{}, and \ref{}), the passive noise reduction that is provided by the head, earplug, and earmuffs appears to have successfully eliminated a substantial proportion of the noise from the signal.

These recordings demonstrate that using the ear canal as a location of recording speech in noise can offer a large reduction in the noise that enters the signal.  Furthermore, ear-recorded signals can provide speech comparable to mouth-recorded speech up to approximately 2.7 kHz.  This is relatively close to the 3.5 kHz cutoff generally used for telephonic communication, which does provide intelligible speech for both humans and computers. The author hypothesized that speech recorded from the ear and cut off at 2.7 kHz would also be intelligible for humans and for computers.  The following sections will review the results of the experiments in Chapters 3\ref{chapter3} and 4\ref{chapter4}, which aimed to test this hypothesis.


\section{Review of Human Perception of Ear-Recorded Speech}\label{sec:chap3-review}

Due to the high SNR of the speech recorded in the data collection experiment, described in Chapter 2\ref{chapter2} and Section \ref{sec:chap2-review} above, two additional speakers were recorded in the same manner, ie. with a microphone placed in an earplug and in their ear canal, with earmuffs placed over their ears.  Another directional microphone was placed in front of their mouth, except instead of pointing at their mouth, the microphone pointed at the loudspeaker in the room, which produced the background noise. The ear-recorded speech was pre-emphasized and lowpass filtered in the same manner as the recordings conducted in Chapter 2\ref{chapter2}. For further details about the collection of these additional recordings, refer to Chapter 3\ref{chapter3}, Section \ref{expt2}.

The speech recorded from these two speakers was used as stimuli for the human perception experiment.  There were 24 participants, all native speakers of English, who listened to the provided stimuli.  There were 24 conditions, gender of the speaker of the stimulus (male,female), recording location of the stimulus (in front of mouth, in ear canal) and noise type (clean (no noise), bus, caf\'{e}, pedestrian area, street, factory).  The stimuli were placed in counter-balanced groups, where each sentence stimulus occurred in each of the 24 conditions.  Each of the 24 participants recieved one of the 24 counter-balanced stimulus groups, meaning that each distict sentence-condition pairing was only heard by one participant listener.
%Each participant listener heard each stimulus utterance once, and heard three utterances per condition.  This yielded 72 stimuli (72 distinct utterances), with each of the 24 conditions repeated three times.  Each listener could only hear a given utterance once before they would be given an advantage to understanding it.  To avoid this, counter-balanced lists were developed to place each stimulus in each of the 24 conditions, requiring

The participant listeners would hear an utterance, and type what they heard into a response box.  They were instructed that what they heard may not be a complete sentence, and to only type what they thought they heard.  These responses were recorded for each participant, and WER was calculated between the actual utterance transcription and the proposed transcription provided by the participant listeners.

A 3-way ANOVA was conducted on the data, with dependent variable WER, and factors included ``speaker [of the stimulus] gender'' (female, male), ``recording [mic] location'' (mouth, ear), and ``noise type'' (clean, bus, caf\'{e}, pedestrian, street, factory).  

\textbf{[Further discussion will be included when stats are fixed.]}



\section{Review of Automatic Speech Recognition of Ear Recorded Speech}\label{sec:chap4-review}

The author used the Kaldi ASR system (\cite{povey:11}) and the acoustic and language models trained on the data from the LibriSpeech corpus (\cite{panayotov:15}) to test the ability of a standard ASR system to recognize both ear-recorded speech and noisy mouth-recorded speech.  For testing data, the ear- and mouth-recorded speech was used which was obtained in the data collection experiment - presented in Chapter 2\ref{chapter2} and reviewed in Section \ref{sec:chap2-review}.  The ear-recorded speech was transformed with pre-emphasis, lowpass filtering, and a second application of pre-emphasis (described in Chapter 2\ref{chapter2} and in Section \ref{sec:chap2-review}). For more details about these methods, refer to Chapter 4\ref{chapter4}, Section \ref{expt3}

The clean speech from the ear and the mouth can be compared in Table \ref{tab:clean-wers}, and a comparison of the transformed ear-recorded speech and the noisy mouth-recorded speech can be seen in Table \ref{tab:disc-all-wers}.  These tables demonstrate that, in all but a few noisy conditions, an ASR system will more easily recognize noisy speech than it will ear-recorded speech.  

\begin{table}[h]
\begin{center}
\begin{tabular}{| c | c |} \hline
 Mouth-Recorded, Clean & Ear-Recorded, Clean \\ \hline\hline
 9.38 & ?? \\ \hline
\end{tabular}
\end{center}
\caption{A comparison of ASR performance on mouth-recorded speech and ear-recorded speech with no background noise. These values are obtained from the highest-performing (4-gram) language model, utilizing the 960-hour LibriSpeech DNN acoustic model - both trained using the LibriSpeech corpus.  All values are give as WER.}\label{tab:clean-wers}
\end{table}


\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
      & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
      & Mth & Ear & Mth & Ear & Mth & Ear & Mth & Ear & Mth & Ear \\ \hline\hline
60 dB & 21.5 & ?? & 20.3 & ?? & 18.6 & ?? & 19.0 & ?? & 17.9 & ??  \\ \hline
70 dB & 41.7 & ?? & 32.9 & ?? & 32.0 & ?? & 36.4 & ?? & 29.9 & ??  \\ \hline
80 dB & 88.2 & ?? & 73.3 & ?? & 75.6 & ?? & 85.0 & ?? & 71.4 & ??  \\ \hline
\end{tabular}
\end{center}
\caption{These values are obtained from the highest-performing (4-gram) language model, utilizing the 960-hour LibriSpeech DNN acoustic model - both trained using the LibriSpeech corpus.  Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type), with each subcolumn comparing the performance on mouth-recorded speech in that condition with ear-recorded speech in that condition.  All values are given as WER.}\label{tab:disc-all-wers}
\end{table}

In an attempt to improve the recognition of ear-recorded speech, two additional tests were performed, mimicking the additional investigations performed in Chapter 3\ref{chapter3} and in Section \ref{sec:chap3-review} above.  The first combined the pre-emphasized, ear-recorded speech, which is lowpass filtered to 2.5 kHz with a 500 Hz slope, with the simultaneous speech recorded at the mouth, which was bandpass filtered from 3.0 kHz to 8.0 kHz, with a 500 Hz slope in either direction\footnote{The mouth-recorded speech was already lowpass filtered to 8.0 kHz.}.  These ASR performance on these recordings are displayed in Table \ref{tab:e-m-combo}, with `E/M' referring to the ear-and-mouth combination speech.  The ASR performance on the regular, noisy mouth-recorded speech is repeated from Table \ref{tab:disc-all-wers} above for ease of comparison.

\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
      & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
      & Mth & E/M & Mth & E/M & Mth & E/M & Mth & E/M & Mth & E/M \\ \hline\hline
60 dB & 21.5 & ?? & 20.3 & ?? & 18.6 & ?? & 19.0 & ?? & 17.9 & ??  \\ \hline
70 dB & 41.7 & ?? & 32.9 & ?? & 32.0 & ?? & 36.4 & ?? & 29.9 & ??  \\ \hline
80 dB & 88.2 & ?? & 73.3 & ?? & 75.6 & ?? & 85.0 & ?? & 71.4 & ??  \\ \hline
\end{tabular}
\end{center}
\caption{These values are obtained from the highest-performing (4-gram) language model, utilizing the 960-hour LibriSpeech DNN acoustic model - both trained using the LibriSpeech corpus.  Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type), with each subcolumn comparing the performance on mouth-recorded speech in that condition with the combination of lowpass ear-recorded speech and `higher' bandpassed mouth-recorded speech in the same condition.  All values are given as WER.}\label{tab:disc-all-wers}
\end{table}

\textbf{Further discussion will be included when results are obtained.}

A common method of improving ASR performance on speech with different characteristics (e.g. in noise, with different dialects, etc.) is to train the ASR acoustic model on speech in that domain.  This also mimics the use of additional training for the human speech perception task described in Chapter 3\ref{chapter3} and Section \ref{sec:chap3-review} above.  Rather than recording enough ear-recorded speech to train a model, the 100-hour dataset from the LibriSpeech corpus was used.  The speech in this dataset was lowpassed with the exact same filter used on the ear-recorded speech - 2.5 kHz with a 500 Hz slope.  A speaker-adapted GMM-HMM model was trained on this data.  Due to resource constraints, computationally and otherwise, only the 100-hour dataset was used (not 960 hour), and a DNN model was not trained.  These results are displayed in Table \ref{tab:retrainedGMM}.

% \begin{table}[h]
% \begin{center}
% \begin{tabular}
% 
% \end{tabular}
% \end{center}
% \caption{}\label{tab:retrainedGMM}
% \end{table}

\textbf{Further discussion will be included when results are obtained.}

Note that the model used for the ear-recorded speech in Table \ref{tab:retrainedGMM} was trained on significantly less data than the model used for other tests discussed above.  If additional training data were used and if a DNN (rather than GMM) model were trained, results would be expected to improve.


\section{Conclusions}



\section{Future Direction}

One of the primary limitations discussed throughout this report is that the SNR for the speech recorded in the original data collection experiment (cf. Chapter 2\ref{chapter2}) was much higher than originally intended or hoped for, by slightly more than +20 dB SNR for each condition.  This yielded noisy speech which was largely greater than +10 dB SNR; current ASR technology is rather adept at recognizing signals with these levels of noise (cf. \cite{braun:16}).  Due to this, it was difficult to see how much improvement could be gained over very noisy, low-SNR signals.  Two possibilities stand out as potentially workable solutions.  The first, would be to simply increase the ambient background noise level, and the second involves using an omnidirectional microphone to record the speech from the mouth (rather than a directional microphone).  

The former introduces potential risk to those exposed to the noise, depending on the level of noise and the amount of hearing protection offered.  Additionally, for very high noise conditions, it would be difficult to limit the noise exposure to the surrounding area.  The latter solution allows the speaker to talk normally and for the ambient noise level to remain relatively low.  However, this does not necessarily result in a fair comparison between ear-recorded and mouth-recorded speech, as in any real-life application, a directional microphone would be utilized with the specific intention of reducing the noise picked up by the microphone.  If adequate hearing protection is offered, and the noise contained, the former solution offers the most realistic comparison between the two microphone locations.

Regarding human speech perception, ear-recorded speech seems promising.  There is a significant difference between the ability of human listeners to recognize ear-recorded speech and mouth-recorded speech in noise.  Ear-recorded speech does not provide the same recognition performance as speech recorded at the mouth without background noise.  However, two methods were proposed to further improve the performance of ear-recorded speech.  The first introduced a training session prior to the actual experiment in which participants listened to the narration of a short story, recorded from the ear canal.  The second spliced the low-frequency ear-recorded speech with the higher frequencies of the mouth-recorded speech.  Neither of these tested methods were performed with enough participants (due to resource limitations) to run adequate statistics (due to requiring counter-balanced groups).  

The non-statistical, preliminary results for the former (training) method showed little or no improvement over the standard ear-recorded speech, while the non-statistical, preliminary results for the latter (spliced) method provided indication that this could yield a noticeable improvement.  Future research should test these methods statistically.  The former (training) method may need altered methodology in order to determine whether a form of training could result in an improvement in future recognition of ear-recorded speech.  A training task that requires interaction and provides feedback may promote active attention during the training task, unlike passively listening to a story as used in this study.  In the method using spliced stimuli, further research should pursue the possibility of using the clean, lower frequencies to actively clean any noise from the higher mouth-recorded frequencies.  

The ASR recognition task initially indicated that ear-recorded speech offered essentially no performance improvement over noisy mouth-recorded speech. \textbf{add more details when available.}

Additional research should also pursue other methods of improving the human or computer recognition of ear-recorded speech, especially those which would not require a secondary microphone at the mouth. \textbf{say something else.}



