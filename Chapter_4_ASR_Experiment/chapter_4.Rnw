% %\Sexpr{set_parent(â€˜/Users/mwilli/Documents/Spring_2017/Dissertation_Document/Dissertation_Working_Directory_Draft/Dissertation_Main.Rnw')}
% 
% <<chunk_options, echo=FALSE>>=
% # This is where we set basic knitr options.
% opts_chunk$set(echo=TRUE, message=FALSE, warning = FALSE, cache = FALSE, comment="")
% options(width=75) # This sets how wide the R printout can be.
% @
% 
%  <<setup-child, include=FALSE>>=
%  set_parent('/home/sam/Dissertation/Dissertation_Template/Dissertation_Main_Template/Dissertation_Main.Rnw')
% @
% 
% <<load_libraries, echo=FALSE>>=
% library(tidyr)
% library(dplyr)
% library(ggplot2)
% library(lme4)
% library(lsmeans)
% library(car)
% library(pbkrtest)
% library(xtable)
% library(cowplot)
% library(plyr)
% @

\chapter{Automatic Speech Recognition of Ear-Recorded Speech\label{chapter4}}


\section{Introduction}\label{chap4:introduction}

The automatic recognition of human speech by a computer has been a subject of interest spanning decades.  Humans first and foremost communicate their ideas via speech and human language, and training computers to take verbal instructions would make interaction more straightforward for a majority of the population, particularly the elderly and disabled. This task has been a subject of much study for over half a century, and is only recently gaining significant success.  It is important to briefly discuss the architecture of an ASR system, and highlight some successes and the challenges that still remain.
% Despite these successes, challenges still remain when there is noise in the signal (\cite{zhang:17}).   
%As before, it is important to understand the mechanics and acoustics of why this proves to be a challenge for automatic speech recognition (ASR), and traditional methods of dealing with this as well as more modern techniques.

The present study proposes a new technique to be used in the advancement of noise-robust automatic speech recognition (ASR).
The experiment in Chapter \ref{chapter2} collected ear-recorded speech data, which aimed to overcome the difficulty of accurately perceiving speech in a noisy environment, for recognition both by computer (ASR) or by human speech perception.  This data will be used in an experiment utilizing the standard open-source ASR system Kaldi (\cite{povey:11}) with a standard, freely-available acoustic model developed from the LibriSpeech corpus (\cite{panayotov:15}).

% It is important to first understand the mechanics behind automatic speech recognition in order to understand the difficulties posed by noise in the signal. The background will take the form of a high-level walk-though of ASR architecture.  A very basic visualization of how signal processing quantifies speech in noise will be given first, followed by the ASR walk-through. Solutions proposed to address issues arising from speech in noise will be woven into this description.

\section{Background}
\label{chap4:background}

% Things to hit on:
% - Base-level Acoustics-to-Features
% - Traditional general ASR mechanics?
% - Current methods of ASR (Cutajar + more recent)
% - Outline problem of noise in the signal
% - Traditional methods of filtering noise out of the signal
% - Previous methods of ASR in noise
% -- Issues with these traditional methods (of removing noise and ASR in noise)
% - Current methods of ASR in noise (Li 2014, and more current)
% -- Multiple Microphones and Beamforming
% -- Other methods??  Look at current CHiME challenge papers
% - Try to find continued issue with electronically identifying speech in noise
% - Emphasize unpredictability of noise, and unknown amplitude - how ear-recorded speech may be able to `protect' against this unpredicatability

% The basics of the acoustics of speech was discussed at the beginning of Chapter \ref{chapter2}.  As a recap, speech contains voiced and voiceless sounds.  The voiceless sounds are generally produced by turbulence - air moving rapidly through a small openning in the vocal tract.  The voiced sounds are more complex, and contain harmonics (acoustic energy focused in very narrow bands of frequency).  Certain harmonics (out of the full set of harmonics in the voiced signal) contain more energy than the other harmonics; these regions in the frequency spectrum with harmonics containing greater energy are called formants, and this is where much of the speech information comes from.
% 
% The human auditory system does a remarkable job of finding these acoustic features and interpreting them, but a computer does not have an inherent auditory system and (as of now) needs to be told what to look for.  Simply put, via a microphone, computers receive a series of digits (numbers) which correspond to the amount of pressure at a given point in time.  
% %
% \begin{wrapfigure}{L}{0.5\textwidth}
% \centering
% \includegraphics[width=0.45\textwidth]{figure/single-channel-animals.png}
%   \caption{A waveform graph representing the high and low pressure fluctuations that comprise sound.}
%   \label{fig:waveform}
% \end{wrapfigure}

While there are more than 50 years of research involving ASR, and nearly as many working with speech in noisy environments, it is impossible to mention all or even most techniques within this very brief overview.  Therefore, %the discussion will highlight only several `noise-robustness' methods that are used at each level of processing along the ASR pipeline. 
 only several areas of research in noise-robust ASR that pertain to the present study will be discussed.

Equation \ref{eq:basic} is often used to represent the combination of speech and noise,
\begin{equation}\label{eq:basic}
y(t) = x(t) \cdot h(t) + n(t)
\end{equation}
where $x(t)$ is the clean speech signal at time $t$, $h$ is an impulse response to the source ($x(t)$), such as room reverberation, microphone channel warp, etc. ($h$ is convolved with $x$ via `$\cdot$', the convolution operator), $n(t)$ is additive noise, and $y(t)$ is the noisy speech signal, both at time $t$.  In addition, the phase of the component signals contributing to the resulting signal $y$ also has an effect.  Taking phase into account means that additive noise is not always simply additive, as energy at frequencies with competing phases could cancel each other out to varying degrees, depending on phase (cf. Chapter \ref{chapter1}).  

Generally in speech perception research, phase is ignored as it contains no speech information; it does become important, however, when multiple sound sources begin to interact substantially and one attempts to tease them apart.  While there is some research that does take phase into account and demonstrates that by doing so, more accurate noise removal can be achieved (eg. \cite{deng:04,leutnant:09}), the improvements are countered by the significant complexity involved with attempting to account for phase, and the problematic (many researchers admit) assumption is frequently made in ASR that phase plays no role and is consequently ignored (\cite{li:14}).  

% \section{Mechanics of ASR}
% 
% \subsection{The Incoming wave}
% 
% For illustration, the word $W$ is spoken and sent to an ASR recognizer.  This, acoustically in the temporal domain, takes the form seen in the waveform graph if Figure \ref{fig1}.
% 
% \begin{figure}[h!]
% INSERT WORD WAVEFORM - fig1
% \end{figure}
% 
% When encoded digitally for the ASR system, it takes the form of a series of numbers, each number corresponding to normalized pressure value along the waveform (cf. Fig. \ref{fig2}).  These occur in a vectorized format (cf. Fig. \ref{fig3}).  `Sample' is the term that will be used to referred to an individual numeric value in the `sample vector' seen in Figure \ref{fig3}.  Therefore, the `sample rate' is the number of samples in a given length of time (generally normalized to 1 second).  For example, for ASR research, a common sample rate is 16 kHz, ie. 16,000 samples are used to represent a 1-second portion of a waveform.
% 
% \begin{figure}[h!](A)(B)
% INSERT Waveform w/ number points - fig2
% \end{figure}
% 
% \begin{figure}[h!](A)(B)
% INSERT vector w/ numbers (same numbers as in plot above) - fig3
% \end{figure}
% 
% A prespecified `window' size will divide the sample vector into `chunks', or `windows'.  Typically, this window is given in terms of milliseconds, since the number of samples in a specified length of time varies depending on the sample rate.  Typical window sizes range in width from \textbf{X} to \textbf{Y} milliseconds.  The first window will start at the first sample of the speech signal.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  OLD LIT REVIEW  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Multi-style training, what it is (training with multiple types of noise) - it is ineffective.  Older - paper cited was written in 1987. \cite{li:14}, \cite{lippmann:87}.

%The categorization of noise-robustness techniques utilized by hundreds of researchers over the years is difficult, but, broadly, two domains can be outlined (\cite{li:14,zhang:17}).  
ASR architecture can be categorized into two primary domains. The first is the `feature-space' domain, which focuses on front-end processing of the signal $y$ itself.  The second group utilizes the `model-space' domain, or the back-end processing that contains probability distributions and makes predictions about a signal. %modifies the acoustic model to account for any noise in the signal $y$.    
Noise can be accounted for in either the feature space domain or the model space domain, or occasionally, both.  %In the feature domain, noise is dealt with, and the signal is enhanced, prior to sending the features to the acoustic model for recognition.  These modifications are made without altering the acoustic model parameters, resulting in low computation cost.  In the model space, acoustic parameters themselves can be modified in accordance with the noisy signal.  This generally results in high computation cost when training the acoustic model.  There is normally a trade-off between the two domains: one of computation efficiency and performance. Model space alterations often yielding higher performance improvement, while feature space alterations are less computationally intensive (\cite{li:14}).
%Advances in neural network technology have also lent itself to application in noise-robust ASR.  These have provided additional methods of tackling the problem.

\subsection{Feature Space Domain}

`Feature space' in ASR is where an acoustic signal is transformed into `features' describing salient parts the acoustic signal that the ASR model will receive as input. The typical features used in ASR applications are Mel-Frequency Cepstral Coefficients (MFCCs) and Perceptual Linear Prediction (PLP) coefficients. The process to calculate these features involves a number of steps, which are described below.  In the feature domain, noise is dealt with, and the signal is enhanced, during or directly after feature extraction and prior to sending the features to the acoustic model for recognition.  Many of these noise-reduction methods are employed after feature extraction via MFCC or PLP calculation.

\subsection{MFCC Calculation}\label{sec:mfcc-calc}

First, a very brief window of the acoustic signal is taken; this is typically 25 milliseconds in length, but can range from 20-40 ms (\cite{vergin:99,molau:01}); each windowed portion of the signal is called a frame.  The frame is then passed through some variety of a Fourier Transform (most often a Fast Fourier Transform (FFT)) that converts the signal into its spectral components in the frequency domain.  The power spectrum, sometimes referred to as the energy spectrum, is calculated from the frequency spectrum by\footnote{Equations in this section are derived from \cite{vergin:99}}: \begin{equation}\label{eq:power-spectrum} P_f = |S(f)|^2 \end{equation} where $S$ is the frequency spectrum, $f$ is the frequency component, and $P$ is the power spectrum.

A series of Mel-frequency filters are then applied to the power spectrum.  The frequency spectrum is generally shown on a linear scale, with Hertz being the unit of measurement.  Hertz is the absolute measure of the number of repetitions of a component wave per second.  The Mel-frequency scale, by contrast, is logarithmically based.  This means low frequency information is represented in greater detail (ie. narrower-bandwidth filters are used on the lower frequencies), while the higher frequencies are represented in less detail (ie. wider-bandwidth filters are used).  One purpose of a logarithmic scale is to better model the human auditory system, which perceives frequencies on a logarithmic basis, ie. humans perceive the lower frequency range in more detail than the higher frequency ranges (\cite{rosen:91}).

The Mel-frequency filterbanks, as already described, are heavily concentrated in the lower [linear] frequencies due to the logarithmic nature of the scale, and are spread out in the higher frequencies (cf. Fig. \ref{fig:filt-mfcc}). The filters themselves take the shape of a triangle.  The number of filter banks applied to the power spectrum can vary from 20-40 (\cite{honig:05,gold:11}) depending on the methodology. 

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{figure/filt-mfcc.png}
\caption{Demonstration of triangular Mel-frequency logarithmic filters. Scale is not exact.}\label{fig:filt-mfcc}
\end{figure}

Afterwards, the log of each filter output is taken, and a discrete cosine transform (DCT) is applied: \begin{equation}\label{eq:mfcc-calculation} c_m = \beta_c \sum_{j=1}^{J-1} \cos(m\dfrac{\pi}{J} (j+0.5)) \log_{10} (P_j) \end{equation} where $J$ is the number of Mel filterbanks used, $P_j$ is the power spectrum of filter band $j$, and $m=[1,2,3...,M]$ is the number of the Mel-frequency cepstral coefficient $c$.  $\beta_c$ is an amplification factor.  Generally for ASR, only the first 13 cepstral coefficients are used, ie. $M=12$ (\cite{etsi:02}); this also has the effect of `smoothing' the cepstral representation of the spectrum, reducing the effect of noise (\cite{gold:11}).  These 13 cepstral coefficients comprise the MFCC vector $c$.  While sometimes these 13 MFCC components compose the acoustic vector alone,  the frame's `energy' component is often added to the end of this vector.  With energy vector contains the 14 `raw' feature components (\cite{etsi:02}).  

The frame, or acoustic window, is then shifted temporally along the signal in the time domain, generally by 10 ms, to obtain the next frame. This temporal shift is called the `step', and the duration of the shift (eg. 10ms) is called the `step size'.  The acoustic vector is then calculated for this - and subsequent - frames in the speech signal.

While the traditional acoustic vector captures speech information from a single frame, it is also useful to explicitly model how these features change from frame to frame.  An additional set of 13 cepstral features\footnote{Only the 13 cepstral features will be assumed, but this can be a vector of length 14 if `energy' is used.} can be calculated by\footnote{Delta and Delta-Delta equations are derived from \cite{htk:15}.} \begin{equation} d_t = \dfrac{\sum_{n=1}^{N} n(c_{t+n} - c_{t+n})}{2\sum_{n=1}^{N} n^2} \end{equation} where $c_t$ is the cepstal acoustic vector at time $t$, $n$ is the number of acoustic vectors in either direction (from $t$) that will be included in the calculation, and $d_t$ is the delta vector at time $t$.  

If delta vectors encode the change in cepstral and energy values, delta-delta vectors encode the change in delta vectors.  These are computed in the same way, though delta vectors replace the cepstral vectors in the calculation, such that \begin{equation} dd_t = \dfrac{\sum_{n=1}^{N} n(d_{t+n} - d_{t+n})}{2\sum_{n=1}^{N} n^2} \end{equation} where $dd_t$ is the delta-delta vector at time $t$.  $N$ can change between delta and delta-delta calculations. Both delta and delta-delta vectors have the same length as the cepstral vector, 13, so that when combined, they yield an acoustic feature vector with 39 features.  %This calculation hierarchy can be visualized in Figure \ref{fig:mfcc-hier}.  
Delta and delta-delta computation is quite standard procedure for modern ASR systems (\cite{htk:15}).


\subsection{Perceptual Linear Prediction}\label{sec:plp}

An alternative to MFCC feature vectors that generally results in more noise-robust features is perceptual linear predictive cepstral coefficients (PLP; \cite{hermansky:85}).  The first few steps in PLP calculation are the same as in MFCC calculation.  A window is used to extract a frame of audio (generally between 20-40 ms), a FFT is computed and the power spectrum (cf. Equation \ref{eq:power-spectrum}) is computed from that.  PLP obtains its `perceptual' moniker by implementing the following: critical band filtering, `equal-loudness' pre-emphasis, and the conversion from intensity to loudness.  

Unlike the triangular filters used in the Mel-filtering for MFCC calculation, PLP makes use of `critical band' filters (cf. Fig \ref{fig:filt-cb}).  These trapezoidal filters are intended to incorporate a loose implementation of `critical bands'\footnote{Critical bands in the cochlea do not behave like a computational filterbank.}. Physiologically, these `critical bands' are regions along the basilar membrane in the cochlea which are activated by a particular a range of frequencies (\cite{fletcher:40}).  These critical bands are logarithmic in that the critical bandwidth of lower-frequency regions is narrower (ie. higher resolution of lower frequencies) and is wider in higher-frequency receptor regions of the basilar membrane.  In application, the trapezoidal filters mimic the logarithmic bandwidth aspect of cochlear critical bands; for PLP these utilize the Bark-frequency scale (also logarithmic, similar to the Mel-frequency scale).

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{figure/filt-cb.png}
\caption{Demonstration of trapezoidal critical band logarithmic filters. Scale is not exact.}\label{fig:filt-cb}
\end{figure}

After the critical band filterbank is constructed, `equal-loudness' pre-emphasis is applied.  This is very similar to conventional pre-emphasis (increasing the amplitude of higher frequencies), but follows the frequency-dependent human perceptual sensitivity of loudness; it does not emphasize the very high frequencies as much as the mid-range frequencies. % (cf. Fig. \ref{?}, adapted from \cite{honig:05}).  
The explicit conversion from intensity to loudness is then performed on these pre-emphasized, critical band filterbanks by effectively compressing the signal by taking the cubic root of the values in the filterbanks.

Following these perceptually-based transforms, an inverse Discrete Fourier Transform is performed to return the signal to the time domain (from the power domain), and a linear predictive (LP) filter (such as the Levinson Durbin algorithm) is applied to the time-domain signal (\cite{gold:11}).  This has the effect of smoothing out the variance in the spectral envelope (if it were to be reconverted to the spectral domain).  During this process, autoregressive weights (coefficients) are trained on the time-domain signal which are used in the LP filter.  Using a recursive formula these LP coefficients are converted into cepstral coefficients - the same unit used in MFCCs.  It is the LP component and the smoothing of the signal and the spectral envelope that produces much of the specific benefit offered by PLP when recognizing speech in noise.  While the perceptual-based transformations offer recognition benefit, some have found that Mel-frequency filter banks can perform as well or better than Bark-frequency critical band filterbanks (cf. \cite{honig:05}).

MFCC and PLP are the two primary methods used to extract cepstral coefficients that are most widely used in ASR architecture.  On top of these `feature-generation' pipelines, many methods have been proposed to further remove ambient noise from the features.

\subsection{RASTA and CMVN}\label{sec:rasta_cmvn}

RASTA (RelAtive SpecTrAl methodology) is a noise-robust technique designed specifically to fit into the PLP feature extraction algorithm (\cite{hermansky:92}).  After the critical band filterbank is applied in PLP processing, the logarithm of these values is obtained.  In log-space, a bandpass filter transfer function is traditionally applied, but a number of other filtering techniques could be used during this step.  Equal-loudness pre-emphasis and intensity-to-loudness conversion are also performed in log-space, before the exponent (reverse-log) is taken, and PLP calculation resumes as normal.

The reason for the utilization of log-space is to take advantage of its special properties in order to remove (primarily) convolutional noise (\cite{gold:11}).  Recall Equation \ref{eq:basic}, where $x$ is the source signal, $h$ is a convolutional noise, and $y$ is the resulting `noisy' signal at time $t$.  In the spectral domain, this transforms into \begin{equation} Y(f) = X(f)H(f) \end{equation} where $Y$, $X$ and $H$ are the spectral representations of $y$, $x$, and $h$ with frequency bin $f$, and the relationship between $X$ and $H$ is multiplicative.  Therefore, the conversion to the log domain yields \begin{equation} \log(Y(f)) = \log(X(f)) + \log(H(f)) \end{equation} where the relationship between $X$ and $H$ now becomes additive, and consequently much easier to account for.  The RASTA technique is most useful when signal distortion is caused by some sort of convolution, or by some sort of steady-state noise.

Another common procedure, similar to RASTA in that it primarily removes signal distortion or steady-state noise, is Cepstral Mean Normalization, or Cepstral Mean and Variance Normalization (CMN and CMVN; \cite{atal:74,viikki:98}).  To calculate the cepstral mean and variance, a window of cepstral feature vectors is required.  Because of this, there will be a brief delay of $N$ multiplied by the frame window shift in milliseconds, where $N$ is the number of frames to incorporate into the mean and variance calculations.  If $N=20$, which is the minimal acceptable value given by \cite{viikki:98} and the step size is a standard 10ms, then the delay in calculating each cepstral value would be 200ms.

The mean is calculated by\footnote{Equations adapted from \cite{viikki:98}} \begin{equation} m_t(i) = \dfrac{1}{N}\sum_{t=1}^{N} c_{t}(i) \end{equation} where $c_t$ is the cepstral vector at time $t$, $i$ is the $i^{th}$ cepstral coefficient feature in the vector, and $m_t$ is the rolling mean at time $t$.  The N frames that follow $c_t$ are used to calculate the mean cepstral vector $m_t$.  The variance is calculated as \begin{equation} \sigma_t(i) = \sqrt{\dfrac{1}{N}\sum_{t=1}^{N} [c_t^2(i)] - [m_t(i)]^2} \end{equation} where $\sigma_t(i)$ is the cepstral variance used for the cepstral coefficient $i$ at time $t-N$.  The new cepstral value is \begin{equation} \hat{c}_{t-N}(i) =  \dfrac{c_{t-N}(i) - m_t(i)}{\sigma_{t}(i)} \end{equation} where the mean is subtracted by the cepstral value, and then normalized by the variance.  Note the value $c_{t-N}$, because the means and variances computed at time $t$ are used to recalculate the cepstral values at time $t-N$.

The frame delay $N$ prevents real time recognition, as $N$ can become rather large, even to encompass the whole utterance (\cite{li:14}).  Nevertheless, CMVN is a simple calculation which can result in significant improvement of convoluted signals, or signals with steady state noise, such as an environment with a noisy car (\cite{viikki:98}).

\subsection{Spectral Subtraction and Wiener Filtering}\label{sec:spec-sub_wiener}

Spectral subtraction (\cite{boll:79}) is an intuitive method of removing noise by taking a frame, converting the signal into the spectral domain, and subtracting an estimated noisy signal. Simply, ignoring convolution, if linearly (cf. equation \ref{eq:basic}) \begin{equation} y(t) \approx x(t) + n(t) \end{equation} and spectrally \begin{equation} Y(f) \approx X(f) + N(f), \end{equation} then \begin{equation} \hat{X}(f) \approx Y(f) - \hat{N}(f). \end{equation}  On a practical level, the power spectra (equation \ref{eq:power-spectrum}) are used in calculations.  $N(f)$ is estimated by computing the mean power spectrum $\mu(f)$ of noise during non-speech portions of the signal.  This results in \begin{equation} |\hat{X}(f)|^2 \approx |Y(f)|^2 - \mu(f) \end{equation} where the power spectrum of $X(f)$ is estimated from the noisy power spectrum $Y(f)$ minus the mean of the power spectra $\mu(f)$ calculated during the portion of the signal preceding speech. $X(f)$ can either be converted back into the time domain, or the power spectra could be used for calculation of MFCC or PLP coefficients.  The process is repeated along the length of the signal.

This method still has several problems (\cite{li:14}). First and foremost, the location of speech in the signal in a noisy environment is difficult to detect.  The accurate estimation of the mean of the noise power spectra $\mu(f)$ depends on the ability to accurately detect the onset of speech and to stop the calculation of the average.  Spectral subtraction also requires relatively stationary, slow-variation noise; the use of the mean noise spectra $\mu(f)$ critically assumes that the noise during the speech signal $x$ has an approximate power spectra of $\mu(f)$.  Furthermore, this is only an average of the noise, and not exactly the noise itself.  This subtraction of $\mu(f)$ can inadvertently producing extraneous acoustic artifacts in the `clean' signal which were not there to begin with (\cite{berouti:79}).

Wiener filtering (\cite{lim:79}) is another method used to remove noise from a signal, and is, theoretically, a specific kind of spectral subtraction (\cite{agarwal:99}), though there are some differences. The Wiener filter is calculated in the frequency domain, but unlike Spectral subtraction, can be applied in either the frequency or time domain. There are many modifications to the Wiener filter, one in particular, the two-stage Mel-warped Wiener filter used in the Advanced Front End algorithm (cf. Section \ref{sec:background-afe}). % is applied in the time domain.
However, just as with spectral subtraction, Wiener filtering requires an estimate of the noise $|N(f)|^2$ in the the signal.  Furthermore, it does not do well in very low SNR environments, as it generally results in suppression and dampening of the entire signal, and not just the noise (\cite{li:14}).


\subsection{The Advanced Front End Standard}\label{sec:background-afe}

The Advanced Front End (AFE) is a standard noise-reduction algorithm established in 2002 by the European Telecommunications Standards Institute (ETSI; \cite{etsi:02}).  It is comprised of three different algorithms: two-stage Mel-warped Wiener filtering (\cite{agarwal:99}), SNR-dependent Waveform Processing (\cite{macho:01}), and blind equalization (\cite{mauuary:98}).  It yields more than 50\% improvement over standard MFCC features alone, and has become a frequent baseline for comparison in noisy ASR research.

Two-stage Mel-warped Wiener filtering is an adapted form of the Wiener filter that `Mel-warps' the filter, and utilizes two applications of the filter onto the noisy signal $y$ (\cite{etsi:02}).  The process first involves calculating an estimate of the SNR\footnote{Equations in this section are adapted from \cite{etsi:02}} \begin{equation}\label{eq:afe:mwwf:snr} SNR(f) = \dfrac{E|S(f)|^2}{E|N(f)|^2} \end{equation} where $E$ indicates an estimate of the speech power spectrum $|S(f)|^2$ or the noise power spectrum $|N(f)|^2$.  Since no actual filtering is done at this point, the speech power spectrum is first estimated with a variation of spectral subtraction ($|Y(f)|^2 - E|N(f)|^2$, roughly).  The Wiener filter $H$ is then calculated by \begin{equation}\label{eq:afe:mwwf:wf} H(f) = \dfrac{\sqrt{SNR(f)}}{1+\sqrt{SNR(f)}}. \end{equation}  A new [more accurate] estimation of the speech power spectrum is obtained by multiplying the filter $H(f)$ by the noisy signal $|Y(f)|^2$.  This new estimate $E_2|S(f)|^2$ is then used in equation \ref{eq:afe:mwwf:snr} to obtain a new SNR, to be used in equation \ref{eq:afe:mwwf:wf} which produces a modified Wiener Filter $H_2(f)$.  Most of the heavy work of the AFE ensemble is performed by this Mel-warped Wiener filtering (\cite{li:14}).

This modified Wiener filter is then Mel-warped, which modifies the frequency spectrum of the filter $H_2(f)$, so Wiener Filter coefficients are now applied to frequencies on a Mel-scale, now notated $H_{mel}(m)$.  A Mel-Inverse Discrete Fourier Transform is then performed to transform the Mel-frequency domain filter into the time domain, \begin{equation} h_{mel}(m_1) = \sum_{m_2=1}^{M} H_{mel}(m_2) \cdot IDCT_{mel}(m_2,m_1) \end{equation} where $M$ is equal to the number of Mel-frequency coefficients $m$ in $H_{mel}(m)$, and $IDCT_{mel}$ is a special Inverse Discrete Cosine Transform function, provided in more detail in \cite{etsi:02}.  The time domain Wiener Filter $h_{mel}(m)$ is weighted with a Hanning Window at each Mel-frequency bin $m$, and is then applied to the noisy signal \begin{equation} \hat{x}(t) = \sum_{m=-(M-1)/2}^{(M-1)/2} h_{mel}(m+(M-1)/2) \cdot y(t-m) \end{equation} where $y(t)$ is the noisy signal sample - and $\hat{x}(t)$ the estimated speech sample - at time $t$.  This is iterated throughout all the samples $t$ out of $T=80$ samples in a given 10ms frame for a 8.0 kHz sampled acoustic signal.  This is repeated for all frames in a signal, recalculating the Wiener Filter $H(f)$ from the noisy signal power spectrum estimate $E|N(f)|^2$ and the new speech power spectrum estimate $E|S(f)|$ for each frame.  This filtering is performed in the time domain prior to any feature extraction.

The SNR-dependent Waveform Processing is a methodology used to dampen the low-SNR portions of the signal and enhance the high-SNR portions of the signal (\cite{macho:01}), performed after Two-stage Mel-warped Wiener filtering.  First, an energy contour based off of the Teager energy contour (\cite{teager:80}) is calculated for all samples $n$ in a $N=200$-sample frame, \begin{equation} E_{cont}(n) = |\hat{x}(n)^2 - \hat{x}(n-1) \cdot \hat{x}(n+1)| \end{equation} where $\hat{x}$ is the output of the Two-stage Mel-warped Wiener Filter.  This contour is then smoothed.  

The maxima peaks of the smoothed energy contour $E_{cont}$ are identified by first locating the global maximum in the frame, and then the maxima on either side of the identified global maximum, resulting in $N_{max}$ total maxima, where $pos_{max}(n_{max})$ is the sample index in the maxima. a weighting vector $w(n)$ of length $N_{weight}=200$ (a weight for each individual sample) is created.  For each sample $n$ in $w(n)$, if $n$ falls between 
\begin{equation}\label{eq:afe:swp:window} 
\[ w(n)= \begin{cases} 1.2,& \text{if} \langle [pos_{max}(n_{max})-4], [0.8 \cdot pos_{max}(n_{max}+1)-pos_{max}(n_{max})] \rangle \\ 0.8,& \text{otherwise,}
\end{equation} where the range indicated above is 4 samples before a maxima and extending 80\% of the distance to the next maxima. 
This is to say, that regions surrounding a maxima are enhanced by a weight value of 1.2, otherwise dampened by a weight value of 0.8.  This manages to enhance the portions of the signal with high SNR, and dampens those with a low SNR.  Voiced speech consists of repetitive glottal pulses, which can be seen in Figure \ref{fig:glot-spect} as dark vertical striations or Figure \ref{fig:glot-wav} as the periodic peaks in the waveform.  Each glottal pulse would be able to be emphasized, while the non-speech information in-between the glottal pulses (along the axis of time) will be dampened.

\begin{figure}[H!]
\centering
\begin{subfigure}{0.75\textwidth}
\centering
\includegraphics[width=0.65\textwidth]{figure/glot-spect.png}
\caption{A wide-band spectrogram of a portion of /\textipa{A}/. Dark vertical striations (louder amplitude) indicate a periodic glottal pulse.}\label{fig:glot-spect}
\end{subfigure}
\begin{subfigure}{0.75\textwidth}
\centering
\includegraphics[width=0.65\textwidth]{figure/glot-wav.png}
\caption{A waveform of a portion of /\textipa{A}/.  Periodic peaks are the temporal location of the glottal pulse.}\label{fig:glot-wav}
\end{subfigure}
\caption{Spectrogram and waveform demonstrating periodic glottal pulses.}\label{fig:glot-puls}
\end{figure}

In the feature extraction pipeline, at this point, the signal has been modified only in the temporal domain.  Traditional AFE would now compute the MFCC acoustic feature vectors, largely as described above in Section \ref{sec:mfcc-calc}.  Once the raw feature vector for a frame is obtained (not including delta and delta-delta features), Blind Equalization is performed directly on the 13 cepstral coefficients.

Blind Equalization is the compensation for any convolutive channel distortion imposed on the signal due to recording (\cite{mauuary:98}).  This process is rather straightforward, in that it requires the vector of cepstral coefficients $c(m)$, the energy component $c_{energy}$ from the feature vector of the frame, a bias term $\beta$ which is initialized to $0.0$, and a reference vector of cepstral coefficients $c_{ref}(m)$ which contains coefficients for a flat spectrum (\cite{etsi:02}).  The equalized cepstral vector $c_{eq}$ is computed \begin{equation}\label{eq:afe:be:ceq} c_{eq}(m) = c(m) - \beta \end{equation}, where $\beta = 0.0$ when $m = 1$.  A weighting factor is calculated \begin{equation} w = min( 1 , max( 0 , ln(c_{energy}) - \dfrac{211}{64} ) ) \end{equation} which is used in the computation of a step size. \begin{equation}\label{eq:afe:be:step} w_{step} = 0.0087890625 \cdot w \end{equation}  Between each subsequent cepstral coefficient equalization, the bias term needs to be re-estimated as \begin{equation} \beta \mathrel{{+}{=}} w_{step} \cdot c_{eq}(m) - c_{ref}(m) \end{equation} which is then used in the next cepstral equalization calculation for $c_{eq}(m+1)$ in equation \ref{eq:afe:be:ceq}.

As this is the last step in AFE processing, the equalized raw acoustic feature vector can then be used to compute deltas and delta-deltas, and passed on to the back-end portion of an ASR system - the model domain - described in Section \ref{sec:model-domain} below.  There are hundreds of other proposed noise-reduction techniques which are too numerous to describe here; the methods above have been selected to highlight some of the most well-established front-end mechanisms for feature extraction and noise reduction.


\subsection{Model Space Domain}\label{sec:model-domain}

When the speech signal has been converted from an acoustic vector of audio samples into a set of acoustic feature vectors, these vectors are sent to the back-end model domain.  These vectors can either be used for training an acoustic model, or if an existing acoustic model is to be used, they can be used in the \textit{decoding}, or recognition, of the speech the vectors represent.  There are hundreds of different methods used to train acoustic models, but for the present, very brief overview, only the core principles of acoustic model training will be discussed in a simplified manner.  

\subsection{Training an Acoustic Model}\label{sec:acoust-mod}

Acoustic feature vectors are sent from the front-end feature extraction system to the back-end acoustic modelling mechanism.  The ultimate goal of ASR is to correctly identify the words in a speech utterance; put another way, the goal is to find the most likely utterance transcription given a speech signal.  Simplified further, the purpose is to identify the most likely speech sound, or phoneme, for each provided feature vector passed from the front-end.  This can be notated as \begin{equation} p(ph|c) \end{equation} where $c$ is the cepstral feature vector (with or without deltas and delta-deltas), and $ph$ is an abstract phoneme.  A mathematically easier question to ask is \begin{equation} p(c|M_{ph}) \end{equation} or, what is the probability of the feature vector $c$ belonging to the distribution of feature vectors defined by a phoneme model $M_{ph}$.  That is, if the phoneme model $M_{ph}$ were to contain a function that defined which features and feature combinations were representative of the phone $ph$ - and the degree to which the features were representative - it could output the probability that $c$ is a feature vector representative of phoneme $ph$.

The traditional and still widely used function that comprises the phoneme model $M_{ph}$ is a Gaussian function (\cite{gales:07}).  A Gaussian put simply, is a normal distribution (cf. Fig. \ref{fig:norm-dist}) that contains a mean $\mu$ and a variance $\sigma^2$; $M_{ph}$ can be represented by the Gaussian function \begin{equation} \mathcal{N}_{ph}(\mu,\sigma^2). \end{equation}  The mean, intuitively, specifies the most likely value, or center, of the distribution.  The variance dictates the width of the distribution; a small variance creates a narrow distribution (cf. Fig. \ref{fig:norm-narrow}) with fewer overall values (along the $x$-axis) and is `taller', whereas a larger variance yields a wide distribution with more values and is `shorter' (cf. Fig. \ref{fig:norm-wide}).  Width indicates how many distinct values fall into the distribution, and height indicates how probably a given value is.  Therefore the center value, the mean, is always the most probable.  It is also necessary that the integration of all probabilities of all possible feature values of a feature $c(m)$ in this distribution is 1. \begin{equation} \int_{f=0}^\infty p(c(m)) df = 1 \end{equation}  It is these characteristics that make a `normal' distribution.  
%
\begin{figure}[H!]
\centering
\begin{subfigure}[c!]{0.5\textwidth}
\centering
\includegraphics[width=0.5\textwidth]{figure/norm-narrow.png}
\caption{Example of a Gaussian (normal) distribution with a small variance.}\label{fig:norm-narrow}
\end{subfigure}
\qquad
\begin{subfigure}[c!]{0.5\textwidth}
\centering
\includegraphics[width=0.5\textwidth]{figure/norm-wide.png}
\caption{Example of a Gaussian (normal) distribution with a large variance.}\label{fig:norm-wide}
\end{subfigure}
\caption{Two Gaussian (normal) distributions with differing variance.}\label{fig:norm-dist}
\end{figure}


%This example has shown a one-dimensional Gaussian function.  In reality the Gaussian model of an acoustic feature vector $c$ with length $N$ has $N$ dimensions; if there are 39 features in a vector, one of the 39 dimensions of the Gaussian represents one feature, another dimension represents another feature, and so on.  
For example, take the feature vector $c_0(m)$ in first row of the matrix of features in Figure \ref{fig:feat-vectors}.  Each row in this simplified example is a feature vector of the phoneme /u/.  Each column holds the values for a particular feature.  The first feature of the first vector $c_0(0)$ is 12.179.  A Gaussian Model $\mathcal{N}_{/u/}(\mu,\sigma^2)$ trained on this feature alone would result in a mean $\mu=12.179$ and variance $\sigma^2\approx 0$, where \begin{equation} p(c_0(0)|\mathcal{N}_{/u/}(\mu,\sigma^2))\approx 1 \end{equation} As the model is trained on $c_1(0)$ and $c_2(0)$ and subsequent features, variance begins to develop, and the mean $\mu$ and variance $\sigma^2$ are updated accordingly.
%
\begin{figure}[H!]
\begin{center}
\begin{tabular}{|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1cm}} \hline
    & $c_n(0)$ & $c_n(1)$ & $c_n(3)$ & $\cdots$ \\ \hline
$c_0(m)$ & $12.179$ & $-2.007$ & $0.059$ & $\cdots$ \\ \hline
$c_1(m)$ & $12.010$ & $-2.279$ & $0.060$ & $\cdots$ \\ \hline
$c_2(m)$ & $11.976$ & $-1.766$ & $-0.002$ & $\cdots$ \\ \hline
\vdots & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\ 
\end{tabular}
\end{center}
\caption{Cepstral feature vectors $c_0$ through $c_N$.}\label{fig:feat-vectors}
\end{figure}

Thus far only a univariate, or one-dimensional, Gaussian has been displayed.  The Gaussian models $\mathcal{N}_{ph}(\mu,\sigma^2)$ used to model acoustic feature vectors representative of different phonemes are multivariate, or multi-dimensional.  This can be visualized in Figure \ref{fig:two-dim-gauss}, which contains two dimensions ($x$-axis and $y$-axis); each axis has its own mean and variance, and the distribution of the multivariate (two-dimensional) Gaussian is represented by the circles around the datapoints.  
\begin{wrapfigure}{l}{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{figure/two-dim-gauss.png}
\caption{Example a distribution along two dimensions, \textbf{x} and \textbf{y}.  The ellipse represents the boundaries of the distribution in both \textbf{x} and \textbf{y} dimensions. Distribution is only approximate.}\label{fig:two-dim-gauss}
\end{wrapfigure}
This could also be extended visually into a third dimension, but visualization beyond this breaks down.  It is very difficult to visualize the 39-dimensional Gaussian model $\mathcal{N}_{ph}(\mu,\sigma^2)$ where each dimension ($x$-axis, $y$-axis, $z$-axis, etc.) corresponds to a particular acoustic feature (eg. cepstral coefficient).  The multivariate Gaussian used to represent the acoustic feature vectors for a phoneme $ph$ essentially can be thought of as a combination of 39 univariate Gaussians into one.
This is [partially] why, for example, a model trained on raw MFCC acoustic vectors (of length 13) cannot recognize feature vectors also including delta and delta-delta features (length 39) - there is a dimensionality mismatch.

Only single, isolated feature vectors for model training have been used up to this point.  But speech is temporal and context dependent, and it is important to be able to model those characteristics.  For example, a /u/ vowel in the context preceding a /n/ has different qualities than if it were to precede a /t/, due to context dependency.  In ASR, it is normal to take into account the phonemes on either side of the phoneme in question, resulting in a three-phoneme sequence, eg. /t/+/u/+/n/.  These \textit{triphone} sequences use Hidden Markov Models (HMMs) to accomplish this sequence modelling (\cite{jurafsky:09}).

HMMs consist of states and of transitions between the states.  Each state $q_i$ has an emission probability, $b_i$, which is the probability of `landing' in state $q_i$. % (cf. Figure \ref{fig:hmm-plain}).  
Each transition has a `transition' probability, $a_{ij}$, which is the probability of transitioning from state $q_i$ to $q_j$.  For ASR, the emission probability of a state is a multivariate Gaussian.  So the state for phoneme /u/ would use the previously trained multivariate Gaussian $\mathcal{N}_{/u/}(\mu,\sigma^2)$ described earlier, in order to compute its emission probability.  Most ASR systems use triphone acoustic models; the HMMs in these systems use three states to model triphones.  An example three-state triphone HMM for the sequence /t/+/u/+/n/ can be seen in Figure \ref{fig:hmm-sounds}.  

\begin{figure}[htbp]
\centering
\tikzstyle{state}=[circle,
                                    thick,
                                    minimum size=1.2cm,
                                    draw=black!80]

\begin{tikzpicture}[>=latex,text height=1.5ex,text depth=0.25ex]

  \matrix[row sep=0.5cm,column sep=0.5cm] {

        \node (q_{/t/}) [state] {$\mathbf{q}_{/t/}$}; &
        \node (q_{/u/})   [state] {$\mathbf{q}_{/u/}$};     &
        \node (q_{/n/}) [state] {$\mathbf{q}_{/n/}$}; & \\
    };

    \path[->]
        (q_{/t/}) edge[thick] (q_{/u/})	% states via the state
        (q_{/t/}) edge[loop above,thick] (q_{/t/})
        (q_{/u/}) edge[loop above,thick] (q_{/u/})
        (q_{/n/}) edge[loop above,thick] (q_{/n/})
        (q_{/u/})   edge[thick] (q_{/n/})		% accentuated.
        ;

\end{tikzpicture}

\caption{An example HMM with three states, each state representing a different phoneme.  The arrows between the states represent transitions from one state to another.  The arrows pointing to the same state are called `self-loops'.}\label{fig:hmm-sounds}
\end{figure}

The state $q_{/t/}$ has a multivariate Gaussian $\mathcal{N}_{/t/}(\mu,\sigma^2)$ trained to emit its emission probability, as does $q_{/u/}$ have $\mathcal{N}_{/u/}(\mu,\sigma^2)$ and $q_{/n/}$ have $\mathcal{N}_{/n/}(\mu,\sigma^2)$.  In Figure \ref{fig:hmm-sounds}, there are transition probabilities from one state to another (eg. the probability of transitioning from state $q_{/t/}$ to $q_{/u/}$), but also transitions from one state to itself (eg. the probability of transitioning from state $q_{/t/}$ to $q_{/t/}$).  Recall from Section \ref{sec:mfcc-calc} that a normal duration of a frame of audio used for feature extraction is 25 ms; ie. each feature vector corresponds to 25 ms of audio.  Most phonemes have a duration longer than 25 ms, and so there is a need to transition back to itself if the subsequent feature vector indicates that the phoneme has not changed.  The probability for any state $q_i$ to transition to itself is \begin{equation} p(q_i|q_i) = a_{ii}\cdot b_i \end{equation} and the probability to transition to any other state $q_j$ is \begin{equation} p(q_j|q_i) \approx a_{ij}\cdot b_j \end{equation} where $a_{ii}$ and $a_{ij}$ are the transition probabilities, and $b_i$ and $b_j$ are the emission probabilities.  Note that both transition and emission probabilities are required for each `jump', including `jumping' to the same state.  These probabilities are estimated and set using the acoustic feature vectors from the speech provided for training (\cite{gales:07}).

For a total of $N$ phonemes in a language, developing a triphone for every combination of three phones will yield $N^3$ triphones.  Since this results in a very large number of hypothetical triphones, and some phoneme combinations never actually occur, many can be dropped.  The combination of some states is sometimes used to reduce the total number, which is unwieldy.  State combination would then occur if certain pre-specified parameters are met.  This is called `state tying'. These parameters can vary, and different ASR implementations tie states under different conditions.  The basic logic is that, similar states with similar Gaussian models can be combined (eg. the two /n/s in `/t/+/u/+/n/' and `/l/+/u/+/n/').  The multivariate Gaussian models from any states that are combined together are also combined; this yields what is called a Gaussian Mixture Model (GMM) (\cite{gales:07}).  This process of state tying is not addressed in further depth in this paper.

This describes a traditional HMM-GMM acoustic model, that uses the GMM to calculate the emission probability $b_i$ for any state $q_i$.  The GMMs in the acoustic model are trained directly from the feature vectors output by either MFCC or PLP front-end feature extraction techniques.  It is important that the features from the speech used for training are representative of the speech that the model will be tested on.  The distribution of the GMM along each of $N$ feature dimensions may not accurately match speech that has been altered.  It is apparent how additive noise can distort the feature values and worsen recognition, eg. where the features of a feature vector $c_{/u/}$ have been distorted such that they no longer fit into the distribution of GMM $\mathcal{N}_{/u/}(\mu,\sigma^2)$ with high probability.

A recent modification replaces the GMM model with a Deep Neural Network (DNN) to generate the emission probabilities. This method has enjoyed much success (\cite{zhang:17}); the HMM-DNN architecture in Kaldi requires that an HMM-GMM first be trained, which is adapted to replace the GMMs with DNNs.  This is the model type used in this paper.  Other modern neural network-based ASR systems exist besides an HMM-DNN, but these are not utilized and are not discussed.



\subsection{Language Models}\label{sec:lang-mod}

Language Models (LMs) are not a major component of the analysis in this study and are therefore only addressed superficially.  They have a very wide variety of uses in ASR, and can be utilized in many different ways.  A common and intuitive example of the usage of a LM is described.

A LM is typically trained on a very large corpus of sentences, learning the probabilities of various word sequences that occur in the corpus.  Sequences that appear more often than others will have a higher probability.  A LM, in a sense, necessarily makes the assumption that the corpus is fully and accurately representative of the language in which it is written, ie. all possible word combinations in the language also occur in the corpus. Moreover, it assumes that these word combinations occur in the corpus in the same proportions that they occur in the language as a whole.  There are methods that have been developed to deal with this, which will not be discussed, but these assumptions demonstrate that the extent and breadth of the corpus on which the LM is trained plays an important role in successful and accurate recognition.  

Similar to an acoustic model, LMs encode these sequence probabilities using Hidden Markov Models (HMMs); in this case, a sequence of phonemes and a sequence of words.  Generally, trigram HMMs are used.  
The acoustic model outputs a series of likely phonemes for the sequence of acoustic feature vectors it received.  These phoneme sequences are then classified into sets of probable words by the LM.  After the probable words have been generated, the LM then calculates the probabilities of possible sequences of words, given the possible words identified in the first step.  For example, the language model generated two possible sequences of words from the sequence of phonemes output by the acoustic model: `the big cat' and `the pig cat'.  The trigram `the big cat' has a greater likelihood of occurrence in the corpus on which the LM was trained, and therefore it is chosen as the best candidate to represent the speech signal that originally entered the ASR system.  This is a generalization of the LM decision process, as a number of other factors also come into play.

There have also been a rise in language models using alternative (non-HMM) frameworks, such as Recurrent Neural Network Language Models (RNN-LMs), which have shown some improvement over traditional HMM LMs.  However, the present report utilizes HMM-based LMs, and so RNN-LMs are not discussed.


% \subsection{Noise-Robust Modifications to Traditional MFCC Calculation}
% 
% In the feature domain, noise is dealt with, and the signal is enhanced, during or directly after feature extraction and prior to sending the features to the acoustic model for recognition.  
% %These modifications are made without altering the acoustic model parameters, resulting in low computation cost.
% 
% % In the feature space domain of noise-robust ASR processing, there are a number of broad techniques, including (a) noise-resistant features, (b) feature normalization, and  (c) feature compensation.  Noise resistant features are, quite simply, features in the acoustic signal which are not sensitive to environmental changes.  Many researchers have proposed many methods of signal derivation that incorporate features of the human auditory system, including Perceptual Linear Prediction (PLP, \cite{hermansky:85}), introducing the `auditory spectrum' and explicit formant information into ASR processing, and Relative Spectral processing (RASTA) applied to PLP (\cite{hermansky:92}), making PLP less sensitive to slow varying speech information and more sensitive to the more rapid-varying transitions of speech, which is important in human speech perception (\cite{?,willi:17}).  \cite{kim:99} attempts to model functions of the cochlea and auditory nerve. These methods are quite effective at dealing with short-term, stationary, additive noise (\cite{zhang:17}).  More recent methods include SPARK (\cite{fazel:12}, 1369), which is ``neurobiologically inspired'' by ``auditory receptive fields'' and ``local competitive behavior'', and that proposed by \cite{moritz:15}, which emulates the amplitude modulation found in mammalian auditory cortexes.  These are just a selection of methods incorporating biologically based noise resistant features, which generally outperform `vanilla' MFCC methods. The fact that these features can be quite complex to generate, and the parameters difficult to set, makes it hard to utilize a combination of them, preventing widespread usage and incorporation with other techniques.  A straightforward relation between the cleaned features and noisy speech is also difficult to derive due to the complexity involved in the feature calculation (\cite{li:14}).
% 
% % Feature normalization (b) generally involves normalizing cepstral feature vectors in the form of cepstral mean normalization (CMN) and cepstral mean and variance normalization (CMVN).  CMN involves finding the mean values out of all cepstral vectors (\cite{atal:74}).  All cepstral vectors are then normalized, such that the mean cepstral value becomes zero.  CMN primarily eliminates reverberation and channel-related distortion, but signals with noise and no channel distortion also see improvement (\cite{droppo:08}).  CMVN takes the mean and normalizes it together with the co-variance of the cepstral vectors, yielding improved performance on speech data with additive noise (\cite{viikki:98}).
% % These methods do not work in real-time, however, as they require cepstral vectors from the entire utterance in order to calculate means and variances.
% 
% % Feature compensation (c) actually attempts to remove the noise from the noisy speech signal, allowing for use of traditional features. Spectral subtraction (\cite{boll:79} is an intuitive method of removing noise by taking a small window of the waveform, turning the linear signal into the spectral domain, and subtracting an existing or estimated noise spectrum $n$ from a noisy speech spectrum $y$, leaving a spectrum of clean speech $x$.  This is then converted back into the time domain, and the process is repeated all along the waveform.  Spectral subtraction often estimates the noise by looking at sections of the observed signal that do not contain speech information.  
% % 
% % This method still has several problems (\cite{li:14}). First and foremost, the location of speech in the signal in a noisy environment is difficult to detect, which consequently affects the ability to accurately compute a noise average. It also requires relatively stationary, slow-variation noise; noise that changes quickly can have a different average spectrum during the portion of the signal containing speech than the portion of the signal in which the noisy spectrum was calculated.  Furthermore, this is only an average of the noise, and not exactly the noise itself.  This subtraction of the average can inadvertently have an additive noise effect by producing extraneous acoustic artifacts in the `clean' signal which were not there to begin with (\cite{berouti:79}).
% % 
% % Wiener filtering (\cite{lim:79}) is another method used to remove noise from a signal.  As opposed to spectral subtraction, however, this is a linear filter that works without the need to convert the signal into spectra.  However, this method also requires an estimation of the noise.  Furthermore, it does not do well in very low SNR environments, as it generally results in suppression and dampening of the entire signal, and not just the noise (\cite{li:14}).
% 
% % More standard, is the `advanced front-end' (AFE) ensemble proposed in \cite{etsi:02}.  It yields more than 50\% improvement over standard MFCC features alone, and has become a frequent baseline for comparison in noisy ASR research.  It is composed of three separate `tools': two-stage Mel-warped Wiener filtering, SNR-dependent waveform processing, and blind equalization (cf. \cite{agarwal:99,macho:02,macho:01,mauuary:98}).
% 
% % Most of the heavy work of the AFE ensemble is performed by the Mel-warped Wiener filtering (\cite{agarwal:99,li:14}).  This filter differs from the more standard Wiener filter in that it uses the Mel-frequency power spectrum in the Wiener filter calculations, as opposed to the linear signal, the result of which is then converted back into the time domain.  The filter is applied once, and then a second time to remove residual noise.  SNR-dependent waveform processing (SDWP, \cite{macho:01}) assumes that the noise remains relatively constant, whereas the speech signal causes variation in the amplitude of the signal.  SDWP uses this assumption to dampen portions of the signal with a relatively constant and low SNR compared with the high SNR (ie, speech-less versus speech-bearing) portions of the signal, which are amplified.  Blind equalization serves to eliminate convolutional (eg. reverberant) distortion from the signal (\cite{mauuary:98}).
% % 
% % Considered to be the best `general-purpose' noise-removal tool (\cite{zhang:17}, 4) using traditional (non-neural network) techniques, the minimum mean square error (MMSE) magnitude modulation estimator (MME) was developed by \cite{paliwal:12}, and based on the acoustic modulation estimator (AME) first proposed by \cite{ephraim:84}.  The approach utilizes the spectral modulation magnitude domain, rather than the spectral frequency domain (as is used in the AME method), which is where much of its success originates.
% 
% \subsection{Feature Space Domain: Neural Networks}

% Broadly, there are two primary categories of utilizing neural networks to account for noisy speech: `mapping methods' and `masking methods' (\cite{zhang:17}).  Mapping methods involve finding the non-linear function that maps the noisy speech to the clean speech.  In neural network terms, the noisy speech is the input to some type of neural network (eg. Deep Neural Network (NN), Convolutional NN, Recurrent NN, etc.) and the (intended) output is an approximation of the clean speech.  Due to the complexity of speech in the temporal domain, the input to the neural network usually comes from one of the higher-processed input transformations, such as from the spectral or cepstral domains.
% 
% Masking-based approaches work similarly to a traditional filter, albeit learned via a neural network. A method using an `Ideal Ratio Mask' will use a neural network to learn the ratio (value between 0 and 1) of the presence of clean speech to noise.  %The neural network then ``learns'' this masking ratio value that can be applied to the noisy speech signal $y$ to ideally return the clean speech $x$ as output.  
% This process is most beneficial when using spectral or cepstral features as inputs.  These calculated masking ratio values are then multiplied element-wise to each spectral or cepstral feature (from the noisy signal $y$) at every time index, returning the estimation of the clean speech $x$ as output (\cite{zhang:17}).


% Model Space
% \subsection{Model Space Domain}
% 
% Model space comes after feature space in the ASR process, and encompasses the acoustic model parameters, methods of training the model, etc.  In the model space, acoustic parameters themselves can be modified in accordance with the noisy signal.  This generally results in high computation cost when training the acoustic model.
% 
% The description of model space domain compensation techniques will be brief, as this is not the focus of the present study.  This form of compensation usually involves adapting an existing acoustic model (presumably trained on relatively clean speech) to enable recognition of more noisy features.  Variations of maximum likelihood linear regression (MLLR, \cite{leggetter:95}) are often used to adjust the %gaussian component vector 
% model means and co-variance parameters to account for differences in the signal that is introduced by noise.  There are many variations and extensions of MLLR; one such variation, feature-space MLLR, or fMLLR, actually moves MLLR application into the feature domain (\cite{gales:98}).  
% 
% There are also a few model-based approaches using neural networks for noise-robust ASR.  Most widely used is multi-condition training (\cite{seltzer:13,zhang:17}), which, similar to multi-style training originally developed by \cite{lippman:87}, uses a collection of training data that exhibits a wide range of noise conditions.  Another technique, similar to methods used in non-neural network approaches, involves adapting the already trained acoustic model with a small subset of noisy data.  However, as doing so can inadvertently result in significant overfitting, \cite{mirsamadi:15} have developed a technique unique to neural networks that - instead of slightly adjusting all weights, adds an additional layer to the neural network with its own weights.  This largely avoids the issue of overfitting, while increasing the model's robustness to noise.
% 
% \cite{weninger:13}, among many others, also combine the modifications in the feature space domain with modifications in the model space domain, referred to as joint model training.  Broadly, this takes the form of using the feature-based noise removal techniques to output feature-enhanced data which is then used as training data itself for the acoustic model.
% 
% \subsection{Microphone Arrays}
% 
% There are also techniques that employ multiple microphones as a method of source-separation to extract the speech source from any extraneous noise sources. Beamforming (\cite{veen:88}), for example, has become a central technique to using microphone arrays for source separation (\cite{hori:15,zhang:17}).
% The direction of arrival of the different sound sources is calculated, taking into account the distance between the two (or more) microphones, and the time of arrival of the different sources in each signal recorded by each microphone.  Recent work (cf. \cite{heymann:15,sivasankaran:15,heymann:16}) has also employed neural networks to aid and enhance the beamforming process . 

\subsection{Interim Summary}
% 
% Most research over the past few decades has focused on feature space domain modifications.  This is likely due to the intensive computation required by many model space domain techniques.  Leading feature-space techniques include MMSE-MME (\cite{paliwal:12}) and AFE (\cite{etsi:02}). %which is often used as a standard baseline for comparison with new feature space modifications.  
% Model space domain approaches include adjusting the acoustic model parameters, often using a form of MLLR (\cite{leggetter:95}), which can also take the form of fMLLR in the feature space domain.  In the last few years, the advent of neural networks has seen further improvement in both feature and model space domains.  Other recent approaches have combined feature and model space modifications (joint model training), and the use of multiple microphones into a microphone input array has also become more mainstream. The recent CHiME challenges (\cite{chime:16}) have incorporated the use of multi-channel ASR input as well as single channel input as part of its task.
The structure of an ASR system is twofold: front-end feature space domain and back-end model space domain.  There are several techniques used to extract features; MFCC and PLP are the most common. Noise reduction methods can be applied to the signal at various times during this process, either in the time domain (eg. Wiener Filtering, cf. Section \ref{sec:spec-sub_wiener}), after a number of levels of processing (eg. RASTA in the power domain, cf. Section \ref{sec:rasta_cmvn}), or after the generation cepstral coefficients themselves (eg. Blind Equalization, cf. Section \ref{sec:background-afe}), right before being passed to the acoustic model.  Those discussed in this section are some of the most established methods of noise reduction, and are incorporated into many ASR systems, but this is by no means a comprehensive overview.  

The acoustic model then uses the feature vectors to train Gaussian Mixture Models (GMMs) or Deep Neural Networks (DNNs) to model different phonemes.  Hidden Markov Models (HMMs) are used to emulate the temporal and sequential aspects of speech. During decoding (recognition), the acoustic model can find the probability of a feature vector given any of the GMMs or DNNs and identify the most likely phoneme categorization for the vector.  The phoneme output sequence is sent to the Language Model (LM), which strings phonemes into words and words into sentences using its trained weights.  There are some methods of noise-reduction proposed which can be applied in the model space domain, but these are not discussed.

Most of the noise-reduction techniques that are used in ASR to account for noise are still forced to make estimations about noise type, the location of speech in the signal, or the SNR of the signal.  As would be expected, as SNR decreases, and as noise becomes more variable (non-stationary), these methods begin to falter.  There are hundreds more noise-reduction techniques that have been proposed, but these are beyond the scope of this project.

The focus of the present study is to test the ability of an ASR system to accurately recognize the ear-recorded speech collected in Chapter \ref{chapter2}.  The study in Chapter \ref{chapter2} proposed a method of collecting speech in noisy environments, recording speech at the mouth (which is traditionally done) or from the inside of the ear canal (the proposed technique).  The latter is hypothesized to be largely immune to noise type and the stationarity (or lack thereof) of noise.  Therefore, it does not require any noise estimation or inference, unlike many of the noise-removal applications described above.  This noise removal process happens passively, without computation, and prior to MFCC feature extraction from the raw speech audio signal.  One trade-off is that the speech in the recorded signal is heavily low-pass filtered; the highest speech frequencies observed in the signal are generally found near 2.7 kHz.  There are a number of methods proposed to deal with this limitation, discussed further in Section \ref{sec:lpf-compensation} below.


\subsection{Compensation for Low-Pass Filtering}\label{sec:lpf-compensation}

The low-pass filtered speech from the data collection experiment (Chapter \ref{chapter2}) is considered to have a `limited bandwidth' - a term used to describe a speech signal which has undergone some degree of filtering.  Contrasted with `full-bandwidth' speech, limited bandwidth speech is much more difficult for ASR systems to understand (\cite{morales:09}). This is due primarily to missing acoustic features which were in the missing frequencies that the ASR system is expecting to find.

The problem of limited bandwidths can be described using a variation of Equation \ref{eq:basic}, shown below as \begin{equation} y(t) = x(t) \cdot h(t) \end{equation} where $x$ is the clean signal, $h$ is the bandwidth distortion, $y$ is the resulting signal, all at time $t$, and - for the purposes of these equations - additive noise is assumed to be absent or already dealt with.  This can be extended to the frequency domain by converting each signal to a power spectrum, such that\footnote{Equations in this section are adapted from \cite{morales:09}} \begin{equation} |Y_{jt}|^2 = |X_{jt}|^2 \cdot |H_jt|^2 \end{equation} where $j$ corresponds to the frequency bin and $t$ corresponds to the frame.  For all frequency bins $j$, $j$ can either be a member of the set of filtered frequencies $F$, or is not filtered and does not belong to the set $F$.  A simplified version of the limited bandwidth distortion assumes a binary filter, where the frequency information is either completely passed, or it is completely filtered.  Under this assumption, \[ h_j= \begin{cases} 0,& \text{if }  j \in F \\ 1,& \text{if }  j \notin F \end{cases} \] where $h_j$ is a simplified version of $|H_j|^2$.

Recall the MFCC calculation from Equation \ref{eq:mfcc-calculation} in Section \ref{sec:mfcc-calc}, which, given the previous assumptions, can be simplified to \begin{equation} c_x(m) = \sum{j=1}^{J} C_{mj} \cdot \log(|X_{j}|^2). \end{equation} where $c_x(m)$ is the cepstral coefficient at index $m$ of the feature vector $c_x$, representing the clean signal. $J$ is the total number of frequency bins, and $C_{mj}$ is the Discrete Cosine Transform.  Adding in the filter component yields \begin{equation}\label{eq:mfcc-w-filt} c_y(m) \approx \sum{j=1}^{J} C_{mj} \cdot \log(h_j \cdot |X_{j}|^2) \end{equation} with $c_y(m)$ representing the cepstral value $m$ for the cepstral vector of the distorted signal $y$.  Since $h_j$ is binary and cancels any frequencies in which $h_j=0$, Equation \ref{eq:mfcc-w-filt} can be rewritten as \begin{equation} c_y(m) \approx \sum_{\substack{j=1\\j\notin F}}^J C_{mj} \cdot \log(|X_j|^2) \end{equation} and extended to \begin{equation}\label{eq:mfcc-filter-separation} c_x(m) \approx c_y(m) + \sum_{\substack{j=1\\j\in F}}^J C_{mj} \cdot \log(|X_j|^2). \end{equation}
  
Equation \ref{eq:mfcc-filter-separation} recalculates the true cepstral vector $c_x$ for the original speech signal $x$.  However, $|X_j|^2$, where $j\in F$, is still unknown.  A set of mapping functions $G$ can be developed such that \begin{equation} G_j^k(c_{y}) = |X_j|^2 \end{equation} where $j$ is the frequency bin, and $k$ represents a cluster of similar sounds.  This makes the assumption that there are correlations between the speech frequency information present in the signal, and those frequencies that have been filtered out, which can be at least partially predicted or recovered by a set of mapping functions $G$. A recovery of high-frequency information may improve ASR recognition of the low-pass filtered, ear-recorded speech collected in Chapter \ref{chapter2}.

Existing research has theoretically (\cite{morales:09}) and empirically (\cite{morales:05b,morales:09,he:11}, among others) demonstrated that information from filtered frequencies correlates with, and can partially be compensated with information in frequencies remaining in the signal.  These compensation mapping functions are primarily applied in the feature space domain (cf. Section \ref{sec:feat-space-comp}), though there are some compensation techniques that can also be used in the model space domain (cf. Section \ref{sec:mod-space-comp}).

\subsection{Feature Space Domain Compensation}\label{sec:feat-space-comp}

\cite{morales:09} gives perhaps the most comprehensive coverage of feature space compensation for limited-bandwidth data.  They present a general pipeline for feature retrieval.  First (a) the training data is optionally clustered into phonemes or other similar sound categories.  Following this, (b) the system develops class-specific mapping functions, or, if initial clustering is not performed, a global mapping function.  Finally, (c) the mapping functions $G$ are applied to the distorted features $c_y$ which returns an estimation of the full bandwidth features $c_x$.

For clustering (a), this can be performed either via a knowledge-based approach using transcribed phoneme classes, or a `data' driven approach using Gaussian distributions.  For the latter, a single distribution is calculated.  The mean of this distribution is `perturbed' in both directions by adding and subtracting a preset hyper-parameter (combined with the variance).  These two new means are used to define two separate clusters, and the mean and variance of both of these clusters is recomputed.  This separation technique is repeated until a satisfactory number of feature vector (`phoneme') clusters are obtained.  The final result is a set of Gaussian-defined clusters which were automatically generated based on acoustic properties.  For these reasons, namely that no labeled data is required, and clusters are acoustically-based without linguistic biases, \cite{morales:05b} states that Gaussian-based clustering is superior to the knowledge-based method.

Once feature vectors are clustered into their assigned class, mapping functions, or `corrector' functions, can be developed (b).  In \cite{morales:05b}, this takes the form of either a simple offset, or a polynomial function.  The simple offset is derived by \begin{equation} c_o^k = \dfrac{1}{N^k} \sum_{n=1}{N^k} c_x^n - c_y^n \end{equation} where $c_o^k$ is the offset cepstral vector for class $k$, computed as the average of the difference between all full-bandwidth ($c_x$) and limited-bandwidth ($c_y$) vector pairs, and $N^k$ is the number of cepstral vectors for class $k$.  Alternatively, a polynomial can be developed using the class' Gaussian mean feature vector and covariance matrix (\cite{morales:05b,morales:09}).  However, if classes are not used, a general offset or polynomial function would be developed for all training vectors.

After compensation functions have been successfully trained, they can be applied to novel feature vectors (c).  This involves using some form of an Expectation Maximization function to classify the novel vector into one of the preset classes.  Then, the class specific transformation would be applied to the vector.  This is done for all vectors in the audio signal.  If classes were not used, and a generic transform were obtained, \cite{morales:09} present two methods of compensation.  The first is a simple, single application of the transform to all test data.  The second involves combining the general application with the phoneme-based clustering performed in (a).  

Gaussian distributions are not developed for the phoneme clusters, and since test data does not come phonetically labeled, it is not possible to initially separate the test vectors into classes.  To remedy this, the general compensation transform is applied, followed by an initial round of ASR phoneme recognition.  This automatically generates phoneme labels for each vector.  Following this classification, the phoneme-specific corrector function is applied to the appropriate vectors.  Once final vectors are calculated, regardless of classification technique or corrector function, they are sent to an ASR recognizer for word-level recognition.


\subsection{Model Space Domain Compensation}\label{sec:mod-space-comp}

For model domain compensation, this generally takes the form of either adapting an existing acoustic model with domain-specific data, or retraining a new acoustic model on domain-specific data.  Both of these techniques are utilized throughout many domain-specific ASR applications, including specific training for noisy data.  Retraining a new acoustic model is by far the most widely used technique (\cite{morales:09}).

\subsection{Comparison of Performance}

\cite{morales:09} report an array of comparisons.  They test various methods of correcting the cepstral vectors, using various levels of bandpassed filtered signals (eg. cut-off frequencies of 6 kHz, 4 kHz, 2 kHz, bandpass of 0.3-3.4 kHz).  First, they find empirically what was hypothesized in \cite{morales:05b}, that Gaussian-based classification is superior to knowledge-based phoneme classification.  This is true regardless of frequency cut-off.  Gaussian polynomial functions are marginally better than simply using the Gaussian offset (\cite{morales:05b}), and there is no statistical difference between different orders of polynomials, with a simple linear function performing as well as higher order functions, and it is suspected that using polynomial orders past 6 will lead to a decrease in performance (\cite{morales:09}).

For the data with 6 kHz and 4 kHz cut-off frequencies, Gaussian-based approaches are comparable to model adaption and model retraining using domain specific data, and these are also comparable to performance on full-bandwidth data.  However, as the cut-off frequency decreases to telephone bandwidth, and a 2 kHz cut-off frequency, the difference between model approaches and feature-based approaches increases, with a word error rate (WER) divergence of approximately 12\% between model retraining and Gaussian feature compensation on 2 kHz data. As a comparison, see Table \ref{tab:feat-comp}, which reports some of the results obtained by \cite{morales:09}.

It is worth noting that \cite{morales:09} created an `upper-bound' condition, in which they manually classified each phomeme of the test data into its appropriate phoneme category.  The corrector functions were applied, and accuracy calculated.  In nearly all cut-off conditions, this method proved to be comparable to the performance of full-bandwidth data - even in the 2 kHz cut-off condition - outperforming model retraining.  This indicates that higher frequency information must be correlated to lower-frequency information, and that a problem does not primarily lie with the calculation of corrector functions, but with the accurate classification of unlabeled feature vectors.  Furthermore, it indicates that the knowledge-based phoneme categories are optimal, or at least, result in nearly the same performance as full-bandwidth speech.  Again, the matter is the classification of incoming signals after the classes have been set and mapping functions have been trained.

\begin{table}[h]
\centering
\begin{tabular}{|c||>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{2cm}|} \hline
 & Full- Bandwidth & 4 kHz Low-pass & 0.3-3.4 kHz Bandpass & 2 kHz Low-pass \\ \hline\hline
No Compensation & 28.82  & 55.33 & 67.33 & 73.90 \\ \hline
Two-stage Phone. &  --   & 35.69 & 49.82 & 55.58 \\ \hline
Gaussian-based &  --     & 32.55 & 39.82 & 50.47 \\ \hline
Model retraining &  --   & 30.67 & 34.27 & 38.43 \\ \hline
Hand-clustered test & -- & 28.56 & 28.99 & 29.68 \\ \hline
\end{tabular}
\caption{A comparison of the results of selected bandwidth-compensation methods from \cite{morales:09} using three of the cut-off frequency datasets (4 kHz, 0.3-3.4 kHz, and 2 kHz).  `No Compensation' means no transformation was applied.  `Two-stage Phone.' uses the general compensation, followed by an initial ASR clustering, followed by phone specific compensation.  `Gaussian-based' uses the Gaussian clustering algorithm.  `Model retraining' does no feature-space compensation, but retrains an acoustic model on data of the same bandwidth. `Hand-clustered' indicates that the data was clustered by hand, and a corrector function was then applied; this is treated as an upper bound.  Results were originally presented as a measure of `accuracy', the reverse of word error rate (WER). However, for continuity with the rest of this report, the results in this table are given in WER.}\label{tab:feat-comp}
\end{table}

One area in which feature compensation excels over model adaption or retraining is when the incoming speech data has variable bandwidth distortions.  An example given in \cite{morales:09} is a broadcast in which an historical recording was aired, which had very limited bandwidth, followed by the full-bandwidth speech of the broadcaster.  The success of model retraining depends critically on the use of training data that contains the same distortion as the testing data.  Returning briefly to the first topic of noise-robust ASR, this is partly why retraining a model on noisy speech is not guaranteed to be successful - because noise is often not predictable. However, for the data in the present study, the low-pass distortion had been demonstrated to be predictable (cf. Chapter \ref{chapter2}).  Furthermore, these signals had been pre-emphasized and explicitly low-passed to 2.5 kHz with a 500 Hz slope, so each signal has the same frequency range.  Since these factors are constant, and since model retraining substantially out-performs feature compensation (especially for the lower-passed frequencies), the former method was used in the ASR experiment described below (Section \ref{expt3}) in place of feature compensation.

\subsection{Summary}

Standard ASR systems use a two-step decoding process: feature extraction followed by acoustic and language model decoding.  MFCC features and PLP features can both be used, with PLP producing more noise-robust features that can offer a slight performance benefit in noisy-background speech.  CMVN is also often applied to normalize the cepstral vectors as a measure of noise-robustness.  Additional methods of noise-robust processing exist, a few of which (eg. Spectral Subtraction, AFE; cf. Sections \ref{sec:spec-sub_wiener} and \ref{sec:background-afe}, respectively) were discussed further. The process by which an acoustic model is trained was described in Section \ref{sec:acoust-mod}, as well as the purpose and use of a language model (Section \ref{sec:lang-mod}).

Many methods of noise removal require noise estimation, which can be quite difficult in rapidly changing noise environments, and without adequate estimation techniques, performance will suffer. Furthermore, as SNR drops, the performance of these noise-robust and noise-removal applications also drops, and WER increases.  For the present study, data has been collected from the inside of the ear-canal that has shown to have a substantial noise-reduction effect on the signal, without requiring an estimation of the noise.  However, in addition to filtering out noise, high-frequency speech information if also filtered out, resulting in a signal with a low-frequency bandwidth.

A number of feature compensation techniques exist that aim to modify the cepstral coefficients of limited-bandwidth data such that they mimic those which have full-bandwidth information.  These are able to increase the performance to near full-bandwidth levels in many cases, and are relatively robust to frequently changing bandwidths.  However for very low-pass filtered speech, namely telephone bandwidth (0.3-3.4 kHz) and below, the performance of model retraining exceeds that of feature compensation.  Model retraining works best when the expected bandwidth distortion is relatively regular and stationary.

The distortion of the speech recorded from the ear canal has been shown to be regular and predictable, but furthermore, processing is performed on the signal to create additional uniformity.  The only processing performed after the signal is recorded is pre-emphasis and low-pass filtering to 2.5 kHz with a 500 Hz slope, with a second application of pre-emphasis.  The relatively predictable distortion of the signal, and the standardized processing, is hypothesized to optimize the advantage of model retraining as an effective compensation for low-pass filtering.  The author hypothesizes that this method of passive noise removal via recording speech from the ear canal, plus the minimal modification of pre-emphasis and low-pass filtering, and the use of a model retrained on data with a similar bandwidth will demonstrate similar or better ASR performance over noisy speech.


% Another tool used is the exploitation of any prior knowledge about the distortion (\cite{li:14}); this is prior knowledge that is utilized during the training stage, not knowledge about the noise during the testing stage.  Some methods include learning the mapping between noisy and non-noisy pairs of acoustic signals.  This mapping is then extended to novel noisy utterances during testing.  This is used in feature domain space to enhance a noisy speech feature to then send to the model.  
% 
% Other methods utilze mutliple acoustic models, each trained on data from different environments and different noises and different SNRs.  The means and covariance matrices of each of these models are stored, and during recognition, the most appropriate model is chosen to use to decode the signal in question.  Either of these tactics, though, do require prior knowledge about the noise.  As with multi-style training, explained above, it is very difficult to ensure that all noises, SNRs, etc, are adequately accounted for during training in order to be prepared for what is seen during testing.
% 
% 
% %Implicit vs Explicit Distortion Modelling
% 
% Explicit distortion modelling uses a ``physical model'' which allows for high performance with few distortion parameters. An example of an explicit distortion model would be spectral subtraction, discussed earlier.  It seems obvious that spectral subtraction, when matched with an agreeable signal that best utilizes its noise removal abilities, would result in more accurate speech recognition.  Consequently, other noise reduction methods that \textit{explicitly} specify the distortion tend to perform well.
%
%
% \textbf{For examples of what this distortion model actually is, go to the primary literature:}
% Y. Zhao and B. H. Juang, â€œA comparative study of noise estimation
% algorithms for VTS-based robust speech recognition,â€ in
% Proc. Inter-
% speech
% , 2010, pp. 2090â€“2093.
% J.Li,L.Deng,D.Yu,Y.Gong,
% andA.Acero,â€œHi
% gh-performance
% HMM adaptation with joint compensation of additive and convolutive
% distortions via vector Taylor series,â€ in
% Proc. ASRU
% , 2007, pp. 65â€“70.
% [132] J.Li,L.Deng,D.Yu,Y.Gong,andA.Acero,â€œAuni
% fi
% ed framework of
% HMM adaptation with joint compensation of additive and convolutive
% distortions,â€
% Comput., Speech, Lang.
% , vol. 23, no. 3, pp. 389â€“405, 2009.






 




% For noise-robust ASR utilizing multiple microphones, refer to any of the following:
% T.Virtanen,R.Singh,andB.Raj
% , Techniques for noise robustness in
% automatic speech recognition
% .  New York, NY, USA: Wiley, 2012. OR
% 
% [31] S. Makino, T.-W. Lee, and H. Sawada
% , Blind Speech Separation
% .
% New York, NY, USA: Springer, 2007.
% [32] J. Benesty, M. M. Sondhi, and Y. Huang
% , Springer Handbook of Speech
% Processing
% .  New York, NY, USA: Springer, 2007.
% [33] P. A. Naylor and N. D. Gaubitch
% , Speech Dereverberation
% .New
% York, NY, USA: Springer, 2010.
% 
% ALSO
% 
% [41] T. Yoshioka and T. Nakatani, â€œNoise model transfer: Novel approach
% to robustness against nonstationary noise,â€
% IEEE Trans. Audio, Speech,
% Lang. Process.
% , vol. 21, no. 10, pp. 2182â€“2192, Oct. 2013
% 
% AND
% 
% [42] M.Souden,S.Araki,K.Kinoshita,T.Nakatani,andH.Sawada,â€œA
% multichannel mmse-based framework for speech source separation and
% noise reduction,â€
% IEEE Trans. Audio, Speech, Lang. Process.
% ,vol.21,
% no. 9, pp. 1913â€“1928, Sep. 2013.





\section{Experiment 3: ASR of Ear-Recorded and Noisy Mouth-Recorded Speech}\label{expt3}

While there are many proposed techniques, discussed in Section \ref{chap4:background}, that have been used to modify the acoustic features of noisy speech, or to modify the acoustic model to compensate for noise, noise-robust ASR is still imperfect and requires additional advances to ASR technology (\cite{zhang:17}).  This particular study proposes the new technique of using speech recorded from the inside of the ear canal.  This would be classified as a feature space modification in the temporal domain, prior to any processing.  Rather than using significant computation to achieve the noise reduction, this study employs purely passive mechanisms (ie. tissues in the head, earplug, earmuffs) to reduce noise.  

As described previously in Chapter \ref{chapter2}, very simple signal enhancement techniques (ie. pre-emphasis and low-pass filtering) are then applied to the recorded signal to produce an enhanced signal with relatively little noise and one that is very similar (below 2.7 kHz) to what could be recorded at the mouth in absence of noise.  The author hypothesizes that retraining a model on speech with this bandwidth will provide more accurate WERs than speech in noise.

\subsection{Stimuli}
\label{chap4:methods:stimuli}

Recordings from twenty speakers, ten male and ten female, from the data collection experiment in Chapter \ref{chapter2} comprised the test data for this experiment.  This included 30 distinct sentences from each speaker, each with 5 different noise conditions (bus, cafe, pedestrian, street, factory) with 3 different noise levels (60dB, 70dB, 80dB), plus an additional `clean' (no noise) condition.  This resulted in 16 iterations of each distinct sentence (30), for each microphone location (2), which resulted in 960 utterances for each of 20 speakers, totaling 19200 test utterances.  There were 9600 ear-recorded utterances (6 hours, 55 minutes)\footnote{Time estimates are calculated from an average utterance duration of approximately 2.6 seconds}, 9000 mouth-recorded noisy utterances (6 hours, 30 minutes), and 600 mouth-recorded clean utterances (26 minutes).

One additional dataset was used, which has undergone the `F0' transform described in Chapter \ref{chapter3}, Section \ref{F0-methods}.  This took the low-frequency ear-recorded signal (0 - 2.5 kHz low-pass filter with 500 Hz slope) and combined it with the high-frequency contents of the mouth-recorded signal (3.0 - 8.0 kHz band-pass filter with 500Hz slope).  This was transformation was performed for each of the 9600 ear-recorded utterances.  This results in full-bandwidth information with the addition of the high-frequency components from the mouth in noise.
%For these two datasets, each contained two speakers, one male and one female.  The dataset of speech combined from ear- and mouth-recorded signals contains 80 distinct utterances, each repeated 5 times (the `clean' speech condition was removed) by each of 2 speakers, totaling 800 utterances.  The dataset of speech recorded at the mouth in noisy conditions contains 80 distinct utterances, each repeated 5 times (again, the `clean' speech condition was removed) by each of 2 speakers, totaling 800 utterances.  These datasets were recorded simultaneously at the ear and at the mouth, from the same two speakers.

\subsection{Design and Procedure}
\label{chap4:methods:design}

An existing deep neural network (DNN) acoustic model trained on 960 hours of speech from the LibriSpeech corpus (published and described in \cite{panayotov:15}) was used to test the collected data.  As described in \cite{panayotov:15}, the 960 hours of data, collected from the audio transcripts of books from the LibriVox website, was divided into two groups - `clean' and `other'.  This designation was chosen by preliminarily running an acoustic model trained on Wall Street Journal (WSJ; \cite{paul:92}) data on the LibriSpeech utterances. The set was split down the middle, with the half containing lower WERs designated as the `clean' set, and those with higher WERs designated as the `other' set.  Data from both sets were used in training the LibriSpeech acoustic model.  The provided LibriSpeech recipe in the Kaldi distribution was used, including the default use of MFCC features (without energy, but with deltas and delta-deltas), CMVN (cf. Section \ref{chap4:background}), etc.

Additionally, the ASR setup utilized the language models from \cite{panayotov:15}, trained on the LibriSpeech corpus.  To verify the current set-up of the acoustic and language models, the author performed a replication of \cite{panayotov:15} using LibriSpeech's `test-clean' and `test-other' datasets.  Afterwards, the same acoustic and language models were used to recognize the speech data collected for this study described in Chapter \ref{chapter2}. This primarily tested the performance between the ear-recorded and noisy mouth-recorded speech, but also between the different noise conditions and noise levels.  
%The author also tested the data collected for, and described in Chapter \ref{chapter3}, containing very noisy mouth-recorded data and the data combination of mouth- and ear-recorded speech.
It is likely that, due to the ear-recorded speech only containing information below 3 kHz, that the existing acoustic model in its current state will have poor recognition of these sentences, and quite possibly will not outperform the noisy speech using the LibriSpeech models, particularly due to the relatively high SNR noisy speech.

A common method of improving ASR performance on speech with different characteristics (eg. in noise, data with a limited frequency-band, etc.) is to train the ASR acoustic model on speech in that domain.  This also mimics the process utilized by the human auditory system to `adapt' its `acoustic model' to better recognize an adverse condition using training; this technique was used in the human speech perception task described in Chapter \ref{chapter3}.  However, rather than recording enough ear-recorded speech to train a model, the 100-hour `train-clean' subset of the LibriSpeech corpus was used as training data.  The speech in this dataset was low-passed with the exact same filter used on the ear-recorded speech - 2.5 kHz with a 500 Hz slope.  A speaker-adapted DNN-HMM model was trained on this data, using the `nnet5a' recipe in Kaldi's LibriSpeech distribution.  Due to computational constraints, only the 100-hour (not the 960-hour) dataset was used.

% Furthermore, the ear-recorded stimuli were modified for a final test that aimed to compensate for the loss of higher-frequency features.  This combined the pre-emphasized, ear-recorded speech, which is low-pass filtered to 2.5 kHz with a 500 Hz slope, with the simultaneous speech recorded at the mouth, bandpass filtered from 3.0 kHz to 8.0 kHz, with a 500 Hz slope in either direction\footnote{The mouth-recorded speech was already low-pass filtered to 8.0 kHz.}.  
% This same transformation was performed for a follow-up human speech perception task in Chapter \ref{chapter3}.

%Due to this assumption, 


%Both the ear-recorded and noisy mouth data will then be enhanced using the well-established advanced front end (AFE, \cite{etsi:02}) technique, and will be retested on the same, unchanged acoustic model.

%A common practice in ASR research is to use a relatively small subset of data to adapt an existing acoustic model to better fit the style of data currently being used.  As additional training was used on a subset of human listeners in the experiment in Chapter 3\ref{chapter3} Section \ref{}, this training will be used in an attempt to modify the acoustic model to better recognize the ear-recorded speech.  The same DNN LibriSpeech acoustic model will be adapted with ear-recorded, low-pass filtered additional utterances (not from the 30 test sentences, nor from any of the speakers being tested).  A total of 50 distinct sentences repeated in 5 noise conditions from 4 additional speakers were used for adaptation of the acoustic model, totalling 1000 utterances used for adaptation.  The total duration of these utterances combined is approximately 44 minutes. The same 19200 utterances from the same 20 speakers were used again as test data.



\subsection{Results}
\label{chap4:results}

The previously-trained open-source acoustic and language models (cf. Section \ref{chap4:methods:design}) were used to recognize the LibriSpeech corpus test data in order to verify that the Kaldi ASR system was configured correctly.  Table \ref{tab:sanity-check} demonstrates that accuracy near the published results (cf. \cite{panayotov:15}) - using the same models and data - was achieved, and the Kaldi set-up was verified.

% Replication using LibriSpeech AM and LibriSpeech test data.
\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c |} \hline
Language Models & Clean Publ. & Clean Repl. & Other Publ. & Other Repl. \\ \hline\hline
3-gram, thresh. 3e-7 & 8.02 & 8.12 & 19.41 & 19.51 \\ \hline
3-gram, thresh. 1e-7 & 7.21 & 7.30 & 17.66 & 17.67 \\ \hline
3-gram, unpruned & 5.74 & 5.80 & 14.77 & 14.74 \\ \hline
4-gram, unpruned & 5.51 & 5.56 & 13.97 & 13.91 \\ \hline
\end{tabular}
\end{center}
\caption{All values are given as \% WER.  The `Language Models' column specifies the Language Model (LM) used in each row; the first two LMs are simply the 3-gram model which was pruned to the specified threshold.  `Publ.' columns list the performance listed in the published paper, and `Repl.' columns contain the replication performance achieved in the present study.  `Clean' and `Other' refer to the clean (2707 utterances) and `noisy' (5968 utterances) LibriSpeech test datasets. All LMs utilize the 960-hour LibriSpeech DNN acoustic model.}\label{tab:sanity-check}
\end{table}

The author then used data collected in the present study, described in Chapter \ref{chapter2}, with the same LibriSpeech acoustic and language models.  All references to `ear-recorded speech' in this chapter refer specifically to the processed (pre-emphasized, low-pass filtered, pre-emphasized) ear-recorded speech.  Table \ref{tab:basic-run} shows these results.  This table combines all noisy speech at the mouth into a single `Noisy Mouth' category and all speech collected at the ear into a single `Ear Speech' category.  Table \ref{tab:split-wer-noise} separates out the noisy mouth-recorded speech and ear-recorded speech into the different noise types and noise levels.

% All 20 subjs; combined results for clean MRS, noisy MRS, and ERS on LibriSpeech data.
\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c |} \hline
Language Models & Clean Mouth & Noisy Mouth & Ear Speech \\ \hline\hline
3-gram, prune thresh. 3e-7 & 14.59 & 47.54 & 79.16 \\ \hline
3-gram, prune thresh. 1e-7 & 12.75 & 45.36 & 78.57 \\ \hline
3-gram, unpruned & 8.45 & 41.40 & 76.88 \\ \hline
4-gram, unpruned & 8.33 & 41.29 & 76.86 \\ \hline
\end{tabular}
\end{center}
\caption{All values are given as \% WER. The `Language Models' column specifies the Language Model (LM) used in each row; the first two LMs are simply the 3-gram model which was pruned to the specified threshold.  `Clean Mouth' includes only the sentences recorded at the mouth with no noise, versus `Noisy Mouth', which includes all other [noisy] sentences recorded at the mouth. `Ear Speech' contains all ear-recorded utterances.  All LMs utilize the 960-hour LibriSpeech DNN acoustic model.}\label{tab:basic-run}
\end{table}

% A simple comparison of clean mouth-recorded speech and the simultaneous (clean) ear-recorded speech is located in Table \ref{tab:clean-wers}.
% 
% \begin{table}[h]
% \begin{center}
% \begin{tabular}{| c | c |} \hline
%  Mouth-Recorded, Clean & Ear-Recorded, Clean \\ \hline\hline
%  9.38 & 79.30 \\ \hline
% \end{tabular}
% \end{center}
% \caption{All values are given as WER.  A comparison of ASR performance on mouth-recorded speech and ear-recorded speech with no background noise. These values are obtained from the highest-performing (4-gram) language model, utilizing the 960-hour LibriSpeech DNN acoustic model - both trained using the LibriSpeech corpus.}\label{tab:clean-wers}
% \end{table}



% All 20 subjs; Normal LibriSpeech AM on full-bandwidth MRS and limited-bandwidth ERS
\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
      & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
      & Mth & Ear & Mth & Ear & Mth & Ear & Mth & Ear & Mth & Ear \\ \hline\hline
60 dB & 16.6 & 75.5 & 15.9 & 76.3 & 14.9 & 76.5 & 16.4 & 74.6 & 14.6 & 75.9  \\ \hline
70 dB & 35.5 & 76.8 & 27.0 & 75.0 & 27.3 & 76.4 & 32.3 & 75.3 & 24.7 & 76.3  \\ \hline
80 dB & 89.5 & 79.2 & 74.3 & 78.1 & 75.1 & 79.1 & 83.9 & 80.7 & 70.7 & 79.3 \\ \hline
\end{tabular}
\end{center}
\caption{All values are given as \% WER. These values are obtained from the highest-performing (4-gram) language model, utilizing the 960-hour LibriSpeech DNN acoustic model - both trained using the LibriSpeech corpus.  Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type), with each sub-column comparing the performance on mouth-recorded speech in the indicated condition with ear-recorded speech in the indicated condition.}\label{tab:split-wer-noise}
\end{table}

% The experimenter performed another test using two sets of data created in Chapter \ref{chapter3}.  One includes the very noisy speech collected for the experiment in Chapter \ref{chapter3}.  This speech was re-recorded in a way that resulted in a lower SNR; for more details, refer to Section \ref{chap3:methods:stimuli}.    
% 
% 
% \begin{table}[h]
% \begin{center}
% \begin{tabular}{| c || c | c |} \hline
% Language Models & Ear/Mouth Combined & Extra Noisy Mouth \\ \hline\hline
% 3-gram, prune thresh. 3e-7 & 85.18 & 98.32 \\ \hline
% 3-gram, prune thresh. 1e-7 & 84.66 & 99.14 \\ \hline
% 3-gram, unpruned & 84.04 & 99.03 \\ \hline
% 4-gram, unpruned & 83.90 & 99.41 \\ \hline
% \end{tabular}
% \end{center}
% \caption{The `Language Models' column specifies the Language Model (LM) used in each row; two LMs are simply the 3-gram model which was pruned to the specified threshold.  The `Ear/Mouth Combined' column contains the results from the dataset using speech reconstructed from low-frequency ear-signal and high frequency mouth-signal parts.  The `Extra Noisy Mouth' column contains results from the re-recorded mouth speech (described in Section \ref{chap3:methods:stimuli}), with a lower SNR.  Note that the data used in these tests are different than those used in previous tests displayed in Tables \ref{tab:basic-run} and \ref{tab:split-wer-noise}. All LMs utilize the 960-hour LibriSpeech DNN acoustic model.  All values are given as WER.}\label{tab:follow-up-asr}
% \end{table}


Another test was performed which manipulated the ear-recorded signal by adding in high-frequency information from a bandpass filtered mouth recorded signal (3.0-8.0 kHz with a 500Hz slope).  This `hybrid' signal was tested using the same DNN acoustic model trained on 960-hour LibriSpeech data.  The results are presented in Table \ref{tab:hybrid-wers} alongside the noisy mouth-recorded speech for comparison (the mouth-recorded results already appeared in Table \ref{tab:split-wer-noise}).  The hybrid ear-recorded data is labeled as `E/M'.

% All 20 subjs; LibriSpeech AM with full-bandwidth MRS, and Hybrid E/M speech
\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
      & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
      & Mth & E/M & Mth & E/M & Mth & E/M & Mth & E/M & Mth & E/M \\ \hline\hline
60 dB & 16.6 & 64.5 & 15.9 & 63.9 & 14.9 & 63.2 & 16.4 & 62.3 & 14.6 & 63.8  \\ \hline
70 dB & 35.5 & 66.6 & 27.0 & 62.6 & 27.3 & 64.3 & 32.3 & 62.9 & 24.7 & 63.9  \\ \hline
80 dB & 89.5 & 82.4 & 74.3 & 72.5 & 75.1 & 73.5 & 83.9 & 77.1 & 70.7 & 73.0  \\ \hline
\end{tabular}
\end{center}
\caption{All values are given as \% WER. These values are obtained from the highest-performing (4-gram) language model, utilizing the 960-hour LibriSpeech DNN acoustic model - both trained using the LibriSpeech corpus.  Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type), with each sub-column listing the performance of mouth-recorded speech (Mth) or `hybrid' ear-recorded speech (E/M) in the indicated condition.}\label{tab:hybrid-wers}
\end{table}
% \begin{table}[h]
% \begin{center}
% \begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
%       & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
%       & Mth & E/M & Mth & E/M & Mth & E/M & Mth & E/M & Mth & E/M \\ \hline\hline
% 60 dB & 21.5 & 52.77 & 20.3 & 51.16 & 18.6 & 52.23 & 19.0 & 51.38 & 17.9 & 51.82  \\ \hline
% 70 dB & 41.7 & 58.80 & 32.9 & 54.30 & 32.0 & 55.10 & 36.4 & 56.16 & 29.9 & 53.49  \\ \hline
% 80 dB & 88.2 & 78.78 & 73.3 & 70.04 & 75.6 & 71.82 & 85.0 & 74.21 & 71.4 & 71.51  \\ \hline
% \end{tabular}
% \end{center}
% \caption{These values are obtained from the highest-performing (4-gram) language model, utilizing the 960-hour LibriSpeech DNN acoustic model - both trained using the LibriSpeech corpus.  Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type), with each sub-column listing the performance of mouth-recorded speech (Mth) or `hybrid' ear-recorded speech (E/M) in the same condition.  All values are given as WER.}\label{tab:hybrid-wers}
% \end{table}


A new model was trained using the 100-hour `clean' portion of the LibriSpeech training corpus.  This [mouth-recorded] LibriSpeech data was low-pass filtered at 2.5 kHz with a 500 Hz slope - the same filtering that is applied to the ear-recorded speech.  A DNN acoustic model was trained using the LibriSpeech training recipe for Kaldi. The basic performance of this model on the pre-emphasized and filtered ear-recorded speech is given in Table \ref{tab:retrainedDNN}.

% All 20 main subjs, LS-lowpass with ERS, and normal LibriSpeech with full-bandwidth MRS
\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
      & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
      & Mth & Ear & Mth & Ear & Mth & Ear & Mth & Ear & Mth & Ear \\ \hline\hline
60 dB & 20.3 & 58.1 & 18.7 & 57.4 & 17.8 & 58.8 & 19.8 & 56.5 & 17.9 & 58.2  \\ \hline
70 dB & 43.3 & 60.4 & 33.6 & 58.7 & 32.8 & 59.2 & 36.8 & 59.4 & 32.2 & 59.0  \\ \hline
80 dB & 91.5 & 69.9 & 77.4 & 66.1 & 78.8 & 68.8 & 85.6 & 69.8 & 76.2 & 65.8  \\ \hline
\end{tabular}
\end{center}
\caption{All values are in \% WER. The performance obtained for the ear-recorded speech is from using the acoustic model trained on 100-hours of LibriSpeech mouth-recorded speech which had been low-pass filtered to 2.5 kHz with a 500 Hz slope; all results are from the use of the (4-gram) language model.  The mouth-recorded speech was tested on LibriSpeech's 100-hour DNN acoustic model.  Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type), with each sub-column contrasting the performance on mouth-recorded speech (Mth; using the original 100-hour LibriSpeech DNN acoustic model on full-bandwidth data) with ear-recorded speech (Ear; using the newly trained acoustic model).}\label{tab:retrainedDNN}
\end{table}

To offer a more accurate comparison of the improvement offered by low-passed training data recorded at the mouth, the existing 100-hour DNN model trained on [full-bandwidth] LibriSpeech data was used\footnote{The 100-hour, full-bandwidth DNN model was used, as opposed to the previously used 960-hour DNN model, the results of which can be seen in Table \ref{tab:split-wer-noise}} to also decode the ear-recorded speech.  These results of the ear-recorded speech tested on the full-bandwidth 100-hour DNN model can be seen in Table \ref{tab:100HrDNNcmp} compared with the ear-recorded speech tested on the limited-bandwidth 100-hour DNN model.  Column `LS' indicates the ear-recorded speech tested on the full-bandwidth, 100-hour LibriSpeech DNN acoustic model, and `LLS' indicates the ear-recorded speech tested on the 100-hour, Low-passed LibriSpeech DNN acoustic model.

% All 20 main subjs, 100-hr LS-lowpass with ERS, and normal 100-hour LibriSpeech with ERS 
\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
      & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
      & LS & LLS & LS & LLS & LS & LLS & LS & LLS & LS & LLS \\ \hline\hline
60 dB & 79.0 & 58.1 & 80.0 & 57.4 & 79.8 & 58.8 & 79.0 & 56.5 & 80.0 & 58.2  \\ \hline
70 dB & 79.4 & 60.4 & 79.0 & 58.7 & 80.6 & 59.2 & 79.4 & 59.4 & 80.2 & 59.0  \\ \hline
80 dB & 82.4 & 69.9 & 82.2 & 66.1 & 83.6 & 68.9 & 84.4 & 69.8 & 83.4 & 65.8  \\ \hline
\end{tabular}
\end{center}
\caption{All values are in \% WER. The performance obtained is from testing ear-recorded speech on the 100-hour LibriSpeech DNN acoustic model (`LS'), and testing ear-recorded speech the acoustic model trained on 100-hours of LibriSpeech data which had been low-pass filtered to 2.5 kHz with a 500 Hz slope (`LLS').  The results are from the 4-gram language model.  Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type).}\label{tab:100HrDNNcmp}
\end{table}

To determine if an acoustic model trained on low-pass filtered mouth-recorded speech is able to better recognize low-passed mouth-recorded speech with greater accuracy than ear-recorded speech, another test was performed.  This filtered the noisy mouth-recorded speech\footnote{This was first tested on the 960-hour DNN model; results can be found in Table \ref{tab:split-wer-noise}.} from the data collection in Chapter \ref{chapter2} using a 2.5 kHz low-pass filter with a 500 Hz slope - the same filter used on both the ear-recorded test data and the mouth-recorded speech used to train the Low-passed LibriSpeech acoustic model.  These results, compared with the ear-recorded results tested on the same Low-passed LibriSpeech model, can be found in Table \ref{tab:lp-ers-mrs}.

The results of testing the low-pass filtered mouth-recorded speech on the 100-hour DNN acoustic model trained with low-pass filtered mouth-recorded speech are compared with the full-bandwidth mouth-recorded speech tested on the full-bandwidth 100-hour DNN acoustic model.  This comparison is displayed in Table \ref{tab:lp-mrs-fb-mrs}.

% All 20 main subjs, using the 100-hr LS-lowpass AM, ERS and MRS-filtered comparison
\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
      & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
      & Mth & Ear & Mth & Ear & Mth & Ear & Mth & Ear & Mth & Ear \\ \hline\hline
60 dB & 25.9 & 58.1 & 24.8 & 57.4 & 25.9 & 58.8 & 28.2 & 56.5 & 25.1 & 58.2  \\ \hline
70 dB & 60.8 & 60.4 & 54.7 & 58.7 & 53.5 & 59.2 & 59.4 & 59.4 & 47.5 & 59.0  \\ \hline
80 dB & 93.9 & 69.9 & 88.6 & 66.1 & 90.5 & 68.9 & 93.1 & 69.8 & 87.3 & 65.8  \\ \hline
\end{tabular}
\end{center}
\caption{All values are in \% WER. The performance obtained is from the acoustic model trained on 100-hours of LibriSpeech data which had been low-pass filtered to 2.5 kHz with a 500 Hz slope, using the 4-gram LM. Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type), with each sub-column comparing the performance on ear-recorded speech (Ear) with \textit{low-pass filtered} mouth-recorded speech (Mth). This test was performed to determine if there were a difference between the recognition of filtered mouth-recorded speech and filtered ear-recorded speech - both signals filtered to the same exact frequency bandwith - using the acoustic model which was trained on filtered mouth-recorded speech.}\label{tab:lp-ers-mrs}
\end{table}



% All 20 main subjs, using the 100-hr LS-lowpass AM with MRS-filt, and normal 100-hour LibriSpeech with MRS  ***********LS 100-STANDARD NOISY MOUTH TEST***********
\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
      & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
      & LS & LLS & LS & LLS & LS & LLS & LS & LLS & LS & LLS \\ \hline\hline
60 dB & 20.3 & 25.9 & 18.7 & 24.8 & 17.8 & 25.9 & 19.8 & 28.2 & 17.9 & 25.1  \\ \hline
70 dB & 43.3 & 60.8 & 33.6 & 54.7 & 32.2 & 53.5 & 36.8 & 59.4 & 32.2 & 47.5  \\ \hline
80 dB & 91.5 & 93.9 & 77.4 & 88.6 & 78.8 & 90.5 & 85.6 & 93.1 & 76.2 & 87.3  \\ \hline
\end{tabular}
\end{center}
\caption{All values are in \% WER. The performance obtained is from testing mouth-recorded speech on the 100-hour LibriSpeech DNN acoustic model (`LS'), and testing low-passed mouth-recorded speech on the acoustic model trained on 100-hours of LibriSpeech data which had been low-pass filtered to 2.5 kHz with a 500 Hz slope (`LLS').  The results are from the 4-gram language model.  Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type).}\label{tab:lp-mrs-fb-mrs}
\end{table}

For comparison, both the LibriSpeech and Low-pass LibriSpeech models were used on the clean ear-recorded and mouth-recorded speech signals.  These results can be seen in Table \ref{tab:final-clean-speech}.

\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c |} \hline
Language Models & Ear-Clean & Mouth-Clean & Low-pass Mouth-Clean \\ \hline\hline
LS & 78.06 & 9.44 & ---- \\ \hline
LLS & 54.03 & ---- & 18.45 \\ \hline
\end{tabular}
\end{center}
\caption{All values are in \% WER.  Each column refers to the test data which was used, the rows correspond to the Acoustic Model that was used: `LS' for the 100-hour DNN LibriSpeech model and `LLS' for 100-hour DNN Low-passed LibriSpeech model. All presented results are from the use of the 4-gram Language Model.}\label{tab:final-clean-speech}
\end{table}

%These tables demonstrate that, in all but the noisiest conditions, an ASR system will more easily recognize noisy speech than it will ear-recorded speech.  


%A post hoc test was performed in which the little available (approximately 7-hours) ear-recorded data was used to train a GMM-HMM model.  A DNN was not trained as there was not enough speech data to create an accurate model.  Again, using the LibriSpeech recipe in Kaldi, a smaller model was trained on the ear-recorded test data described above (and in Chapter \ref{chapter2}).  This model uses the settings for the `\begin{verbatim}tri3b\end{verbatim}' model in the LibriSpeech recipe, which limits the number of total decision tree leaves\footnote{The decision trees are involved in the process of `state tying' mentioned briefly in Section \ref{sec:acoust-mod}} to 2500 and the total number of Gaussians across all leaves to 15000.  The test data consisted of approximately 500 additional ear-recorded sentences. These test sentences were distinct from any of the training sentences, and were spoken by different speakers.  The results can be seen in Table \ref{tab:ear-training}.

% % Subj 72&73 on ear-trained AM, normal LibriSpeech, and LS-lowpass
% \begin{table}[h]
% \begin{center}
% \begin{tabular}{| c || c | c | c |} \hline
% Language Models & Ear-trained & LibriSpeech & LS-lowpass \\ \hline\hline
% 3-gram, prune thresh. 3e-7 & 71.45 & 86.03 & 65.29  \\ \hline
% 3-gram, prune thresh. 1e-7 & 70.26 & 86.58 & 62.26 \\ \hline
% 3-gram, unpruned & 70.66 & 86.39 & 60.13 \\ \hline
% 4-gram, unpruned & 70.00 & 86.76 & 60.21 \\ \hline
% \end{tabular}
% \end{center}
% \caption{All values are given as WER. The `Language Models' column specifies the Language Model (LM) used in each row; two LMs are simply the 3-gram model which was pruned to the specified threshold.  `Ear-trained' includes the average WER of the 500 sentences using the 7-hour GMM model trained on ear-recorded speech, versus `LibriSpeech', which includes the average WER of the 500 sentences using the 960-hour DNN model trained on full-bandwidth speech.  Additionally, these 500 sentences were run on the model trained on 75-hours of low-pass filtered LibriSpeech training data; the results of this test are in the column `LS-lowpass'.}\label{tab:ear-training}
% \end{table}




\section{Discussion}
\label{chap4:discussion}

% The results indicate, broadly, that the ear-recorded speech (which has been pre-emphasized and low-pass filtered $<$ 2.7 kHz) performs, at best, minimally better than mouth-recorded speech only in the bus noise condition with 80 dB background noise (cf. Table \ref{tab:split-wer-noise}) - but does not outperform noisy mouth-recorded speech in any of the other conditions.  Performing better than other conditions achieving 80+\% WER does not offer much in terms of benefit, as there is minimal recognition happening in either case.  This is clear in Table \ref{tab:basic-run}, noting how little improvement is seen when more extensive language models are used.  At this level of recognition, the acoustic model has not provided the language model with sufficient data for it to accurately model language.
% 
% The ear-recorded speech falls well below the performance of the ASR system on the speech in the 60- and 70- dB conditions.  The 60 dB condition demonstrates that the SNR in this condition was very high, noting that its performance was only 10\% WER short of the clean-speech condition.  The lack of other noise-reducing measures applied to the noisy speech is yet another reason why the ear-recorded speech falls behind; if other noise-reduction transforms, such as AFE, were to be applied, the noisy speech would be even more easily recognizable by an ASR system.
% 
% The results provided in Table \ref{tab:split-wer-noise} demonstrate, as did the results of the human speech perception experiment in Chapter \ref{chapter3}, that noise does reach the in-ear microphone.  For each consecutively louder noise level, performance of ear-recorded speech drops slightly, though not nearly as significantly as the noisy mouth recorded speech.  These WERs are promising in that environmental noise itself does not appear to greatly affect recognition of ear-recorded speech.  If a method were developed to substantially increase the WER of speech recorded from the ear, it is likely that this recording location would be very noise robust.

As expected and predicted by \cite{morales:05b}, testing limited-bandwidth speech data (ear-recorded speech) with an acoustic model trained on full-bandwidth speech does not yield good results.  In Table \ref{tab:split-wer-noise}, there are only two noise types (bus, street) in which ear-recorded speech manages to outperform the noisy mouth-recorded speech.  This is also prior to any application of noise-reduction techniques to the noisy-mouth recorded speech; performance on the latter would be expected to improve further after this application, and these limited benefits of ear-recorded speech would be expected to disappear.

% Discuss Hybrid speech

In an effort to remedy the poor performance of the ear-recorded speech using the acoustic model trained on full-bandwidth speech, `hybrid' stimuli were created in the same manner seen in Section \ref{F0-methods} for the human perception study.  The newly-created stimuli took the low-passed ear-recorded speech and the high-frequency bandpassed mouth-recorded speech (3.0 - 8.0 kHz with a 500 Hz slope) and combined the two signals into a mono-channel signal.  This achieves slightly better WERs in most conditions (cf. Table \ref{tab:hybrid-wers}).  In the 80 dB noise condition, the accuracy of this `hybrid' ear-recorded speech manages to out-perform mouth-recorded speech in 4/5 noise types.  

This was done to `reconstruct' the high-frequency information, allowing the acoustic model trained on full-bandwidth data to access the higher frequency information that it expects.  As stated before, this offers a mild improvement to the WER of ear-recorded speech (compare ear-recorded speech in Table \ref{tab:split-wer-noise} with hybrid speech in Table \ref{tab:hybrid-wers}), but, with the exception of the 80 dB noise condition, this method does not improve the only mildly noisy signals recorded with 60 and 70 dB noise.  This is likely due to the inconsistency of noise across the frequency spectrum when the relatively clean low frequencies of the ear-recorded speech are combined with the somewhat noisy high frequencies of the mouth-recorded speech.  The human auditory system does not appear to suffer from this inconsistency (cf. Section \ref{chap3:glob_discussion}), but the current ASR model is not equipped to deal with it.

In addition to the discontinuities caused by the different levels of noise in each of the conjoined `hybrid' signals, it is also possible that there also may be differences between the ear- and mouth-recorded speech below 2.5 kHz that are significant enough to throw off performance.  That is to say, it may not solely be the limited-bandwidth aspect of ear-recorded speech that poses a challenge for acoustic models that are trained on full-bandwidth speech.  The filtering effect of the head may alter the ear-recorded signal below 2.5 kHz in such a way that mouth-recorded speech will not provide adequate training data.

% When compared with the WER of the `normal' ear-recorded speech, the `hybrid' ear-recorded speech offers a reduction of over 10\% WER in most instances noise conditions, even with the simultaneous addition of 80 dB background noise in the higher frequencies. This is important to note, as it is evidence that the acoustic model relies partially on information in the higher frequencies.  The `hybrid' ear-recorded speech results in the 70 and 60 dB noise conditions makes this even more apparent.  In the 60 dB noise condition, the `hybrid' stimuli offer a nearly 30\% WER drop over the `normal' ear-recorded signals in the same condition.  However, in these lower noise level conditions (60 dB, 70 dB), the noisy mouth-recorded speech still obtains a substantially better WER than the `hybrid' speech.  

% This is remarkable, as, in theory, the ear-recorded speech is comparable to the mouth-recorded speech below 2.5 kHz.  If this were strictly the case, the `hybrid' ear-recorded speech should outperform the noisy mouth-recorded speech in every noise condition.  Yet what is observed is that, despite the lower frequencies being `cleaned' by the substitution of ear-recorded speech, recognition rates drop.  The `hybrid' signal is composed of (a) the low-frequency `clean' speech information recorded at the ear and (b) the high-frequency information in noise recorded at the mouth.  Since component (b) obtains a lower WER when combined with noisier low-frequencies recorded at the mouth, this means that component (a) must be causing the harm to performance.  If component (a) were simply a `cleaned' version of the noisy mouth-recorded lower frequencies, one would not expect a decrease in performance when the noisy portion is substituted.
% 
% This supports the theory mentioned earlier that using low-pass filtered mouth-recorded speech to train a model for ear-recorded speech recognition may not be sufficient, as there appears to be differences significant enough to seriously affect the ability of an acoustic model trained on the former to accurately recognize information from the latter.  From the results of the human speech perception experiment in Chapter \ref{chapter3}, we know that humans are able to understand ear-recorded speech with a comparatively high degree of accuracy, and future research may discover methods to further improve this accuracy.  This means that speech information is indeed present in the ear-recorded speech signals, and it is then a matter of training an ASR system to find it.

% Discuss training model

A new acoustic model was trained on the the 100-hour `clean' LibriSpeech data corpus.  The data in this corpus was recorded from the mouth, but was low-pass filtered to 2.5 kHz with a 500 Hz slope by the researcher.  This is the exact same low-pass transform applied to the ear-recorded speech.  This low-passed mouth-recorded speech was used to train a 100-hour DNN. It was used to test both ear-recorded speech and low-pass filtered mouth-recorded speech.  Looking first at Table \ref{tab:100HrDNNcmp}, the limited-bandwidth acoustic model does offer improvement over the full-bandwidth model with the same specifications\footnote{That is, trained on same 100 hours of speech, with the same ASR parameters. The only difference is that one model uses limited-bandwidth data} when recognizing ear-recorded speech, in addition to the `hybrid' feature compensation described above (cf. Table \ref{tab:hybrid-wers}).  There is a greater amount of variability - that is a greater effect of noise level - when using the Low-pass LibriSpeech acoustic model.  Regardless, this new model does not yield improvement over the noisy mouth-recorded speech in the 60 and 70 dB noise conditions tested on the full-bandwidth 100-hour LibriSpeech dataset (cf. Table \ref{tab:retrainedDNN}).

Low-pass filtered mouth-recorded speech was also tested on this model.  Unlike the ear-recorded speech, the low-passed mouth-recorded speech achieved much better accuracy (cf. Table \ref{tab:lp-ers-mrs}) in the low (60 dB) SNR condition.  In this category, performance was within 10\% WER to the full-bandwidth mouth-recorded test data that utilized the 100-hour full-bandwidth model (cf. Table \ref{tab:lp-mrs-fb-mrs}). A slightly greater divergence is noted in the higher 70- and 80 dB noise level conditions.  The performance with the 60 dB `low-noise' level is quite remarkable, as it indicates that enough critical speech information is located in these very low frequencies that - when noise is largely absent - allows for accuracy comparable to full-bandwidth speech.  Additionally, the approximate 10\% WER divergence holds between full- and limited-bandwidth mouth-recorded speech tested on the full- and limited-bandwidth models in the `clean' condition (cf. Table \ref{tab:final-clean-speech}).

The drastic divergence between the WER of ear-recorded speech and filtered mouth-recorded speech supports the hypothesis that ear-recorded speech (0-2.5 kHz) is different enough from mouth-recorded speech in the same frequency range that combining one with the other (`hybrid' speech) - or using data from one to train a model that tests on the other - may not provide adequate performance.


\subsection{Future Research}
\label{chap4:future-research}

The successful experiment on low-noise, low-pass filtered mouth-recorded speech is promising for the future possibility of accurately utilizing ear-recorded speech for ASR tasks.  It indicates that acoustic models trained on limited-bandwidth data in the low frequencies are comparable to full-bandwidth models.  Unfortunately, this success does not cross over to ear-recorded speech when using an acoustic model trained on mouth-recorded speech, regardless of the available frequency bandwidths of the training data. It appears that mouth-recorded training data is not transferable to ear-recorded testing data.  

It is possible that feature compensation methods, such as those described by \cite{morales:09}, may be able to bridge the difference between ear-recorded and mouth-recorded speech such that acoustic models trained on mouth-recorded speech might yield better performance on ear-recorded speech.
The author also hypothesizes that if an acoustic model were trained specifically on ear-recorded speech, performance similar to the low-pass filtered mouth-recorded speech could be obtained.  The next steps for additional research in this domain would be to collect enough ear-recorded speech data (the current corpus only has approximately 7 annotated hours) such that a sufficient acoustic model could be trained or adapted from this data.


% The low-pass filtered ear-recorded data was run using an ASR system trained for full frequency speech on data ranging from 0 to 8 kHz.  The results make it clear that models trained on `normal' speech will not perform well on ear-recorded speech devoid of significant modification.  If future researchers collected enough additional speech from the ear to amass a corpus of adequate size for adapting an existing acoustic model or training a new model, performance would be expected to improve.  The degree to which performance would improve depends on the amount of critical speech information in the higher frequencies that do not make it into the ear-recorded signal.  The author hypothesizes that there would be enough speech information below 2.7 kHz to improve ASR performance with acoustic model adaption or retraining.
% 
% Considering the possibility of using the `hybrid' speech, existing noise-removal methods may be able to utilize the relatively clean low-frequency speech harmonics to `clean' the high-frequency, noisy harmonics.  This would result in a much more uniform signal across the frequency spectrum and likely much lower WERs.

In the ear-recorded signals, one can observe a small amount of noise reaching the microphone and appearing in the signal.  As with the mouth-recorded signals, although the effect is not nearly as stark, WER of ear-recorded signals correlates with the noise in the signal, ie. as noise increases, so does WER.   If performance were to improve significantly, eg. by training an acoustic model on ear-recorded speech, feature enhancement techniques, such as AFE (cf. Section \ref{chap4:background}), could be applied to this signal to eliminate any residual effect of noise.  The ear-recorded signals retain a very high SNR, and the author hypothesizes that these noise removal techniques would work quite well, if overall accuracy could be improved.

% Another potential method to further eliminate any noise that reaches an ear-recorded signal would be to use beamforming on input signals from two different microphones, one in each ear.  This method can utilize the difference in temporal arrival of the same sound source in different microphones to tease apart the multiple sources and identify the speech source; the speech source will arrive at both ear canals at the same time (each canal is the same distance from the vocal tract), whereas noise sources will likely not arrive at both in-ear microphones simultaneously.

This study utilized noise that was by and large stationary in amplitude.  This was intentional, to test the proof of concept and to test the extent of noise reduction the proposed method can accomplish.  Since Chapters \ref{chapter2} and \ref{chapter3} have shown that the noise does not have a dramatic effect on speech recorded from the ear, variations and modulations in the amplitude of the noise (and the SNR of the speech recorded at the mouth) should have little effect on the speech recorded at the ear.  Nevertheless, any future research in this area should incorporate noise that fluctuates in both type and amplitude.  The recent CHiME Challenges (eg. \cite{chime:16}) have incorporated amplitude-varying noise into their task, and similar tests could be performed by collecting another data set of speech recorded from speakers' ears, with amplitude-varying background noise to determine the effect amplitude variance has on ear-recorded speech.

\subsection{Summary}
\label{chap4:summary}

Many methods of noise-reduction have been developed over the last several decades, and much improvement has been seen recently, particularly for signals with a higher SNR (\cite{zhang:17}).  Nevertheless, lower SNR speech, especially that with a negative SNR, still proves to be a challenge for modern ASR systems.  The proposed method of recording the speech from the inside of the ear canal does offer a noise-reduction benefit, but also significantly filters out the higher frequencies within the speech signal.  The results above demonstrate that enough speech information relied upon by the ASR acoustic model is filtered out to substantially reduce ASR recognition performance of ear-recorded speech when using a model trained with full-bandwidth data.  The tests also indicate that the low-frequency information in the ear-recorded signal may also differ enough from the low-frequency information in the mouth-recorded signal to render models trained with mouth-recorded speech relatively ineffective.  

However, testing limited-bandwidth mouth-recorded speech on an acoustic model trained with limited-bandwidth mouth-recorded speech offers significant benefits.  These benefits occur despite the heavy filtering that the signal has undergone.  Extrapolating these results, it seems likely that a model trained specifically on ear-recorded speech with the same frequency bandwidth would be able to accurately recognize other ear-recorded speech.  It is clear that additional steps are necessary to determine the full extent of benefit that can be achieved by using ear-recorded signals for noise robust ASR systems.

% However, adding low-frequency ear-recorded speech, which is mostly devoid of noise, to the noisy mouth-recorded signal (replacing the noisy low-frequency speech in that signal) does substantially improve performance.  This improvement can be seen prior to the application of any further noise-reduction method or signal enhancement method.

% It is also possible for future research, with substantial ear-recorded data, to adapt or train an acoustic model on that style of speech.  If this could be accomplished with success, adequate recognition could be achieved solely through ear-recorded speech, without the need to use it in combination with the noisy speech signal from the mouth.  


<<Exp_1_Data, echo=FALSE, results='asis'>>=



@

