% %\Sexpr{set_parent(â€˜/Users/mwilli/Documents/Spring_2017/Dissertation_Document/Dissertation_Working_Directory_Draft/Dissertation_Main.Rnw')}
% 
% <<chunk_options, echo=FALSE>>=
% # This is where we set basic knitr options.
% opts_chunk$set(echo=TRUE, message=FALSE, warning = TRUE, cache = FALSE, comment="")
% options(width=75) # This sets how wide the R printout can be.
% @
% 
%  <<setup-child, include=FALSE>>=
%  set_parent('/home/sam/Dissertation/Dissertation_Template/Dissertation_Main_Template/Dissertation_Main.Rnw')
% @
% 
% <<load_libraries, echo=FALSE>>=
% library(tidyr)
% library(dplyr)
% library(ggplot2)
% library(lme4)
% library(lsmeans)
% library(car)
% library(pbkrtest)
% library(xtable)
% library(cowplot)
% library(plyr)
% @



\chapter{Ear-Recorded Speech\label{chapter2}}


\section{Introduction}

The accuracy of automatic speech recognition (ASR) has significantly improved over the past several years; similar, but less dramatic improvements have been made regarding the challenging task of recognizing speech in noisy environments (\cite{zhang:17}).  The performance of the latter still falls below the accuracy that most human listeners achieve.  Even the human auditory system has limits and is only able to remove a certain amount of noise from a speech signal before that signal becomes completely unintelligible (\cite{gilbert:13}).  The primary issue arises from the fact that speech normally passes from a speaker's mouth into either the ear (human speech perception) or a microphone (ASR), but the passage of speech through the free field (air) allows the corruption of the signal by noise of unknown amplitude and unpredictable source.  

This research proposed eliminating, by and large, the passage of speech through the free field by recording the speech in an unconventional location - from the inside of the \textit{speaker's} ear canal.  Speech is not only emitted from the oral cavity, but the vibrations also pass throughout the human body.  Using the ear canal as the source of speech adds the benefit that the entrance to the ear canal can be securely occluded behind the microphone (eg. with a noise reduction device such as an ear-plug).  Ambient noise from the air is largely filtered out by both the occlusion device and the human skull.

Due to the very specific nature of the requirements for this study (namely, that the speech data needed to be recorded from the ear), it was necessary to record data from scratch, as the author was not aware of another corpus of ear-recorded speech that existed at the time of this study.  This corpus aimed to collect speech recorded from a microphone placed at the mouth (in noisy and clean environments), and another microphone placed at the ear (in the same environments). Owing to the variability that occurs when even the same speaker repeats the same sentence, speech critically needed to be recorded from the two locations simultaneously.  This new corpus offers a comparison between these two recording locations, and - by recording the speech from both locations simultaneously - a more accurate comparison.  The goal for this corpus was to be used for testing humans' and ASR ability to understand speech in noise compared with their ability to understand speech recorded from the ear canal.


The purpose of the initial experiment described in Section \ref{expt1} below was to create this corpus of `ear-recorded' speech and `mouth-recorded' speech for use in the following two experiments (cf. Chapters \ref{chapter3} and \ref{chapter4}).  The next section explains the theory behind the assumption that usable speech can be collected from inside one's ear canal.

\section{Background}



% \subsection{Mouth-emitted Speech}
% 
% \begin{wrapfigure}{l}{0.5\textwidth}
% \centering
%   \includegraphics[width=0.5\textwidth]{figure/spctrm5k.png}
%   \caption{Spectrum of the middle of an /I/ vowel.  Each ``spike'' is a separate ``band'' of frequency, called a harmonic.}
%   \label{fig:spctrm5k}
% \end{wrapfigure}
% %
% First, it is appropriate to review the acoustic structure of mouth-emitted speech.  Speech could be divided into two primary categories - voiced and unvoiced.  Voiced speech is composed of bands of acoustic energy, called harmonics, located along a frequency spectrum (cf. fig \ref{fig:spctrm5k}).  
% %
% Certain harmonics will be dampened by the vocal tract, leaving others relatively unfiltered.  A region of harmonics containing more energy are called formants.  The location, shape, and transition over time of these formants (among other more minor features) are what encodes speech information for voiced sounds.  A spectrogram is a graph used to conveniently diagram the amplitude of various frequencies over time (cf. fig \ref{fig:spctgrm1k5k}).
% %
% \begin{figure}[h!]
% \begin{subfigure}{0.95\textwidth}
%   \centering
%   \includegraphics[width=0.95\textwidth]{figure/spctgrm1k.png}
%   \caption{Zoomed to the 0-1kHz range for better visualization of harmonics.}
%   \label{fig:spctgrm1k}
% \end{subfigure}%
% \hfill
% \begin{subfigure}{0.95\textwidth}
%   \centering
%   \includegraphics[width=0.95\textwidth]{figure/spctgrm5k.png}
%   \caption{Zoomed to a more standard 0-5kHz range.}
%   \label{fig:spctgrm5k}
% \end{subfigure}
% \caption{Spectrogram of the word ``citizen'' with phonetic transcription above.}
% \label{fig:spctgrm1k5k}
% \end{figure}
% 
% For unvoiced speech, the information used to recognize and categorize the speech sound is likely found in either the turbulent frication generally centered in higher frequencies cf. fig \ref{fig:spctgrm_s} (although some of the information can be found in lower frequencies), or found in the voiced information in the transitions into and out of the sound (\cite{story:10}).
% %
% \begin{wrapfigure}{l}{0.5\textwidth}
% \centering
%   \includegraphics[width=0.5\textwidth]{figure/spctgrm_s.png}
%   \caption{Spectrum of the initial /s/ in ``citizen''. Zoomed to range of 0-8kHz for visualization of high frequency energy.}
%   \label{fig:spctgrm_s}
% \end{wrapfigure}

% Discussion about Body/Bone conduction (mechanical)
% \subsection{Bone Conduction}

The speech vibrations of a person's own voice propagate throughout the head and body. % (cf. Fig. \ref{fig:bekesyBodyTransfer}).  
Of interest to the present study, these waves pass through the tissue in the head, and enter into the ear canal, where they can be recorded. % This brief section will discuss some of the known properties 

Bone conduction of acoustic vibrations through a human head has been well studied (cf. \cite{allen:60,tonndorf:66,hakansson:94,stenfelt:00,reinfeldt:10}, etc); however most of these studies have involved attaching a mechanical vibration device to an animal head or a cadaver skull, or using a vibrating piston on a live human participant, allowing for precise manipulation of the input signal.  
Many of these early mechanical stimulus studies (most performed on cats or other mammals) demonstrated that the sound generated by bone conduction propagating into the ear canal is dominated by low-frequency noise (\cite{tonndorf:72}).

%The acoustic vibrations resulting from the mechanical device positioned on the head propagate to and are recorded by a recording device on a different location on the head.  
% Most of these studies, as well, are focused on audiometric bone conduction, i.e. the propagation of waves through the head and their effect specifically on the cochlea itself, which is not relevant to the present study.
% 
% %One study, \cite{hakansson}, used ``Live'' human subjects .  Titanium implants (pre-existing) for bone-conduction hearing aids anchored in the temporal bone behind the ear were used as the stimulation point for the mechanical vibrators.
% However, some do contain important findings about the general transmission of vibrations through a skull.
% Many of these early studies which were performed on cats do show that the sound generated by bone conduction propagating into the ear canal is dominated by low-frequency noise (\cite{tonndorf:72}).  Normally, the open ear acts as a high pass filter, dampening these lower frequencies passing into it via bone conduction.  When occluded, this filter is non existent, and the lower frequencies are more noticeably present (discussed in depth further below). 
%
% Other more recent studies on human subjects have agreed with these findings.  It has also been found that the acoustic response differs significantly depending on the location of the skull that is stimulated\footnote{Typically in these studies it is either the frontal bone or the mastoid process (\cite{bekesy:60}).}.
A few (cf. \cite{bekesy:48}, \cite{hansen:97b}, \cite{porschmann:00}, and \cite{reinfeldt:10}) have investigated bone conduction when the source of vibration (ie. sound) is a person's own voice, not an artificial mechanically-created vibration.  These studies also record with a microphone at the entrance to a speaker's ear canal, and not via another sensor attached to the skull.

The many studies that use a simple mechanically-created vibration as a stimulus do so in part because the use of speech as a source is inherently messy.  This is due to the fact that speech 
\begin{enumerate*}[label={\alph*)}]
  \item  can not be as precisely manipulated as a mechanically-created vibration,
  \item  contains far more frequency components than a mechanically-created vibration with a single operating frequency, and 
  \item  takes multiple pathways to get to the ear: from the vocal folds, through tissue, and into the ear canal, and also from vibrations in the air all along the vocal tract\footnote{The speech sound is filtered differently at different locations along the vocal tract}, through the solid medium of the head\footnote{Although, of course, the head is composed of different tissues with different densities and acoustic resonances}, and back into the medium of air inside the ear canal (\cite{bekesy:60}).
\end{enumerate*}
% 
% \begin{wrapfigure}{l}{0.5\textwidth}
% %\begin{figure}
% \centering
%   \includegraphics[width=0.5\textwidth]{figure/bekesy60-3b.png}
%   \caption{Diagram of the propagation of speech waves throughout the body. Numbers correspond to percentage of the original amplitude of the speech remaining when reaching the marked location. Taken from \cite{bekesy:60}.}
%   \label{fig:bekesyBodyTransfer}
% %\end{figure}
% \end{wrapfigure}
%
Additionally, the ear canal itself acts as a resonating chamber (\cite{rosen:91}), altering the signal beyond the distortion already caused by the passage through tissue and bone.  


% \subsection{Peripheral Auditory System Anatomy}
% 
% Prior to discussing the resonating characteristics of the ear canal, it is important to become familiar with the basic ear canal anatomy.  The peripheral auditory system is generally grouped into three primary categories, the outer ear, the middle ear, and the inner ear (cf. Figure \ref{fig:ear-anatomy}).  The outer ear includes the pinna, the ear canal tube, and the tympanic membrane (ie. the eardrum).  Air-transmitted vibrations (sound) enter the ear canal through the opening at the pinna.  These then travel along the canal to vibrate the tympanic membrane, which passes the energy to the middle ear.  The middle ear includes the ossicles within the middle ear cavity.  These are a series of very small bones that the vibrational energy travels along, until it is passed into the cochlea in the inner ear.
% 
% The inner ear is composed of the cochlea, the semicircular canals (and vestibule), and the auditory and vestibular nerves.  The semicircular canals, vestibule, and vestibular nerve don't play a part in audition (their primary function regards balance sensitivity).  The cochlea receives the vibrations passed along through the middle ear ossicles.  These vibrations travel through a fluid substance in the cochlea, are sensed by hair cells within the cochlea, and transmitted as electrical pulses to the auditory nerve.
% 
% \begin{figure}[h]
% \centering
%   \includegraphics{figure/ear_anatomy.png}
%   \caption{A diagram of the peripheral auditory system, including the outer ear, middle ear, and inner ear, up to the auditory nerve. (Image from \cite{martin:12})}
%   \label{fig:ear-anatomy}
% \end{figure}
% 
% Of interest to this present study is the outer ear.  Typically, as described above, vibrations will enter the ear canal through the opening at the pinna.  However, vibrations from one's own speech are also transmitted via the bone, cartilage, and tissue of the head.  Regardless of source, sound vibrations entering into the ear canal will be altered by the shape of the ear canal, described more below.

% Discussion about EAC resonance Theory
\subsection{Modelling Ear Canal Resonance}
% \begingroup
A speech signal can reach the ear canal by passing through the free field (air), as well as the solid medium of bone and tissue.  This section will describe what happens to the signal when it reaches the ear canal via the latter pathway. 

There have been many investigations of the resonating characteristics and amplitude response of the ear canal.  One such project was performed by \cite{stinson:89}, which studied fifteen human ear canals.  Their aim was to produce a model to replicate the effect that the ear canal has on acoustics.
One challenge in producing such a model is the considerable variability in the shape of the canal - both between subjects as well as between the right and left ear canal of a single subject (\cite{stinson:89}).  These differences are apparent in curvature, length, volume, and cross-sectional diameter throughout the ear canal.  \cite{stinson:89} created silicon ear molds for each of the ear canals, which were used to generate three different computational models: the first following the contours and dimensions of their ear molds exactly, another following the dimensions of the ear mold, but straightening contours and curvatures as if along a central axis, and the third as if the ear canal were a uniform tube with the same length and volume of the ear canal molds and previous models.% (see Fig. \ref{fig:eac_modelling}).  
They noted that most significant differences between these models' spectral predictions of ear canal resonance occur above 6 kHz.

% %
% \begin{figure}[h!]
% % \begin{wrapfigure}{L}{0.5\textwidth}
% \centering
%   \includegraphics[width=0.4\textwidth]{figure/eac_mod_diffs.png}
%   \caption{\cite{stinson:89} diagrams three different models of the ear canal resonance.  The bold line is based on their 3D canal molds from cadavers, the dashed line removes the curvature of the ear canal and acts as if the axis were straight, the dotted line assumes a constant diameter along a straight axis, with the same ear canal volume as the dashed and solid lines.}
%   \label{fig:eac_modelling}
% % \end{wrapfigure}
% \end{figure}
% %

Another challenge is to obtain the dimensions of the ear canal needed in order to treat it as a uniform tube. Immittance measurements are widely used in audiology, and involve emitting a continuous low-frequency probe tone into a pressurized ear canal.  The probe then reflects back from the tympanic membrane (assumed to have infinite impedance in a pressurized canal) and can be recorded (\cite{ballachanda:97}, 415): ``The sound pressure developed inside a rigid cavity from a known sound source is directly related to the volume of the cavity".  Therefore, the volume of the ear canal can be inferred for a subject using immittance testing without the need for invasive measurements (eg. using a silicon mold).  Making an assumption about either an `average' diameter or an `average' length of the ear canal would allow for the approximate calculation of the other dimension, given the measured volume. The average length of the ear canal has been cited from 23 mm (\cite{rosen:91}) up to approximately 29 mm (\cite{stinson:89}) for a straight tube. The average diameter for the ear canal is approximately 7.1 mm (\cite{salvinelli:91}).
%These ear canal dimensions can then be plugged into any one of several models designed to approximate the ear canal dimensions.

Since much of the acoustic information for distinguishing speech sounds is located below 6 kHz, and the various resonance models developed by \cite{stinson:89} are very similar below 6 kHz, several (cf. \cite{stinson:89,hansen:97b,stenfelt:07}) who have made efforts to model the ear canal, have chosen to simply treat it as if it were a uniform tube.  Treating the ear canal model as a uniform tube, as opposed to incorporating the nuances of its diameter and curvature, will not have much effect on the output of a model, and greatly simplifies the calculations.

\begin{figure}[t!]
\centering
  \includegraphics{figure/open-closed-tube.png}
  \caption{A diagram showing an example of an open-closed tube (top) and a closed-closed tube (bottom).  Applied to the case at hand, the closed end (on the right) in both figures would be the tympanic membrane.  The opening at the left (top) is the entrance to the ear canal, which can be plugged (bottom).}
  \label{fig:open-closed-tube}
\end{figure}

The acoustics of uniform tubes is usually thought of in terms of an open-open tube, an open-closed tube, or a closed-closed tube.  Each of these have different resonant characteristics. The human vocal tract, for example, is normally modeled as an open-closed tube, where one end (the glottis) is generally considered `closed' for modelling purposes, and the other (the mouth) is generally considered to be `open'.  Similarly, for the human ear, the tympanic membrane represents the `closed' end of the ear canal tube, and the ear canal opening at the concha, or pinna, is the `open' end (cf. Figure \ref{fig:open-closed-tube}).  It can also be modeled as a closed-closed tube if the ear is plugged (cf. Figure \ref{fig:open-closed-tube}). This difference changes the resonance and reverberant structure of the ear canal.  This is discussed further in Section \ref{sec:OEBCspeech}.  

%BC+EC-OE
% \subsection{Ear Canal Resonance on Bone-Conducted Speech}

The resonances of the ear canal can be modeled as if the canal were a uniform tube, without much loss in accuracy.  This has been determined by modelling tones at different frequencies emanating from the entrance of the ear, recorded at the tympanic membrane, and noting the change in resonant frequency.  However, there are a few (c.f. \cite{bekesy:48}, \cite{porschmann:00}, \cite{reinfeldt:10}) who used real human speech as the sound source, and bone-conduction as the medium through which it is transmitted into the canal.  This would result in speech \textit{not} emitted at the entrance to the ear canal, but entering the ear canal through the ear canal walls.
%
% The acoustics of uniform tubes is usually thought of in terms of an open-open tube, an open-closed tube, or a closed-closed tube.  Each of these have different resonating characteristics. The human vocal tract, for example, is normally modeled as an open-closed tube, where one end (the glottis) is generally considered `closed' for modelling purposes, and the other (the mouth) is generally considered to be `open'.  Similarly, for the human ear, the tympanic membrane represents the `closed' end of the ear canal tube, and the ear canal opening at the concha, or pinna, is the `open' end (cf. Figure \ref{fig:open-closed-tube}).

\cite{porschmann:00} is generally looking at the \textit{self-perception} of one's own voice, but in order to accomplish this devotes effort to looking at the bone conduction pathway separately.  An approximate 0.9 kHz resonance (with subsequent harmonic resonances) was found in the bone-conducted speech, generally present between 0.7 and 1.2 kHz.  This correlates with the 0.8 - 1.2 kHz range for the first resonance that others (cf. \cite{hakansson:94}) have observed in mechanical-stimulated bone conduction studies. This would mean, in terms of speech, that one would expect to find higher relative amplitudes near the first formant.

However, in this study only two phonemes were used (/s/ and /z/), and a masking threshold\footnote{The masking threshold technique involves playing a pure tone at different frequencies and amplitudes while the participant is phonating. The participant indicates when the tone becomes audible over their own speech. Knowing the amplitude of the tone allows the researcher to know the perceived loudness of the the speaker's own speech as one becomes audible over the other. Having this knowledge, the spectrum of speech as it was perceived by the speaker can be mapped.} technique was used to determine the frequency spectrum of the transfer function of body conduction.  This is admittedly a rather subjective method of determining the spectrum.  

\cite{reinfeldt:10}, use microphones to record the actual sound pressure level (SPL) of both air- and body-conducted speech. Furthermore, \cite{reinfeldt:10} used a more diverse set of phonemes than previous studies.  A resonance was found in generally the same frequency region for /s/ (and other phonemes) as that found by \cite{porschmann:00} (0.7 - 1.2 kHz) and \cite{hakansson:94} (0.8 - 1.2 kHz). %, they discovered some distinct differences, which can be seen in Fig. \ref{BCrelAC}. 
Between each sound class that was used - voiceless sounds (/s/, /t/, /k/, and /tj/),  nasals (/m/ and /n/), and vowels (/i/, /e/, /\textipa{A}/, /o/) - a moderately similar frequency response is seen. %, yet there are some distinctions to note. %(see Fig. \ref{BCrelACall}).
These frequency responses are given in \cite{reinfeldt:10} as the relative amplitude of bone conducted speech when compared with the air-conducted component of speech.  When the response is positive, the bone-conducted signal dominates, but when the response is negative, the air-conducted component has greater energy.

% In particular, %as can be seen in Fig. \ref{BCrelAC}, 
% there is much inter-speaker variation within the body conduction of the same sound.  %While it is difficult to track an individual speaker's relative spectral envelope within the figure, 
% It appears that much of this difference, particularly in the lower frequencies, originates from a difference in amplitude, and not necessarily from different resonance locations along the frequency axis.  %It is important to note that both Figs. \ref{BCrelAC} and \ref{BCrelACall} both contain \textit{relative} spectral envelopes - ie. the difference between the air-conducted and body-conducted components of speech, and do not contain an absolute frequency spectrum of body-conducted speech. 

Overall, there is much variability between subjects; yet there are some salient distinctions to note.  One is that the /e/ vowel has a relatively flat response up to 500 Hz, and dips down to -5 dB around 1 kHz.  This is in contrast with the phone /s/, which has a fairly high (yet falling) response below 500 Hz, and does not dip below -5 dB until nearly 4 kHz.  Compared with /e/, the body-conducted to air-conducted ratio for /s/ has a significant downward slope after 2000 Hz.  This is likely due to the fact that there is relatively little energy produced by /s/ in the low frequencies, allowing for a high ratio, which drops as the general energy of the phone increases.  This could indicate that most of the energy produced by a obstruent does not pass through to the ear canal (\cite{reinfeldt:10}).

% \begin{figure}[h!]
% \includegraphics[width=1\textwidth]{figure/BCrelAC_e_s.png}
% \caption{The amplitude of Body-Conducted speech relative to the amplitude of Air-Conducted  speech as recorded in the ear canal for the phones /e/ and /s/.  A value of less than zero indicates the amplitude of body-conducted speech is less than that of air-conducted speech, and a value greater than zero indicates a higher amplitude of body-conducted speech than air-conducted speech. The solid line indicates the mean, and the remaining data points are from individual speakers.  The signal was measured from the entrance of an open ear canal.  Taken from \cite{reinfeldt:10}.}
% \label{BCrelAC}
% \end{figure}

More specific dichotomies can be found between sounds within the same class. For example, the low vowel /\textipa{A}/ is pronounced with a more open mouth vs the relatively closed mouth of the high vowel /i/; consequently, the body-conducted amplitude relative to the air-conducted counterpart was much higher for /i/ than it was for /\textipa{A}/. An assumption could be made from the data that the more open the mouth is, the more energy is transferred to the air-conducted signal.% (cf. Fig. \ref{BCrelACall}). 
The findings in \cite{reinfeldt:10} are backed by \cite{bekesy:60}, who also noted %diagrammed 
the relative difference in amplitude in the ear canal between the air-conduction and body-conduction of vowels, %(cf. Fig. \ref{bekesyPhoneDiff}), 
that follow a similar pattern.  

There was much inter-speaker variability in the previous studies, but one common characteristic is that the higher vowels had the least \textit{relative} reduction in amplitude.  Since the functions of other high sonorants (eg. /u/) are not given by \cite{reinfeldt:10}, one cannot be certain if this is a phoneme-specific difference, or if it can be generalized to other sounds with a [high] articulation in which the tongue is close to the roof of the mouth.  This latter generalization would make sense, as the oral cavity is more `closed', trapping energy inside the cavity resulting in more reverberations passing through the head and into the ear canal.
%
% \begin{wrapfigure}{l!}{0.5\textwidth}
% % \begin{figure}
% \includegraphics[width=0.45\textwidth]{figure/BC_rel_AC_all.png}
% \caption{The mean relative amplitudes (left ordinate) of body conduction relative to air conduction for vowels (top plot) and other sounds (bottom plot).  The set of lines along the bottom of each plot represent the standard error from the mean (SEM), measure on the right ordinate.  Taken from \cite{reinfeldt:10}.}
% \label{BCrelACall}
% % \end{figure}
% \end{wrapfigure}
%
On the surface, it appears that the more energy that is lost to air conduction during the production of low vowels (ie. from a more `open' articulation), the less energy is transferred into the surrounding tissue.
%\footnote{\cite{bekesy:60} does not break down relative amplitude by frequency.}

Here it is important to re-emphasize that these transforms are given as body-conducted amplitude \textit{relative to} air-conducted amplitude for the given phone, and do not reflect the absolute air- and body-conducted amplitude of phones compared with one another.  For example, /\textipa{A}/ is a relatively loud air-conducted sound due to its open articulation, and this loud air-conducted component may cause its \textit{relative} body-conducted component to appear quieter than the other vowels, when in reality it is possible that the body-conducted component of both vowels have the same absolute amplitude.  Neither \cite{bekesy:60} nor \cite{reinfeldt:10} give information about body-conducted components in relation to one another, and so a direct comparison is not available between two bone-conducted phones.

This section has discussed the resonance of the ear canal, focusing on bone-conducted speech within the ear canal.  The following section will describe the effects that the closure of the ear canal - turning an open-closed tube into a closed-closed tube - has on the resonance of the bone-conducted voice.

% \begin{wrapfigure}{R}{0.5\textwidth}
% \includegraphics[width=0.5\textwidth]{figure/bekesy60-3.png}
% \caption{Demonstrated the different effect on amplitude that closing the ear canal has on the different vowels of English.  Taken from \cite{bekesy:60}.}
% \label{bekesyPhoneDiff}
% \end{wrapfigure}

%BC+EC+OE% Discussion of the Occlusion Effect
\subsection{The Occlusion Effect on Bone-Conducted Speech}\label{sec:OEBCspeech}



This present section focuses on describing the occlusion effect, and what it means for bone conducted speech.  Returning briefly to the notion of tubes, if the ear were to be occluded at its one open end, the ear canal would no longer be an open-closed tube, but a closed-closed tube, and the frequency response would be altered accordingly.  This phenomenon, first noted by \cite{wheatstone:79}, is termed the occlusion effect.  The occlusion effect is the change in sound pressure level (SPL) resulting from body-conducted vibrations emanating into, and reverberating within, a \textit{closed} ear canal.  Objectively, inside the ear canal, the sound pressure level at lower frequencies is increased when the ear canal is occluded.  This has been studied extensively (cf. \cite{wheatstone:79,kelly:37,littler:52,tonndorf:66}, among many others).  %Generally, the occlusion effect (OE) results in a great increase in the amplitude of frequencies below 1-2kHz, acting as a low-frequency gain\footnote{This ``gain'' is a result of the transfer of energy from the higher frequencies} and that the amplitude of higher frequencies is dampened (as previously mentioned in the observation in studies of cats (\cite{tonndorf:72}).

Additionally, some factors that could potentially alter the effect that occlusion has on sound within the ear canal are mentioned, such as the depth of the occluding device inside the ear canal, the Lombard effect on speaker style, and changes to the size of the ear canal during jaw movement.

As with bone conduction in general, most of the research of the occlusion effect has been conducted using controlled mechanical vibrations.  Using a mechanical stimulus, \cite{bekesy:60} reported that when the ear canal is closed, the increase in amplitude within the canal can be observed up to 2 kHz, and above 2 kHz, the increase in loudness `vanishes' quite suddenly. % (cf. Fig. \ref{fig:bekesyOEresponse}).

Early on, the particular focus was to understand the mechanism behind this shift in amplitude to the lower frequencies. \cite{huizing:60} proposed that this `restructuring' of the frequency spectrum was due solely to the change in resonance characteristics of the ear canal when it is occluded.  This proposal is supported by the known physical phenomenon that the occlusion of ends of a uniform tube change the resonance properties of the tube. The resonance frequencies of an open-closed tube are proportional to 4x the length of the tube and a closed-closed tube produces resonant frequencies which are proportional to 2x the length of the tube.  Using the known characteristics of tube resonance to describe an occluded ear can partially account for what is observed, but it largely describes what happens in the upper frequencies (generally above 2 kHz, \cite{stenfelt:03}).  

\cite{tonndorf:66}, through many studies utilizing domestic cats,  discovered the mechanism behind the apparent low-frequency gain.  When the ear canal is open, due to ``the mass-effect of the air column in the ear canal together with the compliance of air in the ear canal'' it acts as a high-pass filter, dampening the lower frequencies (\cite{stenfelt:03}, 910).  Conversely, when the canal is occluded, this high-pass filter is gone, and the formerly filtered low-frequency energy reappears.  In this sense, it is not the case that the occlusion of the ear canal increases the amplitude of lower frequencies, but it prevents them from being filtered out.

%\cite{hansen:97b} and \cite{stenfelt:07} are primarily interested in modelling an occluded EAC, and this study requires the model to be a closed-closed tube - that of an \textit{occluded} ear. The models proposed by \cite{hansen:97b} and \cite{stenfelt:07} take many parameters, yet most can be considered to be either \textbf{too difficult to calculate frequently (- what does this mean? give an example)}, or relatively constant (e.g. the impedance of the middle ear, the speed of sound, etc.).  Both models, however, take the parameters for the length of the EAC, the average cross sectional area or diameter, and the volume of the EAC, which, are highly variable.

There are also differences based on the location of \textit{occlusion}, ie. how deep a plug is placed in the ear canal.  This affects the resonances of the tube that play a part in the shape of the higher frequencies, due to the different depths of plug insertion resulting in `tubes' of different lengths.  Another effect of plug depth is that, the deeper a plug is inserted into the ear canal, the less surface area there is in the canal for acoustic vibrations to enter.  Consequently, there is generally a reduction in amplitude.

\cite{stenfelt:07} developed a model of an occluded ear using measurements generated from stimulating the skull separately at both the frontal bone and the mastoid process.
%
% \begin{wrapfigure}{r!}{0.5\textwidth}
% \includegraphics[width=0.5\textwidth]{figure/bekesy60-1.png}
% \caption{The frequency response inside the ear canal when taking a mechanical vibration as stimulus to a participant's forehead.  Taken from \cite{bekesy:60}.}
% \label{fig:bekesyOEresponse}
% \end{wrapfigure}
%
They noted that the occlusion effect was greatest when using an ear-plug near the opening of the ear canal, as opposed to supra-aural `earmuffs' or a deep-insertion ear-plug, though an occlusion effect was noticeable in each condition.
The results from \cite{dean:00} indicated that the effect of plug depth on the amplitude of a signal does not disappear as frequency increases. The difference in relative amplitude between the different insertion depths was greatest at lower frequencies, with supra-aural earmuffs resulting in the greatest amplitude increase, and the deep inserted ear-plugs with the lowest.

This was in direct contrast with \cite{stenfelt:07}, who found the greatest occlusion effect with the shallow insertion ear-plugs, and not the supra-aural earmuff.  \cite{dean:00} did not mention the size of earmuff used, but \cite{stenfelt:07} reported the use of a `large' and `small' earmuff, with the latter providing a greater occlusion effect than the former; both of these occlusion effects produced a rise in amplitude below that of the shallow-insertion ear-plugs.
However, \cite{dean:00} did find that at 1 kHz, the shallow-inserted ear-plug had a greater relative amplitude gain than the supra-aural earmuff.  \cite{dean:00} did not mention the explicit depth used for each condition. 


With shallow insertion, the model developed in \cite{stenfelt:07} estimated a gain in amplitude of frequencies below 2 kHz, and dampening of those above; all insertion depths, according to their model's predictions, will at minimum slightly dampen frequencies above 2 kHz.  As the plug is inserted deeper, and the size of the ear-canal shrinks, the damping will occur on lower and lower frequencies.  While there is general agreement of the location of the upper frequency cut-off, these results contrast slightly with those of \cite{bekesy:60}% in Fig. \ref{fig:bekesyOEresponse} 
in that they predicted higher, very low frequencies (0-0.5 kHz), as opposed to the resonance around 1-2 kHz described by \cite{bekesy:60}. % bell-shaped 


There is also a variance in the occlusion effect - if using a mechanically-created vibration - depending on the location of mechanical stimulation.  This difference is most present in the lower frequencies (\cite{dean:00}), where the relative amplitude increase of body-conducted sound versus air-conducted sound appears to be the greatest, but this difference tends to wash out when slightly higher frequencies are reached\footnote{\cite{dean:00} found that the greatest relative amplitude increase occurs near 250 Hz, but the difference largely disappears when 1000 Hz is reached.}.  Each stimulation site in \cite{stenfelt:07} yielded a slightly different frequency response for the occlusion effect as well.  Stimulation at the mastoid generally resulted in a greater increase in very low frequencies below 1 kHz.  This is one limitation of using a mechanical stimulus - it cannot be assumed to accurately model the occlusion effect of speech, since the observed effect differs depending on the location of the mechanical stimulus.


In contrast to these studies, \cite{hansen:97b} tested the occlusion effect using one's own voice as the input source.  %\cite{hansen:97b} presents a graph comparing three spectra calculated from continuous speech from three separate publications (From \cite{wimmer:86}, \cite{thorup:96}, and \cite{may:92}, Fig. \ref{fig:hansenAverageOEa}).  
%
% \begin{wrapfigure}{r}{0.5\textwidth}
% \begin{subfigure}{0.45\textwidth}
%   \centering
%   \includegraphics[width=0.8\textwidth]{figure/hansenAverageOE.png}
%   \caption{ }
%   \label{fig:hansenAverageOEa}
% \end{subfigure}%
% \hfill
% \begin{subfigure}{0.5\textwidth}
%   \begin{subfigure}{0.8\textwidth}
%     \centering
%     \includegraphics[width=1\textwidth]{figure/Hansen_OE-plot_a.png}
%   \end{subfigure}
%   \begin{subfigure}{0.8\textwidth}
%     \centering
%     \includegraphics[width=1\textwidth]{figure/Hansen_OE-plot_b.png}
%   \end{subfigure}
%   \caption{ }
%   \label{fig:hansenAverageOEb}
% \end{subfigure}
% \caption{In (a), three separate measured OE spectra. In (b), OE of two subjects with ear molds extending into the canal at different lengths (\cite{hansen:97b}).}
% \label{fig:hansenAverageOE}
% \end{wrapfigure}
%
The tests performed by \cite{hansen:97b} %(seen in Fig. \ref{fig:hansenAverageOEb}), 
by and large agree with %the 
previous studies (\cite{wimmer:86}, \cite{may:92}, and \cite{thorup:96}).  \cite{hansen:97b} found, among much variation, that there was a substantial increase in amplitude below approximately 1 kHz.  This dropped to around 0 dB amplitude increase relative to air-conducted speech from 1 kHz upward, holding steady until approximately 2 kHz.  Between 2 kHz and approximately 5 kHz, there is a large dip.  Above 5 kHz, there is another rise in relative amplitude.  \cite{hansen:97b} developed a model of the occlusion effect which largely agrees with these measurements, ie. very loud low frequencies, dampened mid-range frequencies, and another increase in frequencies above 5 kHz.

The increase in amplitude of the voice in the ear due to occlusion can also influence the manner in which speech is spoken.  This can be seen clearly by the occlusion effect's influence on the Lombard effect (\cite{lombard:11,lane:71}).  The Lombard effect is the tendency of humans to speak louder in noise due to noise interrupting the normal feedback loop of speech, ie. speakers do not hear themselves nearly as well in noise as they do normally, and so they speak louder (and clearer) to compensate.  However, recent research by \cite{brungart:12} demonstrated that the Lombard effect disappears when a participant is wearing ear-plugs in a noisy environment.  This is actually due to the presence of the occlusion effect, and the increase in amplitude of one's own voice by closing off the entrance to the ear canal.  Instead of an interruption in, or lack of, feedback, the occlusion provides the speaker with much louder feedback.  \cite{brungart:12} found that speakers wearing ear-plugs in a noisy environment actually speak \textit{quieter} than those without ear-plugs, \textit{regardless} of the actual noise-attenuation by the ear-plugs.  This indicates that it isn't necessarily the lack of noise that prompts speakers to speak at a lower volume, but rather the presence of occlusion and the consequent increase in amplitude of their own voice.

Yet despite an increase in overall amplitude of the voice, the occlusion effect does not alter all speech sounds equally.  The studies looking at the occlusion effect of human speech result in similar spectral resonances as those dealing with simple mechanical vibrations, except real-speech studies are able to capture the different occlusion effect for different kinds of complex sounds in a real speech environment, such as vowels.  



%% Allaying concerns of the effect of jaw movement on the occlusion effect.
\cite{hansen:97b} found several phone-specific differences in the occlusion effect. It is ambiguous as to whether the differences are solely due to the differences in the transforms of phones as a result of body conduction (as seen above in \cite{reinfeldt:10}), or if there are sound-specific differences introduced within the ear canal or by the occlusion effect itself.  
Some have posited that variability could stem from the placement of the jaw bone during speech next to the external auditory meatus, as contact with the jaw bone changes as it moves up and down for `higher' or `lower' phones (eg. /i/ vs /\textipa{A}/). The movement of the jaw bone changes the impedance characteristics of the vibration of the mandible against the temporal bone that houses the meatus (\cite{bekesy:60}).  \cite{allen:60} studied the occlusion effect on participants with a unilateral resection of the mandible (one side of the jaw has been removed), and found essentially no distinction between the occlusion effect in either ear (ie. with a mandibular joint adjacent to the cartilage and bone of the ear canal or without).

\cite{hansen:97b}) found that additionally, a change in shape of the ear canal due to different jaw positions can create an acoustic `leak" between the ear canal wall and the occlusion device; the occlusion effect, obviously, behaves differently when there are different sized `leaks' resulting in different levels of partial occlusion. 
\cite{hansen:97b} diagrams cross sections of the ear canal with the jaw at different positions; between a closed jaw and 5 mm of opening, there is relatively little difference between the shapes of the ear canal.  Since \cite{borghese:97} found that the jaw moves relatively little vertical distance during actual speech (max opening approx. 6 mm), it can be assumed that the ear canal changes shape negligibly during normal speech with a snug-fitting occlusion device. 

In summary, %it is important to emphasize the key difference between measurements from an open ear canal and those from an occluded ear canal, which can largely be seen between Figs. \ref{BCrelACall} and \ref{fig:hansenAverageOEa}.  
this section introduced the occlusion effect and its expected frequency response.  There is a large `increase' in the amplitude of the lower frequencies, which is not present from an open-closed ear canal, and a sizable drop in amplitude after 2 kHz, which similarly does not seem to manifest itself when the ear canal is not occluded.  Another rise in amplitude can be seen above 5 kHz.  The jaw movement during speech is negligible, and is not expected to result in any noticeable effect on the occlusion effect.  The occlusion effect actually results in speakers talking quieter in noise with the ear occluded, because they have louder feedback (ie. the Lombard effect does not occur).  Finally, placement of the occlusion device does matter, as deeper placed plugs result in even greater dampening of mid-range frequencies.


%CONCLUSION
\subsection{Summary}

The aforementioned studies on bone conduction, ear canal resonance, and the occlusion effect, as would be expected, have indicated a large amount of inter-person and inter-phoneme variability and have shown the complexity involved in estimating the effect of body conduction and ear canal reverberance on speech entering the ear canal.  However, the transfer function from the vocal tract to the ear canal does have some standard characteristics, namely, the body and (occluded) ear canal work in conjunction as a low-pass filter on speech, removing many of the higher frequencies which may contain critical components for speech intelligibility.  The occlusion of the ear canal allows these lower frequencies passed by the head's transfer function to be fully realized.

%CONCLUSION to expt 1 lit review

%There is a possibility that the ear signal might be so low-pass filtered as to dampen the upper frequencies beyond retrieval and to the point that accurate recognition is unlikely.  In this case, there are still several possibilities, one of which will be outlined below.  

%While critical speech information may be lost, there is still critical speech information that will be present in the signal, namely the pitch.  The ear signal could be used to ``clean" a simultaneously recorded signal obtained from the mouth (which includes any ambient noise).  Since the occlusion effect acts as a gain in the low frequencies, the harmonic information of the voice signal can be reliably obtained from the ear signal.  Using this harmonic information, a comb filter (cf. \cite{nehorai:86}) could be developed and applied to the noisy mouth signal (cf. \cite{king:08}, \cite{cai:09}, \cite{jin:10}).  The comb filter acts as a series of narrow bandpass filters at equal (harmonic) intervals; when applied to speech, this can filter out the noise located in-between the harmonics.  

%There are a number of drawbacks with this method, however.  One is that this method primarily is used to recover voiced (harmonic) sounds, and fricatives or other voiceless sounds may not be able to be `cleaned'.  Another is that while speech is generally harmonic-like, it is not a perfectly harmonic signal, and there is a possibility that some of the higher harmonics might be accidentally filtered out.  This is particularly concerning as the higher frequencies are what is missing from the ear-recorded signal in the first place.


The methods described in this section, namely the models and transfer functions used by \cite{hansen:97b}, \cite{stenfelt:07}, and \cite{reinfeldt:10}, predict a general relative emphasis in the lower frequencies (below 2 kHz) when the ear canal is occluded, and a drop in frequencies above that range (with a rise above 5 kHz).  With this knowledge, it appears that it may be possible for recoverable speech information to be recorded from inside the ear canal.  While the skull will prevent much ambient noise from reaching a microphone placed in the ear canal, an occlusion device would need to be placed at the opening of the canal to aid in dampening the noise.  This occlusion results in the filtering described above.

This filtering is hypothesized to be, by and large, predictable (\cite{hansen:97b,reinfeldt:10}), unlike ambient noise from the environment which is generally highly variable in both amplitude and form (\cite{zhang:17}). This prediction is that the signal recorded from the ear canal is expected to be heavily low-pass filtered above approximately 2 kHz.  Minor transformations, such as pre-emphasizing the higher frequencies, are hypothesized to recover some of the information that is lost, resulting in speech that will perform better than a noisy signal collected at the mouth in both ASR and human speech perception tasks.
Due to this, the technique of substituting potentially unanticipated, variable `environmental' noise with the anticipated `noise' of body conduction and the occlusion effect will allow for greater confidence that a usable signal could be recovered. 

Section \ref{expt1} below describes the specific methods used to collect speech data from the mouth and the ear canal, and the analysis of the collected speech in an attempt to recover an intelligible signal with the knowledge outlined in this section.  These recovered signals were used in human speech perception and ASR experiments in Chapter \ref{chapter3} and \ref{chapter4}, respectively.


\section{Experiment 1: Creating a dataset of ear-recorded speech}\label{expt1}

There are numerous constraints and requirements for the speech recordings necessary for the experiments in Chapters \ref{chapter3} and \ref{chapter4}, and so it was essential to create an original dataset for this study.  This small corpus was used for creating stimuli ASR and human speech perception experiments.  Primarily, an original corpus needed to be created because there is no known dataset of speech in which the recording location is inside the ear canal; speech recorded from this location was the primary focus of these studies.  Secondly, in order to be comparable, the speech at the ear, and the speech at the mouth, needed to be recorded at the same time, in the same conditions.  This was mainly to determine - when comparing the results of ear-recorded and mouth-recorded speech - whether the ear or mouth-recorded speech performed better in the ASR and human speech perception tasks and to avoid potentially confounding variables that would occur if the two sets of speech were recorded separately. This is discussed further in Section \ref{chap2:methods:design} below.

\subsection{Design}
\label{chap2:methods:design}
   
The goal of this experiment was to create a dataset of recordings, both from the mouth in noisy conditions and from inside the ear in the same conditions.  These recordings were needed to answer the following questions: (a) whether external noise would be completely or largely eliminated when recording speech from the ear, (b) whether the speech from the ear is more intelligible and recognizable by humans than simultaneous noisy speech recorded at the mouth, and (c) whether the speech from the ear would be more intelligible and recognizable by an ASR system than noisy speech. 

In alignment with the CHiME challenge\footnote{The CHiME challenge tasks researchers to improve upon or surpass the performance of a baseline automatic speech recognizer used on noisy speech data.} guidelines, this study uses different types of background noise at different noise levels.  The noises used include the four sounds (bus, caf\'{e}, pedestrian area, \& street) from the \cite{chime:16}, plus a `factory' noise track.  A short portion of the audio with relatively level amplitude was extracted from each sound file to be played in the background.  Relatively level amplitude was used to allow for a more accurate comparison of the different SNR levels. Furthermore, if a sound file varied in amplitude, it would confound the recognition tasks, eg. whether a certain word or set of phonemes is difficult to recognize, or the noise level that occurred at that portion in the sentence was relatively loud and the primary hindrance to accurate recognition.

Many existing works in ASR and human speech recognition in noise use multiple SNR levels to demonstrate the effectiveness of the technique of noise removal (eg. \cite{braun:16}).  This is generally done by adding a noise signal to already recorded clean speech, giving the researcher acute control over the SNR.  Recording the noise while simultaneously recording the speech was necessary to demonstrate the ability of the ear canal recording location to remove ambient noise.  Due to this need, it was determined that noise would be played from a loudspeaker at pre-determined decibel levels.  

Since human speech is variable in loudness, and amplitude will likely vary between and within speakers, only three, well-spaced noise levels were chosen to allow speakers' various `loudnesses' to fall in the same, broad categories.  Conversational speech is generally around 70 dB, and so the noise levels chosen were 60 dB, 70 dB, and 80 dB.  These were the `averaged' dB levels obtained by averaging over the duration of the sound file.  This would result in approximate SNR conditions of +10 (60 dB), 0 (70 dB), and -10 (80 dB).  A signal with +10 dB SNR is in the range of SNRs where ASR and human listeners have a very high recognition accuracy (cf. \cite{braun:16,gilbert:13}).  A signal with 0 dB SNR occurs in a range in which ASR and human listeners are still able to make out most of a speech signal, but recognition begins to falter.  At -10 dB SNR, recognition performance very noticeably suffers.  As stated before, it was assumed that speakers will vary in how loud they speak, and so actual SNRs were expected to vary.

An amplitude of 80 dB was chosen as a max loudness, rather than a higher noise level to achieve a lower SNR, in order to leave a wide margin between it and any (albeit remote) possibility of hearing damage suffered by participants.  A `clean' (no noise) condition was also utilized for each sentence.  This creates 16 different conditions (5 noise types * 3 noise levels + 1 `clean' condition).  

\subsection{Stimuli}
Thirty sentences were chosen from three Harvard Sentence lists\footnote{The `Harvard Sentences' dataset is comprised of 72 lists, each 10 sentences long, where each list of 10 sentences is phonetically balanced, with the proportion of each phone in the list corresponding with its occurrence in the English language (\cite{harvardSents}).}.  Sentences were chosen from the Harvard Sentence dataset due to being phonetically balanced (the distribution of phonemes in each list proportional to their occurrence in English), and due to their prolific use in speech science research, and specifically their history of serving as stimuli for many speech corpora (cf. \cite{kabal:02,hu:07}, the latter being a noisy speech corpus).  Lists 14, 28, and 57 were used, and chosen semi-randomly, eliminating lists with potentially unfamiliar or rare words.  Each sentence occurs in all 16 conditions (5 noise types * 3 noise levels + 1 `clean' condition), resulting in 480 total stimuli.

  
\subsection{Equipment}

The experiment took place in a large soundbooth.  To create the artificially noisy environment a Yamaha MS101 III loudspeaker was connected to an HP ProBook 6470b laptop.  A sound pressure level meter (SPL meter; Larson Davis Model 831) with a PCB Piezotronics Model 377B20 condenser microphone (omnidirectional) was placed 1 meter from the loudspeaker and measured the sound pressure to verify each of the three noise levels for each of the 5 noise types. A Grason-Stadler GSI Typstar middle ear analyzer was used to measure the ear canal volume and test for plug leaks.  Two Countryman B2D directional lavalier microphones with fixed XLR connections were used to record the signals from the mouth and ear.  These were connected to a PreSonus Digital Audio Firebox preamplifier, which was connected via TRS cables to a Zoom H6 Handy Recorder. A pair of 3M Professional Peltor Earmuffs with an noise reduction rating (NRR)) of -30 dB SPL were worn by the participants during the experiment.


\subsection{Participants}\label{chap2:methods:participants}
Seventy-one participants were recruited for this study, but data from only twenty participants was used, ten female and ten male, all fluent speakers of American English with normal hearing.  The methodology behind this choice was post-hoc, as a number of conditions were chosen as limiting factors in order to standardize the data and limit potentially confounding variables.  These conditions included:

\begin{itemize}
\item{The participant read all sentences in all conditions (no condition was accidentally missed)}
\item{The participant read all instances of a given sentence in approximately the same manner (eg. consistent duration, intonation)}
\item{The participant was a fluent speaker of American English (to standardize recognition experiments)}
\item{The plug did not move during the recording experiment}
\item{The ear was completely clean of cerumen}
\item{There was no potential seal compromise}
\item{The participant had normal hearing (including a normal middle ear function)}
\end{itemize}

It was not possible to satisfy all `requirements' for all speakers, but the first three conditions were met for each of the twenty participants used in the present study.

\subsection{Procedure}\label{chap2:methods:procedure}

The participants were initially asked a few preliminary demographic questions\footnote{eg. 2nd language (if any), etc. For a list of all information gathered, see Appendix \ref{appendixA}.}. They were seated in front of the middle ear analyzer.  An otoscope was used to ensure the right ear was mostly free of cerumen, to avoid blocking the microphone off from the rest of the canal and generally impacting the canal with cerumen.  The ear was fitted with an appropriate sized rubber clinical single-use ear tip, connected to the probe tip of the middle ear analyzer.  An effort was made to ensure a relatively standard depth for the ear plug, as it plug depth has been shown to alter the occlusion effect (\cite{dean:00,stenfelt:07}), but oftentimes this was not possible, and achieving a proper seal was given a higher priority.

Typmanometry was performed, which involves measuring the SPL level of the probe tone while pressure was varied from positive to negative in the ear canal.  The middle ear analyzer confirmed that there was a seal in the ear canal in order to be able to modulate pressure, and would alert the researcher to a leak if the plug were not securely in place.  This test gave an estimate of the volume in milliliters (mL) of the ear canal and of the middle ear, with precision to a tenth of a mL; additionally, a graph of middle ear function was given, which was checked for normalcy (cf. Appendix \ref{appendixA}).  The middle ear analyzer gave other measures which were not used in this study.

The distance from the end of the ear-plug to where it was enclosed by the ear canal was measured to determine how far the plug was inserted in the ear canal.
Since the length of the plug is known, this was done by placing a measuring rod against the cavity of the concha to measure how far the plug was sticking out of the ear, from which the insertion depth can be calculated. The decision to treat the cavity of the concha as the `end" of the ear canal was taken from \cite{stenfelt:07}, who made molds of ear canals, and treated the rapid increase in volume (where the cavity of the concha began) as the end to the ear canal.  This measure allowed for the calculation of the depth of insertion of the ear-plug.



\begin{wrapfigure}{R}{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{figure/overallSetUp.png}
\caption{A diagram of the basic equipment set up for the experiment.  The in-ear microphone is placed under the headphones in the right ear.}
\label{fig:overallSetUp}
\end{wrapfigure}
The middle ear analyzer probe was then removed from the ear-plug - which was carefully left in place to ensure a continuous seal.  The participants then moved to a seat located in front of a computer monitor (cf. Fig \ref{fig:overallSetUp} for set-up diagram).  The participants were then instructed as to the proceedings of the rest of the experiment. The foam wind-screen was removed from one of the two microphones, and the microphone was snugly inserted into the ear-plug.  A mark on the microphone cable was used to ensure the end of the microphone was fully inserted to the end of the ear-plug (cf. Fig. \ref{fig:micInsertPlug}).  
There were several instances where the microphone was inserted deeper than, or just shy of, the end of the ear-plug; the variance was within +/-2 mm depth (cf. Appendix \ref{appendixA}).  The earmuffs were placed over both ears.  Occasionally, participants had glasses, or thick hair, which may have slightly compromised the seal.  A note was taken of this. Necessarily, the cord for this microphone also ran from the microphone inside the ear-plug, out underneath the earmuff seal to the pre-amplifier.  This was assumed to not significantly effect the earmuff seal.




A wooden rod was attached to the earmuffs, which extended forward, beside the participants' face.  
The second microphone was attached to this wooden rod via the lavalier clip at the level of the participants' mouth.  The microphone was directed toward their mouth (cf. Fig \ref{fig:earmuffSetup}).  
The placement of the microphone on the wooden rod was adjusted to be exactly 10cm away from the participants' infra-nasal depression.  At this point, the participants were asked to adjust the placement of their chair so that the microphone on the wooden rod would be approximately 1 meter from the loudspeaker. 
%
\begin{wrapfigure}{L}{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{figure/micInsertPlug.JPG}
\caption{The Countryman B2D directional lavalier microphone, with with the wind-break foam removed, and inserted into a blue ear-plug.}
\label{fig:micInsertPlug}
\end{wrapfigure}
%
Due to the length of the experiment ($~$45min), no effort was made to discourage minor shifting in body position.  The loudspeaker was on another table to the right of the participants, perpendicular to the direction of the microphone facing the mouth (cf. Fig. \ref{fig:overallSetUp}).

Both microphones were connected directly to the preamplifier through a fixed (non-changeable) XLR connection.  Both channels were set to the same gain on the preamplifier.  Two TRS cables took each microphone signal from the preamplifier to the recorder.  Both channels were adjusted to appropriate (different) gain levels on the recorder itself to achieve a similar amplitude for both signals and prevent clipping.  These adjustments were made once the participants were situated, but before beginning the recording.

Once recording, an in-house computer program was used to display the stimuli sentences on a second monitor and to play the background noises.  For each sentence, the participants saw the clean-condition (no noise) first.  The researcher was in the soundbooth with the participants listening through a pair of headphones connected to the preamplifier.  
The participants were asked to repeat the sentence twice to develop a rhythm, at a normal, conversational loudness, with a normal, declarative intonation.  For each of the following 15 iterations of the sentence (one for each noise-type/noise-level combination) the participants were instructed to read the sentence aloud only once.  The researcher asked the participants to repeat the sentence again in a condition if the rhythm or intonation of the sentence did not match those preceding it, or if the participants stumbled over a word.  Out of the loudspeaker - placed 1 meter from the mouth-microphone - each of the background noises was emitted to coincide with a spoken sentence.  The sentences were not randomized, 
%
\begin{wrapfigure}{h!}{0.5\textwidth}
\includegraphics[width=0.5\textwidth]{figure/earmuffSetup.JPG}
\caption{The -30 dB NRR earmuffs, a rod attached to the earmuffs, and a microphone directed toward the mouth.}
\label{fig:earmuffSetup}
\end{wrapfigure}
%
ie. all 16 iterations of a sentence occurred consecutively\footnote{This was done to help the researcher ensure a similar intonation and rhythm for each iteration of the same sentence.}. \textit{Within} each sentence group (after the `clean' condition, which always occurred first), all the noise conditions were randomized. The researcher advanced each stimulus on the display for the participants.

To help the participants notice when a sentence had been advanced - as when wearing the noise reduction earmuffs, participants were often not aware when the noise condition changed - the number of the sentence condition was displayed underneath the stimulus (1-16).  This had the unintended consequence of occasionally producing a mild list-intonation, as participants were aware of the final repetition of a given sentence (ie. the number `16' appeared beneath the sentence). 

After the recording was finished, the participants were asked to complete a short, 4-question survey of their experiences during the experiment.\footnote{cf. Appendix \ref{appendixA} for coded answers.}  
% These included:
% \begin{enumerate}
%   \item{Can you describe your experience with wearing the experimental set-up?}
%   \item{Did you find the sound of your voice to be altered, annoying, or uncomfortable?}
%   \item{Would you consider wearing such a device if in a noisy workplace or environment in order to communicate?}
%   \item{Would you be more inclined to use such a device if earmuffs were not required?}
% \end{enumerate}
Participants were instructed to give as much detail in their answers as desired, and to answer truthfully.  
% The answers to the first question that were provided by participants generally addressed the comfort of the experimental set-up (ie. the ear-mic and earmuffs).  The researcher used the more specific question -``Did you find the experimental set-up (ie. the ear-mic and earmuffs) uncomfortable?'' - to code the answers. The answers were coded using a Likert scale with a range of 1-5, where `5' is `yes - definitely' and `1' is `no - definitely not'.

Sentences were identified and extracted using Praat.  A number of subjects (more than 20) were recorded, but in an attempt to meet a number of conditions, speech from only 20 participants were used for analysis in this study (cf Section \ref{chap2:methods:participants}).  Survey answers from all recorded participants were used.




\section{Discussion and Observations of Collected Speech}\label{chap2:observations}

Each individual sentence was isolated in each recording with boundaries in a Praat TextGrid.  Sentences were identified using the mouth-recorded signal, and a limited window of `non-speech' silence or noise was allowed on either side of the spoken sentence.  Since the mouth-recorded and ear-recorded signals were simultaneous, the same TextGrid was used to extract individual sound files for both signals.  This resulted in a sound file for each sentence, for each participant, for both the mouth-recorded and ear-recorded speech; each simultaneous (mouth/ear-recorded) signal had exactly the same duration, since the same TextGrid boundaries were used to extract both.  Figures \ref{spctgrmNarrowMouth_35}, \ref{spctgrmWideMouth_35}, \ref{spctgrmNarrowEar_35}, and \ref{spctgrmWideEar_35} show the narrow and wide band spectrograms for a `clean' example of the sentence ``A cramp is no small danger on a swim'', recorded at both the ear and the mouth from participant 35, a female.  These two examples are fairly representative of the speech collected from each location.

As can be seen, the speech collected at the ear is heavily low-pass filtered, and the mouth-recorded speech contains much more speech information. However, there are still clear harmonics in the existing range in the ear-recorded speech, and much of the lower two formants can also be seen.  It should be noted that while the two signals - from the mouth and from the ear - appear to have the same intensity, this is due only to the gain adjustment on the recording device during the experiment.  The speech from the ear had consistently greater intensity than the speech from the mouth; on a scale of 0-10, the ear microphone gain was normally set between 2 and 4, while the mouth microphone gain was set between 4 and 6.

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.75\linewidth]{figure/spctgrmNarrowMouth_35.pdf}
  \caption{Narrow band spectrogram of speech recorded at the mouth.}
  \label{spctgrmNarrowMouth_35}
\end{subfigure}%
% \hfill
\\[2ex]
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.75\linewidth]{figure/spctgrmWideMouth_35.pdf}
  \caption{Wide band spectrogram of speech recorded at the mouth.}
  \label{spctgrmWideMouth_35}
\end{subfigure}
\caption{Both (\ref{spctgrmNarrowMouth_35}) and (\ref{spctgrmWideMouth_35}) are the same sentence, ``A cramp is no small danger on a swim'', spoken by a female participant. This is the exact same sentence spoken at the exact same time as that in Fig. \ref{fig:spect_ear}.}
\label{fig:spect_mouth}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth} %0.475
  \centering
  \includegraphics[width=0.75\linewidth]{figure/spctgrmNarrowEar_35.pdf}
  \caption{Narrow band spectrogram of speech recorded from inside the ear canal.}
  \label{spctgrmNarrowEar_35}
\end{subfigure}%
% \hfill
\\[2ex]
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.75\linewidth]{figure/spctgrmWideEar_35.pdf}
  \caption{Wide band spectrogram of speech recorded from inside the ear canal.}
  \label{spctgrmWideEar_35}
\end{subfigure}
\caption{Both (\ref{spctgrmNarrowEar_35}) and (\ref{spctgrmWideEar_35}) are the same sentence, ``A cramp is no small danger on a swim'', spoken by a female participant. This is the exact same sentence spoken at the exact same time as that in Fig. \ref{fig:spect_mouth}.}
\label{fig:spect_ear}
\end{figure}


\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth} %0.475
  \centering
  \includegraphics[width=0.75\linewidth]{figure/female_LTAS.png}
  \caption{Two Long-Term Average Spectra (LTAS), red for ear-recorded speech and black for mouth-recorded speech, averaged over all `clean' female-spoken sentences.}
  \label{female_LTAS}
\end{subfigure}%
% \hfill
\\[2ex]
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.75\linewidth]{figure/female_rel_dB.png}
  \caption{The difference ratio between the ear-recorded speech (red) and mouth-recorded speech (black) in Fig. \ref{female_LTAS}.}
  \label{female_rel_dB}
\end{subfigure}
\caption{The ear-recorded and mouth-recorded Long-Term Average Spectrum (LTAS) (Fig. \ref{female_LTAS}) for all female-spoken sentences, and the difference between the two LTAS in Fig. \ref{female_rel_dB}.}
\label{fig:female_LTAS}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth} %0.475
  \centering
  \includegraphics[width=0.75\linewidth]{figure/male_LTAS.png}
  \caption{Two Long-Term Average Spectra (LTAS), red for ear-recorded speech and black for mouth-recorded speech, averaged over all `clean' male-spoken sentences.}
  \label{male_LTAS}
\end{subfigure}%
% \hfill
\\[2ex]
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.75\linewidth]{figure/male_rel_dB.png}
  \caption{The difference ratio between the ear-recorded speech (red) and mouth-recorded speech (black) in Fig. \ref{male_LTAS}.}
  \label{male_rel_dB}
\end{subfigure}
\caption{The ear-recorded and mouth-recorded Long-Term Average Spectrum (LTAS) (Fig. \ref{male_LTAS}) for all male-spoken sentences, and the difference between the two LTAS in Fig. \ref{male_rel_dB}.}
\label{fig:male_LTAS}
\end{figure}

A Long-Term Average Spectrum (LTAS) was calculated for both ear-recorded speech and mouth-recorded speech, averaging over all female speakers and separately over all male speakers.  Figure \ref{fig:female_LTAS} shows the LTAS for the female speakers, along with the relative amplitude difference between the ear-recorded and mouth-recorded speech.  Figure \ref{fig:male_LTAS} displays the LTAS and difference function for all male speakers.  Both have the same basic characteristics - while the mouth-recorded speech has a relatively level LTAS across the frequency spectrum, the ear-recorded speech is heavily low-pass filtered.  The ear-recorded signal dominates in the low-frequencies, but is overtaken by the mouth-recorded signal at only about 500 Hz.

While the existing literature supports the heavy low-pass filter effect (cf. \cite{stenfelt:07,reinfeldt:10}), the recorded signals appear to be more low-pass filtered than described.  The drop in amplitude for these signals is immediate as the frequency increases, whereas \cite{hansen:97b} (also \cite{wimmer:86,may:92,thorup:96}, among others) predicted higher amplitude until approximately 1 kHz.  The sharp decline in amplitude levels briefly slightly at 1 kHz, and an argument could be made that this is the `resonance' of the ear canal that was identified between 0.7 and 1.2 kHz by \cite{porschmann:00} and \cite{hakansson:94}, among others, although this is only speculation.  %One possibility is the depth of the plug.  \cite{dean:01} and \cite{stenfelt:07} both found that the occlusion effect intensified toward the lower frequencies as the plug depth increased, and the range of amplified frequencies became lower and lower.

%While the LTAS averaged over male speakers compared with female speakers appears to be similar, there are more noticeable differences when comparing individual subjects.  This can be observed in Figure \ref{}. 

A cross-correlation similarity metric used to compare signals was run on each ear-recorded/mouth-recorded pair for each participant speaker (excluding those which contained noise in the background).  Recall that these signals were recorded simultaneously, and were also extracted using the same Praat TextGrid boundaries.  Additionally, the signals were time-aligned using a variation of the Matlab code found in Appendix Section \ref{app:time-align}.  This normally resulted in a shift of $<$10 samples.  For the cross-correlation calculation, the maximum value for each ear-recorded/mouth-recorded pair was obtained, this value was normalized, and then all normalized values were averaged across speakers.  This is shown in Equation \ref{norm_max_xcorr}:
%
\begin{equation}\label{norm_max_xcorr}
\sum_{k=1}^{K} \sum_{u=1}^{U} \dfrac{max(abs(xcorr(s^e_{uk}, s^m_{uk})))}{\sqrt{\sum {s^e_{uk}}^2}*\sqrt{\sum {s^m_{uk}}^2}}
\end{equation}
%
where $K$ is the number of participants, $U$ is the number of utterances, $s^e$ refers to the ear-recorded signal in the time domain, $s^m$ refers to the mouth-recorded signal in the time domain, and $xcorr$ is the cross-correlation function. If $s^e_{uk}$ and $s^m_{uk}$ are identical signals, the fractional portion of Equation \ref{norm_max_xcorr} will return a `$1$'.  The less similar $s^e_{uk}$ and $s^m_{uk}$ are, the closer the value will be to $0$. The average normalized maximum cross-correlation value for ear-recorded speech compared with mouth-recorded speech is 0.358.

When noise is present, it can be visually observed in Figs. \ref{spctgrmNarrowMouthNoise_35} and \ref{spctgrmNarrowEarNoise_35} that the noise level is lower for ear-recorded speech.  There appears to be louder noise (seen in the mouth-recorded speech in Fig. \ref{spctgrmNarrowMouthNoise_35}) that is present in the upper frequencies of the ear-recorded speech, but it is significantly dampened and the signal has an overall higher speech to noise ratio (SNR).

It should also be noted that the SNR in Fig. \ref{spctgrmNarrowMouthNoise_35} is much higher than originally intended.  For this particular example, the speech was recorded with an 80 dB noise background, with the intent of obtaining a -10 dB SNR.  Instead, the SNR for the sentence in Fig. \ref{spctgrmNarrowMouthNoise_35} is +6 dB SNR\footnote{The SNR was calculated by using background noises recorded in isolation in the soundbooth.  These were recorded at 60, 70, and 80 dB in the same soundbooth, with the same conditions and set up as a normal recording.  The noisy speech sound file was passed through a Hilbert Envelope, and a threshold was applied in order to extract just the speech data.  The RMS values of both the speech and raw noise vectors were calculated, averaged, and then used in the SNR calculation.  For explicit code, see Appendix \ref{appendixB}}.  Unfortunately, this was widespread. This is attributed to both the participant speaking louder than anticipated, the directionality of the microphone, which eliminated much more background noise than anticipated.



In another attempt to see if there was recoverable information in the higher frequencies of the ear signal, the spectrogram range was increased from 5 kHz to 20 kHz (see Fig. \ref{spctgrmEarNarrow20kHz}). There is certainly acoustic energy that makes it to the higher frequencies, but noticeable harmonic energy is not present, nor does any of the visible acoustic energy appear to correlate with the speech seen in the lower frequencies.  This contrasts with the results shown in \cite{hansen:97b}, which indicated that there was a rise in amplitude above approximately 5 kHz.

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figure/spctgrmNarrowMthNoise_35.pdf}
  \caption{Narrow band spectrogram of speech recorded at the mouth, with 80 dB `bus' noise playing in the background.}
  \label{spctgrmNarrowMouthNoise_35}
\end{subfigure}%
% \hfill
\\[2ex]
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figure/spctgrmNarrowEarNoise_35.pdf}
  \caption{Narrow band spectrogram of speech recorded inside the ear canal, with 80 dB `bus' noise playing in the background.}
  \label{spctgrmNarrowEarNoise_35}
\end{subfigure}
\caption{Both \ref{spctgrmNarrowMouthNoise_35} and \ref{spctgrmNarrowEarNoise_35} are spectrograms of the sentence ``A cramp is no small danger on a swim'', spoken by a female participant, recorded simultaneously.  The high-energy bands at 2 kHz and above in frequency spectrum are characteristic of the bus background noise (cf. Fig. \ref{fig:bkgrnd-noises}).}
\label{fig:noise_mth_ear}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.475\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figure/spctgrmEarNarrow20kHz.pdf}
  \caption{Narrow band spectrogram of ear-recorded speech with 80 dB `bus' background noise.}
  \label{spctgrmEarNarrow20kHz}
\end{subfigure}%
\hfill
\begin{subfigure}{0.475\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figure/spctgrmNarrowEarNoisePremp.pdf}
  \caption{Narrow band spectrogram of ear-recorded speech with 80 dB `bus' noise.  The signal has been pre-emphasized.}
  \label{spctgrmNarrowEarNoisePremp_35}
\end{subfigure}
% \caption{Narrow band spectrogram of ear-recorded speech from 0-20kHz to look for possible speech information in higher frequencies, of the sentence ``A cramp is no small danger on a swim'' spoken by a female participant. Note, the noise only extends to 8kHz.}
% \end{figure}
% \begin{figure}
% \centering
\begin{subfigure}{0.475\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figure/spctrmEar20k.png}
  \caption{Spectrum of ear-recorded speech; taken from the `/\ae/' in `cramp'.}
  \label{spctrmEar20kHz}
\end{subfigure}%
\hfill
\begin{subfigure}{0.475\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figure/spctrmEar20k_preemp.png}
  \caption{Spectrum of pre-emphasized ear-recorded speech; from `/\ae/' in `cramp'.}
  \label{spctrmEar20kPreemp}
\end{subfigure}
\caption{Narrow band spectrograms (\ref{spctgrmEarNarrow20kHz} and \ref{spctgrmNarrowEarNoisePremp_35}) of ear-recorded speech from 0-20 kHz to look for possible speech information in higher frequencies. The sentence ``A cramp is no small danger on a swim'' spoken by a female participant. Note, the noise only extends to 8 kHz. Underneath (\ref{spctrmEar20kHz} and \ref{spctrmEar20kPreemp}) are two spectra from the same speaker and same sentence, normal and pre-emphasized, respectively.}
\end{figure}

Recall the transfer function in Figs \ref{female_rel_dB} and \ref{male_rel_dB}.  Since the curvature in these plots roughly resembles de-emphasis, the ear-recorded speech was pre-emphasized to partially reverse the filtering effects of the head; this can be seen in Figure \ref{spctgrmNarrowEarNoisePremp_35}. The resulting signals seem to further confirm the lack of high-frequency speech information. It appears that while the fainter harmonics in the lower mid-range frequencies are more pronounced, there is no new speech information in the upper frequencies that makes it past the noise threshold.

This ear-recorded signal was then low-pass filtered at 2.5 kHz with a 500 Hz smoothing slope. To further emphasize the higher frequencies in the available range (and to eliminate the remaining 'muffled' characteristic), the sound was pre-emphasized a second time (after filtering).  This can be seen in Figure \ref{spctgrmNarrowEarNoisePrempFiltPremp_35}, next to the noisy mouth-recorded speech for comparison in Figure \ref{spctgrmNarrowMouthNoise_35_compare}.  The average normalized maximum cross-correlation (Equation \ref{norm_max_xcorr}) was calculated for these transformed ear-recorded signals with the mouth-recorded signals (excluding the signals with noise), which yielded a similarity value of 0.674 - an increase of 0.316 over that of the untransformed ear-recorded speech.

To test this difference statistically, a two-factor, by-subjects mixed design and by-items within-subjects ANOVA was performed, with speaker gender (female, male) and processing level (raw, processed) as the two factors.  Table \ref{tab:anova_data-collection} shows no significant interaction of speaker gender \textbf{x} processing level and no main effect of speaker gender for the by-subjects ANOVA, though the by-items analysis finds significance.  Both by-subjects and by-items ANOVAs found a significant main effect of processing level.

<<ANOVA_Data_collection, echo=FALSE, warning=FALSE, message=FALSE, results='asis'>>=

#Visualize Exp.1 Data:
library(ez)
library(doBy)
library(xtable)
#Read in Exp.1 data:
mydata <- read.csv("/home/sam/Dissertation/xcorr_data.csv")
## ANOVA

# ensure column names have the correct name (I think this just renames the columns in the data structure)
colnames(mydata)[1]="processing_level"
colnames(mydata)[2]="subject"
colnames(mydata)[3]="sentence"
colnames(mydata)[4]="speaker_gender"
colnames(mydata)[5]="xcorr"

# attach(mydata)
# ezDesign(mydata, processing_level, sentence)

# This sets each column (that you specify) as a factor.  Failing to do this will result in wonky degrees of freedom
mydata$processing_level = as.factor(mydata$processing_level)
# attach(mydata)

mydata$subject = as.factor(mydata$subject)
# attach(mydata)

mydata$sentence = as.factor(mydata$sentence)
# attach(mydata)

mydata$speaker_gender = as.factor(mydata$speaker_gender)
attach(mydata)

# mydata$xcorr = as.factor(mydata$xcorr)
# attach(mydata)

## ezANOVA

# By subjects ANOVA
by_subj_ez_anova = ezANOVA(
	data = mydata 	#data structure
	, dv = xcorr	#dependent variable
	, wid = subject	#column containing case identifier (e.g. subjs/items)
	, within = .(processing_level) #(independent variables/factors)
	, between = .(speaker_gender)
	)

by_item_ez_anova = ezANOVA(
	data = mydata 	#data structure
	, dv = xcorr	#dependent variable
	, wid = sentence	#column containing case identifier (e.g. subjs/items)
	, within = .(processing_level,speaker_gender) #(independent variables/factors)
	# , between = .(speaker_gender)
	)

# Don't use 'summary(ez_anova)'
# by_subj_ez_anova
# 
# by_item_ez_anova

@


<<ANOVA_data_collection_results, echo=FALSE, warning=FALSE, message=FALSE, results='asis'>>=
by_subj_ez_anova$ANOVA$ges <- NULL
by_item_ez_anova$ANOVA$ges <- NULL

combined <- data.frame(matrix(NA, nrow = 6, ncol = 7))
colnames(combined)[1]="ANOVA"
colnames(combined)[2]="Effect"
colnames(combined)[3]="DFn"
colnames(combined)[4]="DFd"
colnames(combined)[5]="F"
colnames(combined)[6]="p"
colnames(combined)[7]="*"

##################
combined[1,1] <- "S"
combined[1,2:7 ] <- by_subj_ez_anova$ANOVA[1,1:6 ]
combined[2,1] <- "I"
combined[2,2:7 ] <- by_item_ez_anova$ANOVA[2,1:6 ]
###
combined[3,1] <- "S"
combined[3,2:7 ] <- by_subj_ez_anova$ANOVA[2,1:6 ]
combined[4,1] <- "I"
combined[4,2:7 ] <- by_item_ez_anova$ANOVA[1,1:6 ]
###
combined[5,1] <- "S"
combined[5,2:7 ] <- by_subj_ez_anova$ANOVA[3,1:6 ]
combined[6,1] <- "I"
combined[6,2:7 ] <- by_item_ez_anova$ANOVA[3,1:6 ]
#########################

print(xtable(combined, digits=c(0,0,0,0,0,2,2,0), label="tab:anova_data-collection", caption="ANOVA for by-subjects (S) and by-items (I) analysis of the two-factor, mixed design experiment. Columns: `ANOVA' indicates by-subject (S) or by-item (I) analysis, `Effect' lists the factor(s) tested, `DFn' and `DFd' refer to the degrees of freedom numerator and denominator, `F' is the F-score, `p' is the p-value, and `*' marks a significant p-value."),include.rownames=FALSE)

@

The graph in Figure \ref{fig:data-collection-viz} displays the data broken up by processing level and speaker gender.  It is apparent that there is a large increase in correlation between the `clean' mouth-recorded speech and the `clean' ear-recorded speech after the ear-recorded speech was processed.  This indicates that the processing of the speech via pre-emphasis, low-pass filtering, and a second round of pre-emphasis has resulted in speech that is more similar to speech that could be recorded at the mouth.

\begin{figure}[H]
\centering
<<Xcorr-boxplot, echo=FALSE, warning=FALSE, message=FALSE, results='asis'>>=
par(mar=c(4,5,0,0))
boxplot(xcorr~processing_level*speaker_gender,col=c("white","lightgray"),mydata, names=c("Raw\nFemale","Proc.\nFemale","Raw\nMale","Proc.\nMale"),ylab="Cross-Correlation", las=2) #, cex.axis=1.5) #, cex.lab=2.0)
@
\caption{Comparison of raw ear-recorded speech and `processed' ear-recorded speech at each level of gender.  White bars signify raw ear-recorded speech, and grey bars signify processed ear-recorded speech. All compared data was from the `clean' condition.}\label{fig:data-collection-viz}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figure/spctgrmNarrowEarNoisePrempFiltPremp.pdf}
  \caption{The ear-recorded speech, pre-emphasized, filtered at 2.5 kHz with 500 Hz slope, and pre-emphasized again.}
  \label{spctgrmNarrowEarNoisePrempFiltPremp_35}
\end{subfigure}%
% \hfill
\\[2ex]
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figure/spctgrmNarrowMthNoise_35.pdf}
  \caption{The noisy spectrogram of the mouth-recorded speech; previously seen in \ref{spctgrmNarrowMouthNoise_35}, repeated here for ease of comparison.}
  \label{spctgrmNarrowMouthNoise_35_compare}
\end{subfigure}
\caption{Narrow band spectrogram of ``A cramp is no small danger on a swim'' recorded at the ear (\ref{spctgrmNarrowEarNoisePrempFiltPremp_35}) and the mouth (\ref{spctgrmNarrowMouthNoise_35_compare}) and spoken by a female participant, with 80 dB bus noise in the background.}
\label{fig:ear_pfp}
\end{figure}

% \subsection{Survey Responses}
The author compiled the coded survey questions %\footnote{As mentioned at the end of Section \ref{chap2:methods:procedure}, the actual question in the survey for Question 1 was ``Can you describe your experience with wearing the experimental set-up?'' Responses were coded according to their answer to Question 1 - as it appears in Table \ref{tab:survey-results}} 
and answers collected by each participant after the experiment. %, which are located in Table \ref{tab:survey-results}.
% \begin{table}
% \centering
% \begin{tabular}{ |p{9cm}|M{1.7cm}|M{1.7cm}| } \hline
% Question & Mean Response & Mode Response \\ \hline\hline
% Did you find wearing the experimental set-up to be uncomfortable?  & 3.59 & 4  \\ \hline
% Did you find the sound of your voice to be altered, annoying, or uncomfortable?  & 3.85 & 4   \\ \hline
% Would you consider wearing such a device if in a noisy workplace or environment in order to communicate?  & 3.51 & 5  \\ \hline
% Would you be more inclined to use such a device if earmuffs were not required?  & 3.86 & 5  \\ \hline
% \end{tabular}
% \caption{Coded results from the post-experiment survey. Each response was given a Likert scale code of 1-5.}\label{tab:survey-results}
% \end{table}
% The mean response for each of the questions hovers slightly above the median response of `3'.  Most participants found the recording set-up to be rather uncomfortable; the earmuffs in particular seemed to play a role in the discomfort, as most participants would be more inclined to use the system if earmuffs weren't required.  Participants seem split as to whether they would be willing to use the device in a real-life application (Q3).  The mode is rather high (5) given the mean (3.51), which could be due in part to participant response bias (ie. desiring to provide an `acceptable' response).
Qualitatively, participants were slightly more likely than not to describe the experience wearing the recording equipment as uncomfortable, and slightly more likely than not to find the sound of their own voice annoying.  This latter fact is a consequence of the occlusion effect, and is echoed by many who wear canal-occluding hearing aids (\cite{hansen:97a}).  Participants also described that they would be more likely to use such recording equipment if the noise-reduction earmuffs were not required.


%\section{Discussion}

\section{Limitations}
\label{chap2:limitations}

During data collection, there were several issues that affected the quality of speech.  
The particular recorder which was used would produce a low-frequency humming sound when the gain knobs were turned up into the slightly higher range (cf. Figure \ref{fig:low_freq_hum}).  This was much more prominent for the mouth-recorded signals, as the gain for this channel was turned up higher.  Since the headphones used by the researcher to monitor to the participants' speech were plugged into the preamplifier, this was not noticed until most participants were already recorded.

In addition, the presence of this tone could have artificially increased the correlation between the ear-recorded signals and the mouth-recorded signals.  Since the comparison displayed in Figure \ref{fig:data-collection-viz} is between raw and pre-emphasized ear-recorded signals, both of which have the interfering tone, the comparison is still valid, although the values themselves are likely skewed upwards.

On a physical level, the ear-plugs which were used are fairly standard audiological silicone ear-plugs, which had a hole in the middle that was the correct size to fit the microphones that were used.  The degree of noise damping these plugs offer is unknown, as they are not inherently designed for noise reduction.  It is very likely that a better NRR ear-plug could be found and used in conjunction with the 30 dB NRR earmuffs to achieve a greater noise reduction at even higher noise levels.

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figure/low_frequency_hum.png}
  \caption{The low frequency hum in the mouth-recorded signal.  It can be seen at 120 Hz and subsequent harmonics with decaying amplitude.}
  \label{fig:low_freq_hum-mouth}
\end{subfigure}%
% \hfill
\\[2ex]
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{figure/low_frequency_hum-ear.png}
  \caption{The low frequency hum in the ear-recorded signal. As in Figure \ref{fig:low_freq_hum-mouth}, it can be seen at 120 Hz and subsequent harmonics.}
  \label{fig:low_freq_hum-ear}
\end{subfigure}
\caption{Spectrograms of a low frequency hum introduced by the recorder at 120 Hz and subsequent harmonics in both mouth (Fig. \ref{fig:low_freq_hum-mouth}) and ear (Fig. \ref{fig:low_freq_hum-ear}) recorded signals. The hum in Fig. \ref{fig:low_freq_hum-ear} is less prominent than in Fig. \ref{fig:low_freq_hum-mouth} due to the gain on the recorder being lower.  The range of frequency is 0-1 kHz.}
\label{fig:low_freq_hum}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{0.475\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figure/spctgrm_cramp_bus_60.png}
  \caption{The sentence ``A cramp is no small danger on a swim'' with bus background noise at 60 dB. SNR for this utterance is +28 dB.}
  \label{fig:limitation_bus_60}
\end{subfigure}%
\hfill
\begin{subfigure}{0.475\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figure/spctgrm_cramp_bus_70.png}
  \caption{The sentence ``A cramp is no small danger on a swim'' with bus background noise at 70 dB. SNR for this utterance is +19 dB.}
  \label{fig:limitation_bus_70}
\end{subfigure}
%
\begin{center}
\begin{subfigure}{0.475\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{figure/spctgrm_cramp_bus_80.png}
  \caption{The sentence ``A cramp is no small danger on a swim'' with bus background noise at 80 dB. SNR for this utterance is +6 dB.}
  \label{fig:limitation_bus_80}
\end{subfigure}
\end{center}
\caption{Spectrograms of the sentence ``A cramp is no small danger on a swim'' with bus background noise at 60 dB (\ref{fig:limitation_bus_60}), 70 dB(\ref{fig:limitation_bus_70}), and 80 dB (\ref{fig:limitation_bus_80}). The directional microphone filtered out most of the noise in the lower two noise level conditions.}
\label{fig:noise_level_limitation}
\end{figure}

As mentioned previously, the primary limitation is that the background noise was not loud enough to be able to fit into the SNR ratios desired (+10 dB SNR, 0 dB SNR, -10 dB SNR).  Instead, what was recorded was an average (across all speakers) of 31, 23, and 12 dB SNR for the 60, 70, and 80 dB condition, respectively.  The 60 dB noise condition reached as high as +40 dB SNR, and the lowest 80 dB noise condition SNR was +5 dB. The three noise levels can be seen in the spectrograms in Figure \ref{fig:noise_level_limitation} for the bus background noise.  Note that the noise level can barely be observed in the lowest noise condition in Fig. \ref{fig:limitation_bus_60}.

The three options to remedy this would be to a) ask the participants to speak more quietly, b) increase the volume of the loudspeaker, or c) use omnidirectonal microphones.  Options (a) and (c) are more appealing, as it continues to keep the ambient noise outside the range of amplitude which could possibly damage hearing.  However, in the case of (a), it is difficult for a participant to consciously modify their loudness and keep that consistent for the duration of the experiment; the need to repeat sentences spoken above a certain amplitude would drastically increase the length of time of the experiment.  Additionally, it would be difficult for the researcher monitoring the speech to determine whether or not the speech was an appropriate loudness, particularly with the additive background noise playing at the same time.  

Option (b) - increasing the noise level itself - is less appealing in that it results in a higher risk for hearing damage\footnote{The 3M Professional earmuffs had the lowest frequency-dependent SNR rating of -21 dB at 125 Hz.  This would technically allow ambient noise up to approximately 105 dB prior to encountering a risk for hearing damage while wearing the earmuffs.  Greater care would be required to ensure the earmuffs were properly fitted and that nothing compromised the earmuff seal.}, despite being the most `authentic' scenario, ie. testing the directional microphones' capabilities at the mouth and ear to eliminate noise.  Given the SNR of +6 dB for the 80 dB noise condition (with some 80 dB noise conditions having upwards of +10 dB SNR), this would mean increasing the ambient noise from 80 dB to 96-100 dB in order to achieve the -10 dB SNR that was originally desired.  Even a 0 dB SNR would require a +10 dB increase to 90 dB ambient noise in order to turn the observed (approximate) +10 dB SNR into 0 dB SNR. The risk to participants is substantially mitigated by the use of 30 dB NRR s, however at noises of this magnitude, it would be difficult to find a location which insulated the sound from affecting a significant radius outside the sound booth.  It is also very likely that a more robust loudspeaker would be required to reach the needed amplitudes without clipping.

Option (c) - using an omnidirectional microphone - does not rely on speakers' ability to modulate their voice, nor does it introduce additional risk by increasing the ambient noise.  The only drawback, as mentioned above, is that the use of an omnidirectional microphone results in an unfair comparison between the mouth-recorded speech and ear-recorded speech.  Any real-life application will use a directional microphone at the mouth with the intention of eliminating as much ambient noise as possible.  Nevertheless, this is likely the best option for future research in this area, given the restrictions described above.

\section{Summary}\label{chap2:summary}

Despite some speaker variation, the speech recorded from inside the ear canal contains information up to approximately 2.7 kHz.  This upper cut-off frequency was in the same range described by the aforementioned literature, although the low mid-range frequencies were dampened much more than the literature described.  Two, very basic acoustic transformations (pre-emphasis and bandpass filtering) were used in an attempt to create a more intelligible signal from the speech collected at the ear.
%, the heavily lowpass filtered speech becomes remarkably intelligible, considering the location of recording and the minimal post-processing effort required. 

%Given the relatively well-preserved lower frequency speech information that is present in the signal, no further acoustic transformation to these frequencies is deemed necessary\footnote{E.g. such as a spectral subtraction technique based on the frequency responses given in the literature by \cite{hansen:97b} and \cite{reinfeldt:10}, among others.}.  
Limited benefits might be seen from a sound or sound-category specific alteration, particularly among fricative sounds and those with the majority of speech information located in frequencies above 2.7 kHz.  It seems unlikely though, that a reverse spectral subtraction of sorts (eg. using the explicit transfer functions proposed by \cite{hansen:97b} and \cite{reinfeldt:10}) or a similar method would offer much additional benefit, as much of the upper frequencies were dampened beyond the existing noise level (c.f. Fig. \ref{spctgrmEarNarrow20kHz}).  However, it is hypothesized, given the transformed data collected at the ear, that enough information is present for the speech to be recognizable.  This hypothesis will be tested in a human perception experiment, described in Chapter \ref{chapter3}, and an ASR experiment, described in Chapter \ref{chapter4}, below.




<<Exp_1_Graph, echo=FALSE, eval = TRUE, fig.cap = "Plot 1", results='asis', include=TRUE>>=

#Visualize Exp.1 Data:

#Read in Exp.1 data:
# Exp1Data <- read.csv("/Users/mwilli/Documents/Spring_2017/Dissertation_Data/Experiment_1/Exp1_Summary_Data_for_Stats/Exp1_Dissertation_Data_Summary.csv")
# Exp1Data$Hypothesis <- revalue(Exp1Data$Hypothesis, c("LE"="Locus Equation", "RFDP"="Relative Formant Deflection Pattern"))
# 
# #Bar Graph with error bar Exp.1:
# ggplot(Exp1Data, aes(x=Hypothesis, y=Percent_Participant_Agreement, fill=Vowel_Context)) + 
#     stat_summary(fun.y = mean, geom = "bar", position="dodge") + 
#     stat_summary(fun.data = mean_se, geom = "errorbar", position="dodge") +ggtitle("Hypothesis Comparison") + labs(x=" Hypothesis ", y=" % Participant Agreement") +ylim(0,100)

@

<<Exp_1_Data, echo=FALSE, results='asis', include=TRUE>>=

#Stats with Mean and Standard deviation:
# library(doBy)
# tab <- summaryBy(Percent_Participant_Agreement ~ Hypothesis + Vowel_Context, data = Exp1Data, FUN = function(x) { c(m = mean(x), s = sd(x)) } )
# 
# library(xtable)
# tab2 <- xtable(tab, caption = "Hypothesis Evaluation Results")
# print(tab2)


@
