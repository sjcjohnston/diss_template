%\Sexpr{set_parent(â€˜/Users/mwilli/Documents/Spring_2017/Dissertation_Document/Dissertation_Working_Directory_Draft/Dissertation_Main.Rnw')}

<<chunk_options, echo=FALSE>>=
# This is where we set basic knitr options.
opts_chunk$set(echo=TRUE, message=FALSE, warning = FALSE, cache = FALSE, comment="")
options(width=75) # This sets how wide the R printout can be.
@

 <<setup-child, include=FALSE>>=
 set_parent('/home/sam/Dissertation/Dissertation_Template/Dissertation_Main_Template/Dissertation_Main.Rnw')
@

<<load_libraries, echo=FALSE>>=
library(tidyr)
library(dplyr)
library(ggplot2)
library(lme4)
library(lsmeans)
library(car)
library(pbkrtest)
library(xtable)
library(cowplot)
library(plyr)
@

\chapter{Overall Discussion\label{chapter5}}


The primary goal of this project is to record a small corpus of ear-recorded speech, and test the ability of humans and ASR systems to accurately recognize the speech content.  More specifically, the project aims to determine a) if ear-recorded speech, using an earplug and ear muffs, is able to significantly filter out background noise, b) if ear-recorded speech contains enough information to be intelligible, c) if human listeners find ear-recorded speech more intelligible than the exact same signal recorded at the mouth in a noisy environment, and d) if an ASR system finds ear-recorded speech more intelligible than speech recorded at the mouth in a noisy environment.

The following three sections will briefly review the results of the three experiments discussed in this report: a) the data collection experiment which gathered the ear-recorded speech, b) the human speech perception experiment testing ear-recorded and noisy mouth-recorded speech, and c) the ASR experiement testing the ability of an ASR system to recognize ear-recorded speech compared with noisy mouth-recorded speech.


\section{Review of Ear-Recorded Speech}\label{sec:chap2-review}

Ear-recorded speech was collected and analyzed from 20 participants.  The microphone was placed into a silicone earplug and into the participants' right ear (facing the tympanic membrane), and noise reduction earmuff were placed over top of their ears.  A rod extending out from the side of the earmuffs was used to place a microphone recording the speech as it came from their mouth.  A loudspeaker was placed off to the side, which emitted background noise.  The participants read aloud stimulus sentences, and were recorded, while various kinds of background noises (bus, caf\'{e}, pedestrian area, street, factory) were produced by the loudspeaker.  For more details concerning the experimental methods, refer to Chapter 2\ref{chapter2}, Section \ref{expt1}.

These recordings were then observed.  Overall, the ear-recorded speech was highly lowpass filtered, and, subjectively, it had a very `muffled' quality, which can be visualized with the spectra and spectrograms in Figs. \ref{} and \ref{}, each from a different utterance spoken by a different participant.  To test the similarity between the mouth-recorded signals and their ear-recorded counterparts, the maximum value of the cross-correlation matrix was obtained and normalized \textbf{(ie. to obtain values between 0 and 1, where 1 indicates identical signals)}, and this value was averaged over all clean-speech utterances by all speakers.  Only utterances with no background noise were included in this calculation, in order to only test the similarity of the speech and not the noise.  This test yielded a maximum normalized cross-correlation value of 0.358 as an average across all included utterances.

To increase the amplitude of the speech information in the higher frequencies, the author pre-emphasized the signal (Fig. \ref{x}).  As there was no speech information found above approximately 2.7 kHz, and since the pre-emphasis resulted in the presence of white noise in the higher frequencies, the signal was then low-pass filtered at 2500 Hz with a slope of 500 Hz (Fig. \ref{x}).  The lower frequencies in this signal were still much more prominent than the upper frequencies, and so pre-emphasis was applied a second time (Fig. \ref{}).  The average maximum normalized cross-correlation metric was obtained using the newly transformed ear-recorded speech, and each signal was compared to its simultaneously mouth-recorded counterpart.  As before, only clean speech, with no background noise, was included in this comparison.  This gave a nearly doubled cross-correlation value of 0.674.

The spectra and spectrograms for the sentence ``He said the same phrase thirty times'', spoken by a male participant, are provided for better visualization of speech intelligibility and noise reduction.  A comparison between speech recorded at the mouth and at the ear, with transformations, and with no background noise can be seen in Figures \ref{x} and \ref{y}); the same utterance is displayed, spoken at the mouth with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{} and \ref{}) and at the ear (after transformations) with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{}, and \ref{}).  Each mouth- and ear-recorded pair is the exact same utterance; they were recorded simultaneously (e.g. the already mentioned Figures \ref{x} at the ear and \ref{y} at the mouth).

The figures containing noisy speech recorded at the mouth (ie. Figs. \ref{}, \ref{}, and \ref{}) demonstrate one of the limitations of the collected data - that the SNR of the signal recorded by the mouth microphone was very high for all noise categories.  The noise did not drown out the speech in the high noise (80 dB) condition as originally intended.  The SNR needed to be high to compare the recognition performance of humans and computers on the ear-recorded speech, compared with the noisy mouth speech.  If the noise wasn't sufficiently high enough to interfere with the recognition of speech, then the mouth-recorded speech would be too easy to understand, and an adequate comparison between ear-recorded speech and noisy mouth-recorded speech could not be made.

Despite the high SNRs in the mouth-recorded signals, noise is still present.  When, by comparison, one looks at the ear-recorded signals that were also recorded in the noisy environment (ie. Figs. \ref{}, \ref{}, and \ref{}), the passive noise reduction that is provided by the head, earplug, and earmuffs appears to have successfully eliminated a substantial proportion of the noise from the signal.

These recordings demonstrate that using the ear canal as a location of recording speech in noise can offer a large reduction in the noise that enters the signal.  Furthermore, ear-recorded signals can provide speech comparable to mouth-recorded speech up to approximately 2.7 kHz.  This is relatively close to the 3.5 kHz cutoff generally used for telephonic communication, which does provide intelligible speech for both humans and computers. The author hypothesized that speech recorded from the ear and cut off at 2.7 kHz would also be intelligible for humans and for computers.  The following sections will review the results of the experiments in Chapters 3\ref{chapter3} and 4\ref{chapter4}, which aimed to test this hypothesis.


\section{Review of Human Perception of Ear-Recorded Speech}\label{sec:chap3-review}

\textbf{Will be included with revised statistics}


\section{Review of Automatic Speech Recognition of Ear Recorded Speech}\label{sec:chap4-review}

The author used the Kaldi ASR system (\cite{povey:11}) and the acoustic and language models trained on the data from the LibriSpeech corpus (\cite{panayotov:15}) to test the ability of a standard ASR system to recognize both ear-recorded speech and noisy mouth-recorded speech.  For testing data, the ear- and mouth-recorded speech was used which was obtained in the data collection experiment - presented in Chapter 2\ref{chapter2} and reviewed in Section \ref{sec:chap2-review}.  The ear-recorded speech was transformed with pre-emphasis, lowpass filtering, and a second application of pre-emphasis (described in Chapter 2\ref{chapter2} and in Section \ref{sec:chap2-review}). For more details about these methods, refer to Chapter 4\ref{chapter4}, Section \ref{expt3}

The clean speech from the ear and the mouth can be compared in Table \ref{tab:clean-wers}, and a comparison of the transformed ear-recorded speech and the noisy mouth-recorded speech can be seen in Table \ref{tab:disc-all-wers}.  These tables demonstrate that, in all but a few noisy conditions, an ASR system will more easily recognize noisy speech than it will ear-recorded speech.  

\begin{table}[h]
\begin{center}
\begin{tabular}{| c | c |} \hline
 Mouth-Recorded, Clean & Ear-Recorded, Clean \\ \hline\hline
 9.38 & ?? \\ \hline
\end{tabular}
\end{center}
\caption{A comparison of ASR performance on mouth-recorded speech and ear-recorded speech with no background noise. These values are obtained from the highest-performing (4-gram) language model, utilizing the 960-hour LibriSpeech DNN acoustic model - both trained using the LibriSpeech corpus.  All values are give as WER.}\label{tab:clean-wers}
\end{table}


\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
      & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
      & Mth & Ear & Mth & Ear & Mth & Ear & Mth & Ear & Mth & Ear \\ \hline\hline
60 dB & 21.5 & ?? & 20.3 & ?? & 18.6 & ?? & 19.0 & ?? & 17.9 & ??  \\ \hline
70 dB & 41.7 & ?? & 32.9 & ?? & 32.0 & ?? & 36.4 & ?? & 29.9 & ??  \\ \hline
80 dB & 88.2 & ?? & 73.3 & ?? & 75.6 & ?? & 85.0 & ?? & 71.4 & ??  \\ \hline
\end{tabular}
\end{center}
\caption{These values are obtained from the highest-performing (4-gram) language model, utilizing the 960-hour LibriSpeech DNN acoustic model - both trained using the LibriSpeech corpus.  Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type), with each subcolumn comparing the performance on mouth-recorded speech in that condition with ear-recorded speech in that condition.  All values are given as WER.}\label{tab:disc-all-wers}
\end{table}

In an attempt to improve the recognition of ear-recorded speech, two additional tests were performed, mimicking the additional investigations performed in Chapter 3\ref{chapter3} and in Section \ref{sec:chap3-review} above.  The first combined the pre-emphasized, ear-recorded speech, which is lowpass filtered to 2.5 kHz with a 500 Hz slope, with the simultaneous speech recorded at the mouth, which was bandpass filtered from 3.0 kHz to 8.0 kHz, with a 500 Hz slope in either direction\footnote{The mouth-recorded speech was already lowpass filtered to 8.0 kHz.}.  These ASR performance on these recordings are displayed in Table \ref{tab:e-m-combo}, with `E/M' referring to the ear-and-mouth combination speech.  The ASR performance on the regular, noisy mouth-recorded speech is repeated from Table \ref{tab:disc-all-wers} above for ease of comparison.

\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
      & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
      & Mth & E/M & Mth & E/M & Mth & E/M & Mth & E/M & Mth & E/M \\ \hline\hline
60 dB & 21.5 & ?? & 20.3 & ?? & 18.6 & ?? & 19.0 & ?? & 17.9 & ??  \\ \hline
70 dB & 41.7 & ?? & 32.9 & ?? & 32.0 & ?? & 36.4 & ?? & 29.9 & ??  \\ \hline
80 dB & 88.2 & ?? & 73.3 & ?? & 75.6 & ?? & 85.0 & ?? & 71.4 & ??  \\ \hline
\end{tabular}
\end{center}
\caption{These values are obtained from the highest-performing (4-gram) language model, utilizing the 960-hour LibriSpeech DNN acoustic model - both trained using the LibriSpeech corpus.  Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type), with each subcolumn comparing the performance on mouth-recorded speech in that condition with the combination of lowpass ear-recorded speech and `higher' bandpassed mouth-recorded speech in the same condition.  All values are given as WER.}\label{tab:disc-all-wers}
\end{table}

\textbf{Further discussion will be included when results are obtained.}

A common method of improving ASR performance on speech with different characteristics (e.g. in noise, with different dialects, etc.) is to train the ASR acoustic model on speech in that domain.  This also mimics the use of additional training for the human speech perception task described in Chapter 3\ref{chapter3} and Section \ref{sec:chap3-review} above.  Rather than recording enough ear-recorded speech to train a model, the 100-hour dataset from the LibriSpeech corpus was used.  The speech in this dataset was lowpassed with the exact same filter used on the ear-recorded speech - 2.5 kHz with a 500 Hz slope.  A speaker-adapted GMM-HMM model was trained on this data.  Due to resource constraints, computationally and otherwise, only the 100-hour dataset was used (not 960 hour), and a DNN model was not trained.  These results are displayed in Table \ref{tab:retrainedGMM}.

\begin{table}[h]
\begin{center}
\begin{tabular}

\end{tabular}
\end{center}
\caption{}\label{tab:retrainedGMM}
\end{table}

\textbf{Further discussion will be included when results are obtained.}

Note that the model used for the ear-recorded speech in Table \ref{tab:retrainedGMM} was trained on significantly less data than the model used for other tests discussed above.  If additional training data were used and if a DNN (rather than GMM) model were trained, results would be expected to improve.


\section{Conclusions}



\section{Future Direction}


