% %set_parent(â€˜/Users/mwilli/Documents/Spring_2017/Dissertation_Document/Dissertation_Working_Directory_Draft/Dissertation_Main.Rnw')
% 
% <<chunk_options, echo=FALSE>>=
% # This is where we set basic knitr options.
% opts_chunk$set(echo=TRUE, message=FALSE, warning = FALSE, cache = FALSE, comment="")
% options(width=75) # This sets how wide the R printout can be.
% @
% 
%  <<setup-child, include=FALSE>>=
%  set_parent('/home/sam/Dissertation/Dissertation_Template/Dissertation_Main_Template/Dissertation_Main.Rnw')
% @
% 
% <<load_libraries, echo=FALSE>>=
% library(tidyr)
% library(dplyr)
% library(ggplot2)
% library(lme4)
% library(lsmeans)
% library(car)
% library(pbkrtest)
% library(xtable)
% library(cowplot)
% library(plyr)
% @

\chapter{Overall Summary and Discussion\label{chapter5}}


The primary goal of this project \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend to record a small corpus of ear-recorded speech, and test the ability of humans and ASR systems to accurately recognize the speech content.  More specifically, the project \DIFdelbegin \DIFdel{aims }\DIFdelend \DIFaddbegin \DIFadd{aimed }\DIFaddend to determine a) if \DIFdelbegin \DIFdel{ear-recorded speech, }\DIFdelend using an earplug and \DIFdelbegin \DIFdel{ear muffs, is able to }\DIFdelend \DIFaddbegin \DIFadd{earmuffs would }\DIFaddend significantly filter out background noise \DIFaddbegin \DIFadd{from a signal recorded from the ear}\DIFaddend , b) if ear-recorded speech \DIFdelbegin \DIFdel{contains }\DIFdelend \DIFaddbegin \DIFadd{contained }\DIFaddend enough information to be intelligible, c) if human listeners \DIFdelbegin \DIFdel{find }\DIFdelend \DIFaddbegin \DIFadd{found }\DIFaddend ear-recorded speech more intelligible than the exact same signal recorded at the mouth in a noisy environment, and d) if an ASR system \DIFdelbegin \DIFdel{finds }\DIFdelend \DIFaddbegin \DIFadd{is more effective at recognizing speech content from }\DIFaddend ear-recorded speech \DIFdelbegin \DIFdel{more intelligible }\DIFdelend than speech recorded at the mouth in a noisy environment. \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend The following three sections briefly review the task and results of the three experiments discussed in this report \DIFdelbegin \DIFdel{: a) the data collection experiment which gathered the }\DIFdelend \DIFaddbegin \DIFadd{pertaining to the questions listed above. Specifically, the collection of }\DIFaddend ear-recorded speech \DIFdelbegin \DIFdel{, b)}\DIFdelend \DIFaddbegin \DIFadd{(cf. Section \ref{sec:chap2-review}), }\DIFaddend the human speech \DIFdelbegin \DIFdel{perception experiment testing human ability to perceive }\DIFdelend \DIFaddbegin \DIFadd{recognition ability of }\DIFaddend ear-recorded \DIFdelbegin \DIFdel{and noisy mouth-recorded speech }\DIFdelend \DIFaddbegin \DIFadd{speech (cf. Section \ref{sec:chap3-review})}\DIFaddend , and c) the ASR \DIFdelbegin \DIFdel{experiment testing the ability of an ASR system to recognize }\DIFdelend \DIFaddbegin \DIFadd{recognition ability of }\DIFaddend ear-recorded speech \DIFdelbegin \DIFdel{compared with noisy mouth-recorded speech}\DIFdelend \DIFaddbegin \DIFadd{(cf. Section \ref{sec:chap4-review})}\DIFaddend .  Limitations of the conducted studies and directions for future research will be discussed in Section \ref{chap5:future-research}, \DIFdelbegin \DIFdel{and the over-arching findings }\DIFdelend \DIFaddbegin \DIFadd{the overall findings and implications }\DIFaddend of this report \DIFdelbegin \DIFdel{will be briefly summarized in Section }\DIFdelend \DIFaddbegin \DIFadd{are discussed in Section \ref{chap5:implications}, and a broad concluding summary of the entire paper is given Section }\DIFaddend \ref{chap5:conclusions}.


\DIFdelbegin %DIFDELCMD < \section{Summary of Ear-Recorded Speech}%%%
\DIFdelend \DIFaddbegin \section{Summary of Ear-Recorded Speech Collection}\DIFaddend \label{sec:chap2-review}

Ear-recorded speech was collected and analyzed from 20 participants.  \DIFdelbegin \DIFdel{The }\DIFdelend \DIFaddbegin \DIFadd{A }\DIFaddend microphone was placed into a silicone earplug and into the participants' right ear (facing the tympanic membrane), and noise reduction \DIFdelbegin \DIFdel{earmuff }\DIFdelend \DIFaddbegin \DIFadd{earmuffs }\DIFaddend were placed over top of their ears.  A rod extending out from the side of the earmuffs was used to place a microphone recording the speech as it came from their mouth.  A loudspeaker was placed off to the side, which emitted background noise.  The participants \DIFdelbegin \DIFdel{read aloud stimulus sentences, and were recorded , }\DIFdelend \DIFaddbegin \DIFadd{were recorded reading stimulus sentences aloud }\DIFaddend while various kinds of background noises (bus, caf\'{e}, pedestrian area, street, factory) were produced by the loudspeaker.  For more details concerning the experimental methods, refer to Chapter \ref{chapter2}, Section \ref{expt1}.

These recordings were then \DIFdelbegin \DIFdel{observed}\DIFdelend \DIFaddbegin \DIFadd{analyzed}\DIFaddend .  Overall, the ear-recorded speech was highly low-pass filtered, and, subjectively, it had a very `muffled' quality.  To test the similarity between the mouth-recorded signals and their ear-recorded counterparts, the maximum value of the cross-correlation matrix was obtained and normalized (ie. to obtain values between 0 and 1, where 1 indicates identical signals)\DIFdelbegin \DIFdel{, and this }\DIFdelend \DIFaddbegin \DIFadd{.  This }\DIFaddend value was averaged over all clean-speech utterances by all speakers.  Only utterances with no background noise were included in this calculation, in order to only test the similarity of the speech and not the noise.  This test yielded a maximum normalized cross-correlation value of 0.358 as an average across all included utterances.

To increase the amplitude of the speech information in the higher frequencies \DIFaddbegin \DIFadd{which had been heavily filtered}\DIFaddend , the author pre-emphasized the signal.  As there was no speech information found above approximately 2.7 kHz, and since the pre-emphasis resulted in the presence of white noise in the higher frequencies, the signal was then low-pass filtered at 2.5 kHz with a slope of 500 Hz.  The lower frequencies in this signal were still much more prominent than the upper frequencies, and so pre-emphasis was applied a second time.  The average maximum normalized cross-correlation metric was obtained using the newly transformed ear-recorded speech, and each signal was compared to its simultaneously mouth-recorded counterpart.  As before, only clean speech, with no background noise, was included in this comparison.  This gave a nearly doubled cross-correlation value of 0.674\DIFaddbegin \DIFadd{, which was statistically different from the cross-correlation of the un-processed ear-recorded signals and the mouth-recorded signals}\DIFaddend .

% The spectra and spectrograms for the sentence ``He said the same phrase thirty times'', spoken by a male participant, are provided for better visualization of speech intelligibility and noise reduction.  A comparison between speech recorded at the mouth and at the ear, with transformations, and with no background noise can be seen in Figures \ref{x} and \ref{y}); the same utterance is displayed, spoken at the mouth with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{} and \ref{}) and at the ear (after transformations) with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{}, and \ref{}).  Each mouth- and ear-recorded pair is the exact same utterance; they were recorded simultaneously (eg. the already mentioned Figures \ref{x} at the ear and \ref{y} at the mouth).

%The figures containing noisy speech recorded at the mouth (ie. Figs. \ref{}, \ref{}, and \ref{}) demonstrate 
One of the limitations of the collected data was that the SNR of the signal recorded by the mouth microphone was very high for all noise categories.  The noise did not drown out the speech in the high noise (80 dB) condition as originally intended.  The SNR needed to be high to compare the recognition performance of humans and computers on the ear-recorded speech, compared with the noisy \DIFdelbegin \DIFdel{mouth }\DIFdelend \DIFaddbegin \DIFadd{mouth-recorded }\DIFaddend speech.  If the noise \DIFdelbegin \DIFdel{wasn't }\DIFdelend \DIFaddbegin \DIFadd{were not }\DIFaddend sufficiently high enough to interfere with the recognition of speech, then the mouth-recorded speech would be too easy to understand, and an adequate comparison between ear-recorded speech and noisy mouth-recorded speech could not be made.

Despite the high SNRs in the mouth-recorded signals, noise \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend still present.  Considering the ear-recorded signals that were also recorded in the noisy environment%DIF < (ie. Figs. \ref{}, \ref{}, and \ref{}), 
\DIFaddbegin \DIFadd{, }\DIFaddend the passive noise reduction that \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend provided by the head, earplug, and earmuffs \DIFdelbegin \DIFdel{appears }\DIFdelend \DIFaddbegin \DIFadd{appeared }\DIFaddend to have successfully eliminated a substantial proportion of the noise from the signal.

These recordings \DIFdelbegin \DIFdel{demonstrate }\DIFdelend \DIFaddbegin \DIFadd{demonstrated }\DIFaddend that using the ear canal as a location of recording speech in noise can offer a large reduction in the noise that enters the signal.  Furthermore, ear-recorded signals can provide speech comparable to mouth-recorded speech up to approximately 2.7 kHz.  This is relatively close to the 3.5 kHz cutoff generally used for telephonic communication, which does provide intelligible speech for both humans and computers. The author hypothesized that speech recorded from the ear and cut off at 2.7 kHz would also be intelligible for humans and for computers.  The following sections \DIFdelbegin \DIFdel{will }\DIFdelend review the results of the experiments in Chapters \ref{chapter3} and \ref{chapter4}, which aimed to test this hypothesis.


\section{Summary of Human Perception of Ear-Recorded Speech}\label{sec:chap3-review}

Due to the high SNR of the speech recorded in the data collection experiment \DIFdelbegin \DIFdel{, described in }\DIFdelend \DIFaddbegin \DIFadd{(cf. }\DIFaddend Chapter \ref{chapter2} and Section \ref{sec:chap2-review}\DIFdelbegin \DIFdel{above}\DIFdelend \DIFaddbegin \DIFadd{)}\DIFaddend , two additional speakers were recorded in the same manner, ie. with a microphone placed in an earplug and in their ear canal, \DIFaddbegin \DIFadd{and }\DIFaddend with earmuffs placed over their ears.  Another directional microphone was placed in front of their mouth, except instead of pointing at their mouth, the microphone pointed at the loudspeaker \DIFdelbegin \DIFdel{in the room, which produced the }\DIFdelend \DIFaddbegin \DIFadd{producing }\DIFaddend background noise. The ear-recorded speech was pre-emphasized and low-pass filtered in the same manner as the recordings conducted in Chapter \ref{chapter2}. For further details about the collection of these additional recordings, refer to Chapter \ref{chapter3}, Section \ref{expt2}.

The speech recorded from these two speakers was used as stimuli for the human perception experiment.  There were 24 participants, all native speakers of English, who listened to the provided stimuli.  There were 24 conditions with 3 factors, gender of the speaker of the stimulus (male,female), recording location of the stimulus (in front of mouth, in ear canal) and noise type (clean, bus, caf\'{e}, pedestrian area, street, factory).  The stimuli were placed in counter-balanced groups, where each sentence stimulus occurred in each of the 24 conditions.  Each of the 24 participants received one of the 24 counter-balanced stimulus groups, meaning that each distinct sentence-condition pairing was only heard by one participant listener.
%Each participant listener heard each stimulus utterance once, and heard three utterances per condition.  This yielded 72 stimuli (72 distinct utterances), with each of the 24 conditions repeated three times.  Each listener could only hear a given utterance once before they would be given an advantage to understanding it.  To avoid this, counter-balanced lists were developed to place each stimulus in each of the 24 conditions, requiring

The participant listeners heard an utterance, and typed what they heard into a response box.  They were instructed that what was heard might not be a complete sentence, and to only type what they thought they heard.  These responses were recorded for each participant, and WER was calculated between the actual utterance transcription and the proposed transcription provided by the participant listeners.

Two, 3-way ANOVAs was conducted on the data, with dependent variable WER.  Factors for the first included `speaker [of the stimulus]'s gender' (female, male), `recording [mic] location' (mouth, ear), and `noise type' (clean, bus, caf\'{e}, pedestrian, street, factory).  The second ANOVA removed the level `clean' (no noise) from the factor of noise, because it differed substantially from the other levels in that factor.  Both \DIFdelbegin \DIFdel{ANOVA }\DIFdelend \DIFaddbegin \DIFadd{ANOVAs }\DIFaddend found significant interactions of speaker gender \textbf{x} recording `mic' location and noise type \textbf{x} microphone location.  Factors were split to test simple effects.  \DIFdelbegin \DIFdel{Of note, there }\DIFdelend \DIFaddbegin \DIFadd{There }\DIFaddend was a statistical difference for all simple effects of microphone location, indicating that listeners more easily recognize ear-recorded speech than [sufficiently] noisy mouth-recorded speech.  Additionally, when the `clean' level was removed from the ANOVA, and simple effects of noise were tested, there was a statistical difference of noise for mouth-recorded speech, but no statistical difference for ear-recorded speech.  This \DIFdelbegin \DIFdel{implies }\DIFdelend \DIFaddbegin \DIFadd{implied }\DIFaddend that the noise level was sufficiently dampened in the ear-recorded speech that the various noise types had no effect.  Considering the ear-recorded speech at every level of speaker gender and noise, the WER hovered \DIFdelbegin \DIFdel{mainly }\DIFdelend \DIFaddbegin \DIFadd{primarily }\DIFaddend between 20-30\%.

Two additional investigative studies were performed; the first added high-pass filtered \DIFdelbegin \DIFdel{mouth recorded }\DIFdelend \DIFaddbegin \DIFadd{mouth-recorded }\DIFaddend speech to the low-pass filtered ear-recorded stimuli \DIFaddbegin \DIFadd{to create `hybrid' stimuli}\DIFaddend , and the second added a brief training session - listening to the narration of a story recorded from the ear canal - before proceeding to the task.  There were not enough participants in either of these studies to run statistics, so all results are speculative.  These were compared to the primary study (above) to determine if there might be any improvement in the recognition of ear-recorded speech by utilizing these methods.  No substantial improvement was observed from the participants who were exposed to a pre-task training session.  Very minor improvement was observed for the \DIFdelbegin \DIFdel{high/low frequency combination }\DIFdelend \DIFaddbegin \DIFadd{`hybrid' stimulus }\DIFaddend study; there were a few conditions that did see a stark improvement, including the `clean' noise condition.

In summary, the use of ear-recorded speech as a noise-robust method of communication seems promising, as it is more easily recognizable than speech spoken from the mouth in noise.  However, even a WER of 20\%, ie. the ability to correctly understand only 4 out of every 5 words, is not sufficient for most situations requiring human communication.  The two methods employed do not seem to offer much improvement, though these - and other possibilities for improving recognition - should be fully tested in future research.



\section{Summary of Automatic Speech Recognition of Ear Recorded Speech}\label{sec:chap4-review}

The author used the Kaldi ASR system (\cite{povey:11}) and the acoustic and language models trained on the data from the LibriSpeech corpus (\cite{panayotov:15}) to test the ability of a standard ASR system to recognize both ear-recorded speech and noisy mouth-recorded speech.  For testing data, the ear- and mouth-recorded speech that was \DIFdelbegin \DIFdel{used was }\DIFdelend obtained in the data collection experiment \DIFdelbegin \DIFdel{- presented in }\DIFdelend \DIFaddbegin \DIFadd{was used (cf. }\DIFaddend Chapter \ref{chapter2} and \DIFdelbegin \DIFdel{reviewed in Section \ref{sec:chap2-review}.  The ear-recorded speech was transformed with pre-emphasis, }\DIFdelend \DIFaddbegin \DIFadd{Section \ref{sec:chap2-review}). Several ASR models were used, including a 960-DNN model, a 100-hour DNN model, and a 100-hour DNN model trained on }\DIFaddend low-pass \DIFdelbegin \DIFdel{filtering, and a second application of pre-emphasis (described in Chapter \ref{chapter2} and in Section \ref{sec:chap2-review})}\DIFdelend \DIFaddbegin \DIFadd{filtered mouth-recorded speech}\DIFaddend .  For more details about \DIFdelbegin \DIFdel{these }\DIFdelend \DIFaddbegin \DIFadd{the ASR }\DIFaddend methods, refer to Chapter \ref{chapter4}, Section \ref{expt3}.

The performance of the \DIFaddbegin \DIFadd{960-hour }\DIFaddend ASR system on \DIFdelbegin \DIFdel{clean }\DIFdelend mouth-recorded speech \DIFdelbegin \DIFdel{(9.38}%DIFDELCMD < \% %%%
\DIFdel{WER) and clean }\DIFdelend \DIFaddbegin \DIFadd{and }\DIFaddend ear-recorded speech \DIFdelbegin \DIFdel{(79.30}%DIFDELCMD < \% %%%
\DIFdel{WER) }\DIFdelend demonstrated a tremendous divergence, with mouth-recorded speech substantially out-performing ear-recorded speech.  As the noise level increased, \DIFaddbegin \DIFadd{and consequently as the SNR of the mouth-recorded speech dropped, }\DIFaddend this divergence shrunk\DIFdelbegin \DIFdel{, but }\DIFdelend \DIFaddbegin \DIFadd{.  But }\DIFaddend even at the highest noise level (80 dB), the ear-recorded speech \DIFaddbegin \DIFadd{barely }\DIFaddend out-performed the \DIFdelbegin \DIFdel{mouth recorded speechin only one noise type (`bus')}\DIFdelend \DIFaddbegin \DIFadd{mouth-recorded speech}\DIFaddend .  Furthermore, these WERs are prior to any application of noise-removal techniques beyond standard, built-in procedures that are used during Kaldi feature extraction.  If additional noise-removal techniques were to be applied, such as Advanced Front End (AFE), there would be an even greater gap between ASR performance on ear- and mouth-recorded speech.

\DIFdelbegin \DIFdel{It was very possible that }\DIFdelend \DIFaddbegin \DIFadd{Since }\DIFaddend the acoustic model - trained on 960 hours of mouth-recorded speech - was searching for speech information in the higher frequencies that simply did not exist in the low-frequency ear-recorded signal\DIFdelbegin \DIFdel{.  Another model was trained on a much smaller subset of the 960-hour corpus (75 hours) of mouth-recorded speech, which was low-passed to the same frequency range as ear-recorded speech.  Performance worsened even further when using this model.  It may be that low-passed mouth-recorded speech cannot substitute for ear-recorded speech when training acoustic models for ASR.  This is telling, that humans are able to understand the ear-recorded speech with a reasonable degree of accuracy, and yet the ASR system's peformance suffers greatly.  It would seem that humans are using speech cues in the lower frequencies that the ASR system is not, and/or the ASR system is using speech cues in the higher frequencies that humans do not need.  }%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{A final }\DIFdelend \DIFaddbegin \DIFadd{, these results were not unanticipated.  Another }\DIFaddend test was performed\DIFdelbegin \DIFdel{using the technique of }\DIFdelend \DIFaddbegin \DIFadd{. This used slightly modified stimuli, created by }\DIFaddend adding the high-frequency portion of the mouth-recorded speech to the low-frequency ear-recorded speech \DIFdelbegin \DIFdel{to reintroduce this }\DIFdelend \DIFaddbegin \DIFadd{in an effort to reintroduce the }\DIFaddend upper frequency information that the ASR system may be looking for.  This same technique was also used in Chapter \ref{chapter3} for human speech perception.  For these tests, the original acoustic model trained on 960 hours of speech was used.  Performance of this `hybrid' \DIFdelbegin \DIFdel{speech data increased by as much as 14}%DIFDELCMD < \% %%%
\DIFdel{WER in the 80 dB noise condition, and in the 60 dB noise condition, by as much as 30}%DIFDELCMD < \% %%%
\DIFdel{WER.  All noise types in the 80 dB noise conditions - except one (`factory') - saw an improvement in WER from the mouth-recorded noisy speech to the hybrid mouth- and }\DIFdelend \DIFaddbegin \DIFadd{data improved performance only moderately compared with the regular }\DIFaddend ear-recorded \DIFdelbegin \DIFdel{combination.  Nevertheless, for the much higher-SNR 70 dB and 60 dB noise conditions, performance worsened when using the `hybrid' speech over the `plain' noisy mouth-recorded speech.  This hints }\DIFdelend \DIFaddbegin \DIFadd{data.  These results hinted }\DIFaddend that the ear-recorded speech itself might be \DIFdelbegin \DIFdel{subtley }\DIFdelend \DIFaddbegin \DIFadd{subtly }\DIFaddend different from the mouth-recorded speech in the same frequency range, and that for substantial improvements to be seen, an acoustic model needs to be trained with ear-recorded speech.

\DIFdelbegin \DIFdel{From these results, it appears that }\DIFdelend \DIFaddbegin \DIFadd{Another model was trained on a 100-hour subset of the LibriSpeech corpus of mouth-recorded speech; this mouth-recorded speech from the LibriSpeech corpus was low-passed to the same frequency range as }\DIFaddend ear-recorded \DIFdelbegin \DIFdel{speech only offers a benefit over speech recorded at the mouth in very noisy conditions.  The hybrid speechoffers a slight improvement, but the capability for further benefit is unknown.  Despite the poor performance of the }\DIFdelend \DIFaddbegin \DIFadd{speech.  When compared with an acoustic model trained on the same (but non-low-pass filtered) 100-hour portion of the LibriSpeech corpus, ear-recorded speech notes another slight gain (using the }\DIFaddend model trained on low-pass filtered \DIFdelbegin \DIFdel{mouth recorded speech , the greatest improvement to recognitionwill likely come from retraining or adapting }\DIFdelend \DIFaddbegin \DIFadd{data).  It is noteworthy, however, that clean or low-noise (60 dB), low-pass filtered mouth-recorded speech - using the low-pass trained acoustic model - achieves WERs within only 10}\% \DIFadd{WER of full-bandwidth mouth-recorded speech that uses the full-bandwidth model.  This indicates that there is another difference between ear-recorded and mouth-recorded speech besides the bandwidth, and furthermore, that enough critical speech is located in the available frequency range for adequate recognition.  If enough ear-recorded speech could be obtained to train }\DIFaddend an acoustic model\DIFdelbegin \DIFdel{specifically using ear-recorded speech data}\DIFdelend \DIFaddbegin \DIFadd{, it may be possible to see similar performance to that obtained by the low-pass filtered mouth-recorded speech in this study}\DIFaddend .

%DIF >  Performance of this `hybrid' speech data increased by as much as 14\% WER in the 80 dB noise condition, and in the 60 dB noise condition, by as much as 30\% WER.  All noise types in the 80 dB noise conditions - except one (`factory') - had an improvement in WER from the mouth-recorded noisy speech to the hybrid mouth- and ear-recorded combination.  Nevertheless, for the much higher-SNR 70 dB and 60 dB noise conditions, performance worsened when using the `hybrid' speech over the `plain' noisy mouth-recorded speech.  
\DIFaddbegin 

%DIF >  From these results, it appears that ear-recorded speech only offers a benefit over speech recorded at the mouth in very noisy conditions.  The hybrid speech offers a slight improvement, but the capability for further benefit is unknown.  Despite the poor performance of the model trained on low-pass filtered mouth-recorded speech, the greatest improvement to recognition will likely come from retraining or adapting an acoustic model specifically using ear-recorded speech data.


\DIFaddend \section{Future Directions}\label{chap5:future-research}

One of the primary limitations \DIFdelbegin \DIFdel{discussed throughout this report }\DIFdelend is that the SNR for the speech recorded in the original data collection experiment (cf. Chapter \ref{chapter2}) was much higher than originally intended \DIFdelbegin \DIFdel{or hoped for, }\DIFdelend by slightly more than +20 dB SNR for each condition.  This yielded noisy speech which was largely greater than +10 dB SNR; current ASR technology is rather adept at recognizing signals with these levels of noise (cf. \cite{braun:16}).  Due to this, it was difficult to see how much improvement could be gained over very noisy, low-SNR signals.  \DIFdelbegin \DIFdel{Two }\DIFdelend \DIFaddbegin \DIFadd{Several }\DIFaddend possibilities stand out as potentially workable solutions.  The first, would be to simply increase the ambient background noise level, and the second involves using an omnidirectional microphone to record the speech from the mouth (rather than a directional microphone).  \DIFaddbegin \DIFadd{A third, and most realistic solution, would be to record in real environments (eg. factory, airport, etc.) where this approach to noise-robust communication may applied.
}\DIFaddend 

The \DIFdelbegin \DIFdel{former }\DIFdelend \DIFaddbegin \DIFadd{first }\DIFaddend introduces potential risk to those exposed to the noise, depending on the level of noise\DIFdelbegin \DIFdel{and the amount of hearing protection offered.  Additionally, for }\DIFdelend \DIFaddbegin \DIFadd{.  However, sufficient ear protection would largely mitigate this problem.  For }\DIFaddend very high noise conditions, it would be difficult to limit the noise exposure to the surrounding area.  The \DIFdelbegin \DIFdel{latter }\DIFdelend \DIFaddbegin \DIFadd{second }\DIFaddend solution allows the speaker to talk normally and for the ambient noise level to remain relatively low.  However, this does not necessarily result in a fair comparison between ear-recorded and mouth-recorded speech, as in any real-life application, a directional microphone would be utilized with the specific intention of reducing the noise picked up by the microphone.  \DIFdelbegin \DIFdel{If adequate hearing protection is offered, and the noise contained, the former solutionoffers the most realistic comparison between the two microphone locations}\DIFdelend %DIF > If adequate hearing protection is offered, and the noise contained, the former solution offers the most realistic comparison between the two microphone locations.
\DIFaddbegin \DIFadd{The third approach - recording in a real-life noisy environment - offers the most realistic comparison, with realistic noise types and noise levels.  This solution, however, offers much less control over different noises, and may be more difficult to arrange the availability of both participants and an off-site recording location.  However, for testing the realistic application of this device, the third option is by far the best}\DIFaddend .

Regarding human speech perception, ear-recorded speech seems promising.  There is a significant difference between the ability of human listeners to recognize ear-recorded speech and mouth-recorded speech in noise.  Ear-recorded speech does not provide the same recognition performance as speech recorded at the mouth without background noise.  However, two methods were proposed to further improve the performance of ear-recorded speech.  The first introduced a training session prior to the actual experiment in which participants listened to the narration of a short story, recorded from the ear canal.  The second spliced the low-frequency ear-recorded speech with the higher frequencies of the mouth-recorded speech \DIFdelbegin \DIFdel{.  Neither of these tested methods were performed with enough participants (due to resource limitations) to run adequate }\DIFdelend \DIFaddbegin \DIFadd{into a `hybrid' speech stimulus.  These were run as pilot experiments to demonstrate directions for future research; there were not enough participating subjects to run }\DIFaddend statistics (due to requiring counter-balanced groups).  

\DIFdelbegin \DIFdel{The non-statistical}\DIFdelend \DIFaddbegin \DIFadd{These pilot}\DIFaddend , preliminary results for the former (training) method showed little or no improvement over the standard ear-recorded speech, while the \DIFdelbegin \DIFdel{non-statistical}\DIFdelend \DIFaddbegin \DIFadd{pilot}\DIFaddend , preliminary results for the latter (\DIFdelbegin \DIFdel{spliced}\DIFdelend \DIFaddbegin \DIFadd{hybrid}\DIFaddend ) method provided \DIFaddbegin \DIFadd{some }\DIFaddend indication that this could \DIFdelbegin \DIFdel{potentially }\DIFdelend yield improvement.  Future research should test these methods statistically.  The former (training) method may need altered methodology in order to determine whether a form of training could result in an improvement in future recognition of ear-recorded speech.  A training task that requires interaction and provides feedback may promote active attention during the training task, unlike passively listening to a story\DIFaddbegin \DIFadd{, }\DIFaddend as used in this study.  In the method using spliced stimuli, further research should pursue the possibility of using the clean, lower frequencies to actively clean any noise from the higher mouth-recorded frequencies.  

\DIFdelbegin \DIFdel{The ASR recognition task initially indicated that }\DIFdelend \DIFaddbegin \DIFadd{Regarding future directions for ASR research, there appears to be promise in training a model specifically with }\DIFaddend ear-recorded \DIFdelbegin \DIFdel{speech offered essentially no performance improvement over noisy mouth-recorded speech. This utilized a DNN ASR system trained on a 960-hour corpus of }\DIFdelend speech. A \DIFdelbegin \DIFdel{small GMM-HMM }\DIFdelend \DIFaddbegin \DIFadd{DNN }\DIFaddend model was trained on \DIFdelbegin \DIFdel{75 }\DIFdelend \DIFaddbegin \DIFadd{100 }\DIFaddend hours of speech - a subset of the 960-hour corpus - which was low-pass filtered to mimic the low-frequency ear-recorded speech.  \DIFdelbegin \DIFdel{When tested on this system, recognition accuracy on ear-recorded speechbecame worse.  It could very well be possible that the amount of speech used to train this additional model was not sufficient, or that performance would have improved by training a DNN model, though this would likely not have resulted in adequate improvement alone.
}\DIFdelend \DIFaddbegin \DIFadd{This was tested on }\DIFaddend low-pass filtered \DIFaddbegin \DIFadd{mouth-recorded speech.  The limited-bandwidth }\DIFaddend mouth-recorded \DIFdelbegin \DIFdel{speech may also not be an appropriate substitute for }\DIFdelend \DIFaddbegin \DIFadd{speech performed within 10}\% \DIFadd{WER of the full-bandwidth mouth-recorded speech.  This indicates that acoustic models are capable of recognizing speech with information within the limited-bandwidth in which ear-recorded speech falls.  If ear-recorded speech were used as the training data, significant ASR improvement is hypothesized.
}

\DIFadd{Further gains might also be obtained by actively using the clean lower-frequency ear-recorded speech to actively clean the speech information in the mouth-recorded signal.
Additional research should also pursue these other methods of improving the human or computer recognition of }\DIFaddend ear-recorded speech\DIFdelbegin \DIFdel{, and that by training a model using }\DIFdelend \DIFaddbegin \DIFadd{. 
}


\section{Overall Implications}\label{chap5:implications}

\DIFadd{Chapter \ref{chapter2} demonstrated that speech recorded at the ear can be resilient against environmental noise.  If Chapter \ref{chapter3} demonstrated that the human auditory system is relatively robust in its ability to accurately identify speech in a degraded signal, Chapter \ref{chapter4} has shown that automatic speech recognition systems remain rigid and are largely unable to account for variations in the expected signal without compensation or retraining.  It is likely that the human listeners in the speech perception experiment had never heard }\DIFaddend speech recorded from the ear\DIFdelbegin \DIFdel{would provide the needed increase in performance}\DIFdelend \DIFaddbegin \DIFadd{, yet were still able to understand the signal with a relatively high degree of accuracy and adapt to the task at hand.  ASR, on the other hand, remains relatively `rigid' in its domain-specific ability to recognize speech unless retrained or adapted to the specific task}\DIFaddend .

The resiliency and flexibility of human speech perception could be due in part to the redundancy of cues it uses to represent various speech features (\cite{winter:14}).  If one feature or set of features is rendered unintelligible (eg. voice onset time (VOT) for voicing contrast of stop sounds), others cues are available for use (eg. duration of previous vowel) in order to achieve recognition.  Many proposed ASR system modifications have incorporated human auditory functions (e.g. \cite{kim:99,fazel:12,moritz:15}), and continuing to model aspects of the human auditory system in ASR processing is likely to bring ASR performance closer to the recognition accuracy of human listeners, regardless of variabilty of the tested speech.  The information necessary for recognition exists in the ear-recorded speech signals, as the human auditory system has shown its ability to parse it with a relatively high degree of accuracy (cf. Chapter \ref{chapter2}).  With domain specific training, ASR recognition performance on ear-recorded speech will likely improve.

\DIFaddbegin
\DIFadd{Nevertheless, both chapters \ref{chapter3} and \ref{chapter4} demonstrated that - in some noisy environments }\DIFaddend - \DIFdelbegin \DIFdel{was also used for the ASR task.  This demonstrated moderate improvements to performance on the hybrid ear recorded speech, but it only dropped below the WER of mouth-recorded speechin the 80 dB noise condition and only in 4/5 noise types.  Needless to say this is still not acceptable for use in any real-life ASR application.  It is apparent that the level of noise in the high-frequency }\DIFdelend \DIFaddbegin \DIFadd{ear-recorded speech was more intelligible to both humans and computers.  Critically, the processing performed on these signals was minimal, which makes it potentially suitable for on-line ASR systems and for real-time human communication.  It requires no computationally intensive processing, and no assumption about external noise, which can take a variety of forms and intensities.  While some noise was seen to leak into the in-ear microphone signal, the SNR was very high compared with its }\DIFaddend mouth-recorded \DIFdelbegin \DIFdel{component has an effect on performance; if existing techniques for noise removal were }\DIFdelend \DIFaddbegin \DIFadd{counterpart.  If excessive levels of noise began to pollute the signal, any existing noise-removal techniques could be applied.  The author hypothesizes that any these noise-reduction methods applied to ear-recorded speech would meet greater success than those }\DIFaddend applied to the \DIFaddbegin \DIFadd{noisy, }\DIFaddend mouth-recorded \DIFdelbegin \DIFdel{speech before combination with the ear speech , this might partially mitigate the negative effect of noise observed in Table \ref{tab:hybrid-wers}.  Further gains might be obtained by actively using the clean lower-frequency ear-recorded speech to actively clean the higher frequency information in the mouth recorded signal before combining the two for recognition.  Additional research should also pursue other methods of improving the human or computer recognition of }\DIFdelend \DIFaddbegin \DIFadd{counterpart, since the ear-recorded speech provides a higher `base' SNR.  This has potential implications for the ability to convey speech information in extremely noisy situations, where normal, mouth-recorded speech would be beyond recovery with the use of current noise reduction techniques.
}

\DIFadd{Ear-recorded speech has potential applications for electronic communication in noisy environments.  Particularly those environments in which users are more likely to already be wearing hearing protection (eg. factory floors, airport tarmacs, mechanic shops, etc.) have greater potential to benefit from noise-robust human-to-human communication devices.  Additionally, as the workplace becomes more AI centered, these same devices could be used to communicate vocally to computer interfaces.  In-ear microphones would also be easier for the user to correctly position.  For example, hyper-cardioid microphones may be useful in noisy environments, but only insofar as they are pointed toward the speaker's mouth, which a user may not always do.  An in-ear microphone, by the nature that it is inserted into the speaker's ear, ensures that it is placed in the correct location.  Furthermore, if }\DIFaddend ear-recorded speech \DIFaddbegin \DIFadd{is able to obtain ASR accuracy levels similar to that of clean mouth-recorded speech, an in-ear microphone device could be a useful hands-free method to communicate with AI assistants in the home or office without the risk of being `out of range' of currently used far-field microphone arrays.
}

\DIFadd{Additional advances are needed before the technology is ready for introduction into such applications.  Nevertheless, the results of the presented studies show promise}\DIFaddend .

%DIF <  \section{Theoretical Implications}
%DIF <  
%DIF <  If Chapter \ref{chapter3} demonstrated that the human auditory system is robust in its ability to accurately identify speech in a degraded signal, Chapter \ref{chapter4} has shown that automatic speech recognition systems remain fragile.  It is not unreasonable to assume that the human listeners in the speech perception experiment had never heard speech recorded from the ear, yet were still able to understand the signal with a relatively high degree of accuracy and adapt to the task at hand.  ASR, on the other hand, remains relatively `rigid' in its domain-specific ability to recognize speech unlesstrained on the specific task.
%DIF > Currently, this benefit is primarily limited to low SNR speech.
%DIF >  If Chapter \ref{chapter3} demonstrated that the human auditory system is robust in its ability to accurately identify speech in a degraded signal, Chapter \ref{chapter4} has shown that automatic speech recognition systems remain rigid and are largely unable to account for variations in the expected signal without compensation or retraining.  It is likely that the human listeners in the speech perception experiment had never heard speech recorded from the ear, yet were still able to understand the signal with a relatively high degree of accuracy and adapt to the task at hand.  ASR, on the other hand, remains relatively `rigid' in its domain-specific ability to recognize speech unlesstrained on the specific task.
% 
% The robustness and flexibility of human speech perception could be due in part to the redundancy of cues it uses to represent various speech features (\cite{winter:14}).  If one feature or set of features is rendered unitelligible (eg. voice onset time (VOT) for voicing contrast of stop sounds), others cues can be used (eg. duration of previous vowel) to still achieve recognition.  Furthermore, humans are able to `turn-off' reliance on features that are unavailable or unreliable, eg. those above 2.7 kHz for ear-recorded speech.  Many proposed ASR system modifications have incorporated human auditory functions (e.g. \cite{kim:99,fazel:12,moritz:15}), and continuing to model aspects of the human auditory system in ASR processing is likely to bring ASR performance closer to the recognition accuracy of human listeners, regardless of the speech domain.  The information necessary for recognition exists in the ear-recorded speech signals, as the human auditory system has shown its ability to parse it with a relatively high degree of accuracy (cf. Chapter \ref{chapter2}).  With domain specific training, ASR recognition performance on ear-recorded speech will likely improve.
% To truly achieve human-level parity, however, ASR will need to exhibit the the same flexibility and robustness of the auditory system.


\section{Conclusions}\label{chap5:conclusions}

The passage of speech through the head and into the ear canal has been shown to have a consistent and predictable low-pass filtering effect on the signal.  Nevertheless, when recorded from inside the canal it \DIFdelbegin \DIFdel{is seen to contain }\DIFdelend \DIFaddbegin \DIFadd{contains }\DIFaddend meaningful speech information, such as the lower formants.  When coupled with an earplug and 30 NRR earmuffs, recording in this location also significantly reduces any background noise present in the surrounding environment.  This ear-recorded speech is able to offer significant intelligibility benefits to humans over speech recorded from the mouth, but with noise in the background.  \DIFdelbegin \DIFdel{Whether or not there are potential }\DIFdelend \DIFaddbegin \DIFadd{The extent of }\DIFaddend benefits offered by speech recorded at the ear to ASR systems is not yet certain, though there appears to be \DIFdelbegin \DIFdel{some recognition improvement over speech in very noisy backgrounds}\DIFdelend \DIFaddbegin \DIFadd{promise in training acoustic models specifically for ear-recorded speech}\DIFaddend . While not \DIFdelbegin \DIFdel{yet suitable for real-life }\DIFdelend \DIFaddbegin \DIFadd{currently suitable for practical }\DIFaddend applications, much is yet to be explored regarding the possibilities and potential of an ear-recorded speech signal as a method of noise-robust human-to-human and human-to-computer communication.





