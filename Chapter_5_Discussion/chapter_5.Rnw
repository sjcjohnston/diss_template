% %\Sexpr{set_parent(â€˜/Users/mwilli/Documents/Spring_2017/Dissertation_Document/Dissertation_Working_Directory_Draft/Dissertation_Main.Rnw')}
% 
% <<chunk_options, echo=FALSE>>=
% # This is where we set basic knitr options.
% opts_chunk$set(echo=TRUE, message=FALSE, warning = FALSE, cache = FALSE, comment="")
% options(width=75) # This sets how wide the R printout can be.
% @
% 
%  <<setup-child, include=FALSE>>=
%  set_parent('/home/sam/Dissertation/Dissertation_Template/Dissertation_Main_Template/Dissertation_Main.Rnw')
% @
% 
% <<load_libraries, echo=FALSE>>=
% library(tidyr)
% library(dplyr)
% library(ggplot2)
% library(lme4)
% library(lsmeans)
% library(car)
% library(pbkrtest)
% library(xtable)
% library(cowplot)
% library(plyr)
% @

\chapter{Overall Discussion\label{chapter5}}


The primary goal of this project was to record a small corpus of ear-recorded speech, and test the ability of humans and ASR systems to accurately recognize the speech content.  More specifically, the project aimed to determine a) if using an earplug and ear muffs to record speech was able to significantly filter out background noise, b) if ear-recorded speech contained enough information to be intelligible, c) if human listeners found ear-recorded speech more intelligible than the exact same signal recorded at the mouth in a noisy environment, and d) if an ASR system is more effective at recognizing speech content from ear-recorded speech than speech recorded at the mouth in a noisy environment. The following three sections briefly review the task and results of the three experiments discussed in this report pertaining to the questions listed above. Specifically, the collection of ear-recorded speech (cf. Section \ref{sec:chap2-review}), the human intelligibility ear-recorded speech (cf. Section \ref{sec:chap3-review}), and c) the ASR recognition ability of ear-recorded speech (cf. Section \ref{sec:chap4-review}).  Limitations of the conducted studies and directions for future research will be discussed in Section \ref{chap5:future-research}, and the over-arching findings of this report will be briefly summarized in Section \ref{chap5:conclusions}.


\section{Summary of Ear-Recorded Speech}\label{sec:chap2-review}

Ear-recorded speech was collected and analyzed from 20 participants.  The microphone was placed into a silicone earplug and into the participants' right ear (facing the tympanic membrane), and noise reduction earmuffs were placed over top of their ears.  A rod extending out from the side of the earmuffs was used to place a microphone recording the speech as it came from their mouth.  A loudspeaker was placed off to the side, which emitted background noise.  The participants read aloud stimulus sentences, and were recorded, while various kinds of background noises (bus, caf\'{e}, pedestrian area, street, factory) were produced by the loudspeaker.  For more details concerning the experimental methods, refer to Chapter \ref{chapter2}, Section \ref{expt1}.

These recordings were then analyzed.  Overall, the ear-recorded speech was highly low-pass filtered, and, subjectively, it had a very `muffled' quality.  To test the similarity between the mouth-recorded signals and their ear-recorded counterparts, the maximum value of the cross-correlation matrix was obtained and normalized (ie. to obtain values between 0 and 1, where 1 indicates identical signals), and this value was averaged over all clean-speech utterances by all speakers.  Only utterances with no background noise were included in this calculation, in order to only test the similarity of the speech and not the noise.  This test yielded a maximum normalized cross-correlation value of 0.358 as an average across all included utterances.

To increase the amplitude of the speech information in the higher frequencies, the author pre-emphasized the signal.  As there was no speech information found above approximately 2.7 kHz, and since the pre-emphasis resulted in the presence of white noise in the higher frequencies, the signal was then low-pass filtered at 2.5 kHz with a slope of 500 Hz.  The lower frequencies in this signal were still much more prominent than the upper frequencies, and so pre-emphasis was applied a second time.  The average maximum normalized cross-correlation metric was obtained using the newly transformed ear-recorded speech, and each signal was compared to its simultaneously mouth-recorded counterpart.  As before, only clean speech, with no background noise, was included in this comparison.  This gave a nearly doubled cross-correlation value of 0.674.

% The spectra and spectrograms for the sentence ``He said the same phrase thirty times'', spoken by a male participant, are provided for better visualization of speech intelligibility and noise reduction.  A comparison between speech recorded at the mouth and at the ear, with transformations, and with no background noise can be seen in Figures \ref{x} and \ref{y}); the same utterance is displayed, spoken at the mouth with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{} and \ref{}) and at the ear (after transformations) with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{}, and \ref{}).  Each mouth- and ear-recorded pair is the exact same utterance; they were recorded simultaneously (eg. the already mentioned Figures \ref{x} at the ear and \ref{y} at the mouth).

%The figures containing noisy speech recorded at the mouth (ie. Figs. \ref{}, \ref{}, and \ref{}) demonstrate 
One of the limitations of the collected data was that the SNR of the signal recorded by the mouth microphone was very high for all noise categories.  The noise did not drown out the speech in the high noise (80 dB) condition as originally intended.  The SNR needed to be high to compare the recognition performance of humans and computers on the ear-recorded speech, compared with the noisy mouth speech.  If the noise wasn't sufficiently high enough to interfere with the recognition of speech, then the mouth-recorded speech would be too easy to understand, and an adequate comparison between ear-recorded speech and noisy mouth-recorded speech could not be made.

Despite the high SNRs in the mouth-recorded signals, noise is still present.  Considering the ear-recorded signals that were also recorded in the noisy environment %(ie. Figs. \ref{}, \ref{}, and \ref{}), 
the passive noise reduction that is provided by the head, earplug, and earmuffs appears to have successfully eliminated a substantial proportion of the noise from the signal.

These recordings demonstrate that using the ear canal as a location of recording speech in noise can offer a large reduction in the noise that enters the signal.  Furthermore, ear-recorded signals can provide speech comparable to mouth-recorded speech up to approximately 2.7 kHz.  This is relatively close to the 3.5 kHz cutoff generally used for telephonic communication, which does provide intelligible speech for both humans and computers. The author hypothesized that speech recorded from the ear and cut off at 2.7 kHz would also be intelligible for humans and for computers.  The following sections will review the results of the experiments in Chapters \ref{chapter3} and \ref{chapter4}, which aimed to test this hypothesis.


\section{Summary of Human Perception of Ear-Recorded Speech}\label{sec:chap3-review}

Due to the high SNR of the speech recorded in the data collection experiment, described in Chapter \ref{chapter2} and Section \ref{sec:chap2-review} above, two additional speakers were recorded in the same manner, ie. with a microphone placed in an earplug and in their ear canal, with earmuffs placed over their ears.  Another directional microphone was placed in front of their mouth, except instead of pointing at their mouth, the microphone pointed at the loudspeaker in the room, which produced the background noise. The ear-recorded speech was pre-emphasized and low-pass filtered in the same manner as the recordings conducted in Chapter \ref{chapter2}. For further details about the collection of these additional recordings, refer to Chapter \ref{chapter3}, Section \ref{expt2}.

The speech recorded from these two speakers was used as stimuli for the human perception experiment.  There were 24 participants, all native speakers of English, who listened to the provided stimuli.  There were 24 conditions with 3 factors, gender of the speaker of the stimulus (male,female), recording location of the stimulus (in front of mouth, in ear canal) and noise type (clean, bus, caf\'{e}, pedestrian area, street, factory).  The stimuli were placed in counter-balanced groups, where each sentence stimulus occurred in each of the 24 conditions.  Each of the 24 participants received one of the 24 counter-balanced stimulus groups, meaning that each distinct sentence-condition pairing was only heard by one participant listener.
%Each participant listener heard each stimulus utterance once, and heard three utterances per condition.  This yielded 72 stimuli (72 distinct utterances), with each of the 24 conditions repeated three times.  Each listener could only hear a given utterance once before they would be given an advantage to understanding it.  To avoid this, counter-balanced lists were developed to place each stimulus in each of the 24 conditions, requiring

The participant listeners heard an utterance, and typed what they heard into a response box.  They were instructed that what was heard might not be a complete sentence, and to only type what they thought they heard.  These responses were recorded for each participant, and WER was calculated between the actual utterance transcription and the proposed transcription provided by the participant listeners.

Two, 3-way ANOVAs was conducted on the data, with dependent variable WER.  Factors for the first included `speaker [of the stimulus]'s gender' (female, male), `recording [mic] location' (mouth, ear), and `noise type' (clean, bus, caf\'{e}, pedestrian, street, factory).  The second ANOVA removed the level `clean' (no noise) from the factor of noise, because it differed substantially from the other levels in that factor.  Both ANOVA found significant interactions of speaker gender \textbf{x} recording `mic' location and noise type \textbf{x} microphone location.  Factors were split to test simple effects.  Of note, there was a statistical difference for all simple effects of microphone location, indicating that listeners more easily recognize ear-recorded speech than [sufficiently] noisy mouth-recorded speech.  Additionally, when the `clean' level was removed from the ANOVA, and simple effects of noise were tested, there was a statistical difference of noise for mouth-recorded speech, but no statistical difference for ear-recorded speech.  This implies that the noise level was sufficiently dampened in the ear-recorded speech that the various noise types had no effect.  Considering the ear-recorded speech at every level of speaker gender and noise, the WER hovered mainly between 20-30\%.

Two additional investigative studies were performed; the first added high-pass filtered mouth recorded speech to the low-pass filtered ear-recorded stimuli, and the second added a brief training session - listening to the narration of a story recorded from the ear canal - before proceeding to the task.  There were not enough participants in either of these studies to run statistics, so all results are speculative.  These were compared to the primary study (above) to determine if there might be any improvement in the recognition of ear-recorded speech by utilizing these methods.  No substantial improvement was observed from the participants who were exposed to a pre-task training session.  Very minor improvement was observed for the high/low frequency combination study; there were a few conditions that did see a stark improvement, including the `clean' noise condition.

In summary, the use of ear-recorded speech as a noise-robust method of communication seems promising, as it is more easily recognizable than speech spoken from the mouth in noise.  However, even a WER of 20\%, ie. the ability to correctly understand only 4 out of every 5 words, is not sufficient for most situations requiring human communication.  The two methods employed do not seem to offer much improvement, though these - and other possibilities for improving recognition - should be fully tested in future research.



\section{Summary of Automatic Speech Recognition of Ear Recorded Speech}\label{sec:chap4-review}

The author used the Kaldi ASR system (\cite{povey:11}) and the acoustic and language models trained on the data from the LibriSpeech corpus (\cite{panayotov:15}) to test the ability of a standard ASR system to recognize both ear-recorded speech and noisy mouth-recorded speech.  For testing data, the ear- and mouth-recorded speech that was obtained in the data collection experiment - presented in Chapter \ref{chapter2} and reviewed in Section \ref{sec:chap2-review} - was used.  The ear-recorded speech was transformed with pre-emphasis, low-pass filtering, and a second application of pre-emphasis (described in Chapter \ref{chapter2} and in Section \ref{sec:chap2-review}). For more details about these methods, refer to Chapter \ref{chapter4}, Section \ref{expt3}.

The performance of the ASR system on mouth-recorded speech and ear-recorded speech demonstrated a tremendous divergence, with mouth-recorded speech substantially out-performing ear-recorded speech.  As the noise level increased, and consequently as the SNR of the mouth-recorded speech dropped, this divergence shrunk.  But even at the highest noise level (80 dB), the ear-recorded speech barely out-performed the mouth recorded speech in only noise categories.  Furthermore, these WERs are prior to any application of noise-removal techniques beyond standard, built-in procedures that are used during Kaldi feature extraction.  If additional noise-removal techniques were to be applied, such as Advanced Front End (AFE), there would be an even greater gap between ASR performance on ear- and mouth-recorded speech.

Since the acoustic model - trained on 960 hours of mouth-recorded speech - was searching for speech information in the higher frequencies that simply did not exist in the low-frequency ear-recorded signal, these results were not unanticipated.  Another test was performed. This used slightly modified stimuli, created by adding the high-frequency portion of the mouth-recorded speech to the low-frequency ear-recorded speech in an effort to reintroduce the upper frequency information that the ASR system may be looking for.  This same technique was also used in Chapter \ref{chapter3} for human speech perception.  For these tests, the original acoustic model trained on 960 hours of speech was used.  Performance of this `hybrid' data improved performance only moderately compared with the regular ear-recorded data.  This hints that the ear-recorded speech itself might be subtly different from the mouth-recorded speech in the same frequency range, and that for substantial improvements to be seen, an acoustic model needs to be trained with ear-recorded speech.


Another model was trained on a 100-hour subset of the LibriSpeech corpus of mouth-recorded speech; this mouth-recorded speech from the LibriSpeech corpus was low-passed to the same frequency range as ear-recorded speech.  When compared with an acoustic model trained on the same (but non-low-pass filtered) 100-hour portion of the LibriSpeech corpus, ear-recorded speech notes another slight gain (using the model trained on low-pass filtered data).  It is noteworthy, however, that low-noise, low-pass filtered mouth-recorded speech - using the low-pass trained acoustic model - performs comparably well to full-bandwidth mouth-recorded speech - using the full-bandwidth trained model.  This indicates that there is another difference between ear-recorded and mouth-recorded speech besides the bandwidth, and furthermore, that enough critical speech is located in the available frequency range for adequate recognition.  If enough ear-recorded speech could be obtained to train an acoustic model, it may be possible to see similar performance to that obtained by the low-pass filtered mouth-recorded speech in this study.

% Performance of this `hybrid' speech data increased by as much as 14\% WER in the 80 dB noise condition, and in the 60 dB noise condition, by as much as 30\% WER.  All noise types in the 80 dB noise conditions - except one (`factory') - had an improvement in WER from the mouth-recorded noisy speech to the hybrid mouth- and ear-recorded combination.  Nevertheless, for the much higher-SNR 70 dB and 60 dB noise conditions, performance worsened when using the `hybrid' speech over the `plain' noisy mouth-recorded speech.  

% From these results, it appears that ear-recorded speech only offers a benefit over speech recorded at the mouth in very noisy conditions.  The hybrid speech offers a slight improvement, but the capability for further benefit is unknown.  Despite the poor performance of the model trained on low-pass filtered mouth recorded speech, the greatest improvement to recognition will likely come from retraining or adapting an acoustic model specifically using ear-recorded speech data.


\section{Future Directions}\label{chap5:future-research}

One of the primary limitations is that the SNR for the speech recorded in the original data collection experiment (cf. Chapter \ref{chapter2}) was much higher than originally intended or hoped for, by slightly more than +20 dB SNR for each condition.  This yielded noisy speech which was largely greater than +10 dB SNR; current ASR technology is rather adept at recognizing signals with these levels of noise (cf. \cite{braun:16}).  Due to this, it was difficult to see how much improvement could be gained over very noisy, low-SNR signals.  Several possibilities stand out as potentially workable solutions.  The first, would be to simply increase the ambient background noise level, and the second involves using an omnidirectional microphone to record the speech from the mouth (rather than a directional microphone).  A third, and most realistic solution, would be to record in real environments (eg. factory, airport, etc.) where this approach to noise-robust communication may applied.

The first introduces potential risk to those exposed to the noise, depending on the level of noise.  However, sufficient ear protection would largely mitigate this problem.  Additionally, for very high noise conditions, it would be difficult to limit the noise exposure to the surrounding area.  The second solution allows the speaker to talk normally and for the ambient noise level to remain relatively low.  However, this does not necessarily result in a fair comparison between ear-recorded and mouth-recorded speech, as in any real-life application, a directional microphone would be utilized with the specific intention of reducing the noise picked up by the microphone.  %If adequate hearing protection is offered, and the noise contained, the former solution offers the most realistic comparison between the two microphone locations.
The third approach - recording in a real-life noisy environment - offers the most realistic comparison, with realistic noise types and noise levels.  This solution, however, offers much less control over different noises, and may be more difficult to arrange the availability of both participants and an off-site recording location.  However, for testing the realistic application of this device, the third option is by far the best.

Regarding human speech perception, ear-recorded speech seems promising.  There is a significant difference between the ability of human listeners to recognize ear-recorded speech and mouth-recorded speech in noise.  Ear-recorded speech does not provide the same recognition performance as speech recorded at the mouth without background noise.  However, two methods were proposed to further improve the performance of ear-recorded speech.  The first introduced a training session prior to the actual experiment in which participants listened to the narration of a short story, recorded from the ear canal.  The second spliced the low-frequency ear-recorded speech with the higher frequencies of the mouth-recorded speech into a `hybrid' speech stimulus.  These were run as pilot experiments to demonstrate directions for future research; there were not enough participating subjects to run statistics (due to requiring counter-balanced groups).  

These pilot, preliminary results for the former (training) method showed little or no improvement over the standard ear-recorded speech, while the pilot, preliminary results for the latter (hybrid) method provided indication that this could potentially yield improvement.  Future research should test these methods statistically.  The former (training) method may need altered methodology in order to determine whether a form of training could result in an improvement in future recognition of ear-recorded speech.  A training task that requires interaction and provides feedback may promote active attention during the training task, unlike passively listening to a story as used in this study.  In the method using spliced stimuli, further research should pursue the possibility of using the clean, lower frequencies to actively clean any noise from the higher mouth-recorded frequencies.  

Regarding future directions for ASR research, there appears to be promise in training a model specifically with ear-recorded speech. A DNN model was trained on 100 hours of speech - a subset of the 960-hour corpus - which was low-pass filtered to mimic the low-frequency ear-recorded speech was tested on low-pass filtered mouth recorded speech.  The limited-bandwidth mouth-recorded speech performed within 10\% WER of the full-bandwidth mouth-recorded speech.  This indicates that acoustic models are capable of recognizing speech with only the limited-bandwidth in which ear-recorded speech falls, and that if ear-recorded speech were used as the training data, significant ASR improvement could be observed.

Further gains might also be obtained by actively using the clean lower-frequency ear-recorded speech to actively clean the speech information in the mouth recorded signal.   
Additional research should also pursue other methods of improving the human or computer recognition of ear-recorded speech. 


\section{Overall Implications}

Chapter \ref{chapter2} demonstrated that speech recorded at the ear can be robust to environmental noise.  If Chapter \ref{chapter3} demonstrated that the human auditory system is robust in its ability to accurately identify speech in a degraded signal, Chapter \ref{chapter4} has shown that automatic speech recognition systems remain rigid and are largely unable to account for variations in the expected signal without compensation or retraining.  It is likely that the human listeners in the speech perception experiment had never heard speech recorded from the ear, yet were still able to understand the signal with a relatively high degree of accuracy and adapt to the task at hand.  ASR, on the other hand, remains relatively `rigid' in its domain-specific ability to recognize speech unless trained on the specific task.

The robustness and flexibility of human speech perception could be due in part to the redundancy of cues it uses to represent various speech features (\cite{winter:14}).  If one feature or set of features is rendered unintelligible (eg. voice onset time (VOT) for voicing contrast of stop sounds), others cues are available for use (eg. duration of previous vowel) in order to achieve recognition.  Many proposed ASR system modifications have incorporated human auditory functions (e.g. \cite{kim:99,fazel:12,moritz:15}), and continuing to model aspects of the human auditory system in ASR processing is likely to bring ASR performance closer to the recognition accuracy of human listeners, regardless of the speech domain.  The information necessary for recognition exists in the ear-recorded speech signals, as the human auditory system has shown its ability to parse it with a relatively high degree of accuracy (cf. Chapter \ref{chapter2}).  With domain specific training, ASR recognition performance on ear-recorded speech will likely improve.

Nevertheless, both chapters \ref{chapter3} and \ref{chapter4} demonstrated that - in some noisy environments - ear-recorded speech is more intelligible to both humans and computers.  Critically, the processing performed on these signals is minimal, which makes it potentially suitable for on-line ASR systems and for real-time communication.  It requires no computationally intensive processing, and no assumption about external noise, which can take a variety of forms and intensities.  While some noise was seen to leak into the in-ear microphone signal, the SNR was very high compared with its mouth-recorded counterpart.  If excessive levels of noise began to pollute the signal, any existing noise-removal techniques could be applied.  The author hypothesizes that any these noise-reduction methods applied to ear-recorded speech would meet greater success than an applied to the noisy, mouth-recorded counterpart, since the ear-recorded speech provides a higher `base' SNR.  This has potential implications for the ability to convey speech information in very noisy situations, where normal, mouth-recorded speech would be beyond recovery with the use of current noise reduction techniques.

Ear-recorded speech has potential applications for electronic communication in noisy environments.  Particularly those environments in which users are more likely to already be wearing hearing protection (eg. factory floors, airport tarmacs, mechanic shops, etc.) have greater potential to benefit from noise-robust human-to-human communication devices.  Additionally, as the workplace becomes more AI centered, these same devices could be used to communicate vocally to computer interfaces.  In-ear microphones would also be easier for the user to correctly situate.  For example, hyper-cardioid microphones may be useful in noisy environments, but only insofar as they are pointed toward the speaker's mouth.  An in-ear microphone, by nature that it is inserted into the speakers ear, ensures that it is placed in the correct location.  Furthermore, if ear-recorded speech is able to obtain ASR accuracy levels similar to that of clean mouth-recorded speech, an in-ear microphone device could be a useful hands-free method to communicate with AI assistants in the home or office without the possibility of becoming `out of range' of existing far-field microphone arrays.

Additional advances are needed before the technology is ready for introduction into such applications.  Nevertheless, the results of the presented studies show promise.

%Currently, this benefit is primarily limited to low SNR speech.
% If Chapter \ref{chapter3} demonstrated that the human auditory system is robust in its ability to accurately identify speech in a degraded signal, Chapter \ref{chapter4} has shown that automatic speech recognition systems remain rigid and are largely unable to account for variations in the expected signal without compensation or retraining.  It is likely that the human listeners in the speech perception experiment had never heard speech recorded from the ear, yet were still able to understand the signal with a relatively high degree of accuracy and adapt to the task at hand.  ASR, on the other hand, remains relatively `rigid' in its domain-specific ability to recognize speech unlesstrained on the specific task.
% 
% The robustness and flexibility of human speech perception could be due in part to the redundancy of cues it uses to represent various speech features (\cite{winter:14}).  If one feature or set of features is rendered unitelligible (eg. voice onset time (VOT) for voicing contrast of stop sounds), others cues can be used (eg. duration of previous vowel) to still achieve recognition.  Furthermore, humans are able to `turn-off' reliance on features that are unavailable or unreliable, eg. those above 2.7 kHz for ear-recorded speech.  Many proposed ASR system modifications have incorporated human auditory functions (e.g. \cite{kim:99,fazel:12,moritz:15}), and continuing to model aspects of the human auditory system in ASR processing is likely to bring ASR performance closer to the recognition accuracy of human listeners, regardless of the speech domain.  The information necessary for recognition exists in the ear-recorded speech signals, as the human auditory system has shown its ability to parse it with a relatively high degree of accuracy (cf. Chapter \ref{chapter2}).  With domain specific training, ASR recognition performance on ear-recorded speech will likely improve.
% To truly achieve human-level parity, however, ASR will need to exhibit the the same flexibility and robustness of the auditory system.


\section{Conclusions}\label{chap5:conclusions}

The passage of speech through the head and into the ear canal has been shown to have a consistent and predictable low-pass filtering effect on the signal.  Nevertheless, when recorded from inside the canal it contains meaningful speech information, such as the lower formants.  When coupled with an earplug and 30 NRR earmuffs, recording in this location also significantly reduces any background noise present in the surrounding environment.  This ear-recorded speech is able to offer significant intelligibility benefits to humans over speech recorded from the mouth, but with noise in the background.  The extent of benefits offered by speech recorded at the ear to ASR systems is not yet certain, though there appears to be promise in training acoustic models specifically for ear-recorded speech. While not yet suitable for real-life applications, much is yet to be explored regarding the possibilities and potential of an ear-recorded speech signal as a method of noise-robust human-to-human and human-to-computer communication.





