% %set_parent(â€˜/Users/mwilli/Documents/Spring_2017/Dissertation_Document/Dissertation_Working_Directory_Draft/Dissertation_Main.Rnw')
% 
% <<chunk_options, echo=FALSE>>=
% # This is where we set basic knitr options.
% opts_chunk$set(echo=TRUE, message=FALSE, warning = FALSE, cache = FALSE, comment="")
% options(width=75) # This sets how wide the R printout can be.
% @
% 
%  <<setup-child, include=FALSE>>=
%  set_parent('/home/sam/Dissertation/Dissertation_Template/Dissertation_Main_Template/Dissertation_Main.Rnw')
% @
% 
% <<load_libraries, echo=FALSE>>=
% library(tidyr)
% library(dplyr)
% library(ggplot2)
% library(lme4)
% library(lsmeans)
% library(car)
% library(pbkrtest)
% library(xtable)
% library(cowplot)
% library(plyr)
% @

\chapter{Introduction\label{chapter1}}

Speech in a noisy background presents a challenge \DIFdelbegin \DIFdel{to the }\DIFdelend \DIFaddbegin \DIFadd{for }\DIFaddend recognition of that speech both by human listeners and by computers tasked with \DIFdelbegin \DIFdel{recognizing }\DIFdelend \DIFaddbegin \DIFadd{understanding }\DIFaddend human speech (\DIFdelbegin \DIFdel{termed }\DIFdelend automatic speech recognition\DIFaddbegin \DIFadd{; ASR}\DIFaddend ).  Years of research have resulted in \DIFdelbegin \DIFdel{many solutions, though none so far have }\DIFdelend \DIFaddbegin \DIFadd{a variety of solutions, that have not }\DIFaddend completely solved the problem.  Most approaches have taken the route of trying to remove noise from a signal that is already corrupted with noise.

Figure \ref{fig:signal-SNR-intro} demonstrates \DIFdelbegin \DIFdel{the }\DIFdelend \DIFaddbegin \DIFadd{this }\DIFaddend problem.  The dark horizontal bands seen in the spectrogram in each \DIFdelbegin \DIFdel{figure }\DIFdelend \DIFaddbegin \DIFadd{subfigure }\DIFaddend are part of the speech signal.  In Figure \ref{fig:signal-SNR-intro-low}, these are clearly visible above the background noise.  By contrast, in Figure \ref{fig:signal-SNR-intro-high}, these dark horizontal bands are more difficult to see.  The amplitude (level of darkness in the spectrogram) of the noise at various frequencies drowns out the speech signal, making it more difficult to hear.  In the noisy signal, there is more competing information that makes it difficult for both humans - and computers - to differentiate what is noise and what is speech.  This will be discussed in more depth in Section \ref{sec:snr-difficult}.

\DIFdelbegin %DIFDELCMD < \begin{figure}[h!]
%DIFDELCMD < \centering
%DIFDELCMD < \begin{subfigure}{0.75\textwidth}
%DIFDELCMD <   \includegraphics[width=.9\textwidth]{figure/signal-SNR-intro-low.png}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{A sentence spoken with a high level of background noise, resulting in a }\textit{\DIFdel{low}} %DIFAUXCMD
\DIFdel{SNR.}}
  %DIFAUXCMD
%DIFDELCMD < \label{fig:signal-SNR-intro-low}
%DIFDELCMD < \end{subfigure}
%DIFDELCMD < %%%
%DIF < 
%DIFDELCMD < \begin{subfigure}{0.75\textwidth}
%DIFDELCMD <   \includegraphics[width=.9\textwidth]{figure/signal-SNR-intro-high.png}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{A sentence spoken with a low level of background noise, resulting in a }\textit{\DIFdel{high}} %DIFAUXCMD
\DIFdel{SNR.}}
  %DIFAUXCMD
%DIFDELCMD < \label{fig:signal-SNR-intro-high}
%DIFDELCMD < \end{subfigure}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Waveforms and spectrograms of the sentence ``The glow deepened in the eyes of the sweet girl.''}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:signal-SNR-intro}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend This project presents a new approach to the problem.  It proposes that human speech can be recorded in a manner that largely eliminates the noise before it reaches the microphone recording the signal.  The microphone would be placed inside the ear canal of the speaker and would record speech as it is passed through the bone and tissue of the speaker's head.  The signal is expected to be distorted by passage through the head, but the speech \DIFdelbegin \DIFdel{in the signal }\DIFdelend is expected to be recoverable.  \DIFaddbegin \DIFadd{This has potential wide-ranging applications, including human-to-human electronic communication in loud workplaces (eg. factory floor, airport tarmac), as well as human-to-computer interactions, as the use of artificial intelligence becomes more widespread and mainstream; this includes its use in such noisy workplace environments.
}

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth}
\centering
  \includegraphics[width=.8\textwidth]{figure/signal-SNR-intro-low.png}
  \caption{\DIFaddFL{A sentence spoken with a low level of background noise, resulting in a }\textit{\DIFaddFL{high}} \DIFaddFL{SNR.}}
  \label{fig:signal-SNR-intro-high}
\end{subfigure}
%DIF > 
\begin{subfigure}{\textwidth}
\centering
  \includegraphics[width=.8\textwidth]{figure/signal-SNR-intro-high.png}
  \caption{\DIFaddFL{A sentence spoken with a high level of background noise, resulting in a }\textit{\DIFaddFL{low}} \DIFaddFL{SNR.}}
  \label{fig:signal-SNR-intro-low}
\end{subfigure}
\caption{\DIFaddFL{Waveforms and spectrograms of the sentence ``The glow deepened in the eyes of the sweet girl.''}}
\label{fig:signal-SNR-intro}
\end{figure}
\DIFaddend 

Section \ref{ch1:background} below describes the basic acoustics of sound.  This is intended to be a general background to acoustics and interfering sound waves, leading up to a brief overview of sound in noise. Chapter \ref{chapter2} discusses the collection of ear-recorded speech, Chapter \ref{chapter3} describes a human speech perception experiment using ear-recorded speech, and Chapter \ref{chapter4} outlines an experiment testing the ability of an automatic speech recognition system to accurately recognize ear-recorded speech.  At the end of this chapter, Section \ref{ch1:diss-overview} gives a more detailed overview of the rest of the dissertation.

\section{Background}\label{ch1:background}

Sound itself, along with the ability to perceive it, is a remarkable phenomenon.  Put simply, `sound' is the fluctuation of pressure in neighboring groups of particles over time. Most frequently sound is discussed in terms of the fluctuation of air pressure, because, as humans, we primarily receive sound into our ear canals through the medium of air, but sound can also travel through \DIFdelbegin \DIFdel{media of }\DIFdelend liquids and solids, or pass through any combination of the three.

There are primarily three components to sound: amplitude, frequency, and phase \DIFaddbegin \DIFadd{(\mbox{%DIFAUXCMD
\cite{rosen:91}
}%DIFAUXCMD
)}\DIFaddend , which can be seen in Figure \ref{fig:basic-sound-wave}.  In a simple sound wave, the amplitude of sound corresponds to the peak intensity of the high pressure (and the lowest pressure) portions of a sound wave (cf. Fig. \ref{fig:basic-sound-amplitude}).  The frequency corresponds to the rate at which the high and low pressure portions of the signal fluctuate between one another (cf. Fig. \ref{fig:basic-sound-frequency}).  The phase of a wave is the location of the pressure level (relative to atmospheric pressure) \DIFaddbegin \DIFadd{at a given moment }\DIFaddend in time (cf. Fig. \ref{fig:basic-sound-phase}).

\DIFdelbegin %DIFDELCMD < \begin{figure}[h!]
%DIFDELCMD < \begin{subfigure}{0.5\textwidth}
%DIFDELCMD <   \includegraphics[width=\textwidth]{figure/basic-sound-amplitude.png}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Two waveforms showing a difference in amplitude between the two signals.}}
  %DIFAUXCMD
%DIFDELCMD < \label{fig:basic-sound-amplitude}
%DIFDELCMD < \end{subfigure}
%DIFDELCMD < \qquad
%DIFDELCMD < \begin{subfigure}{0.5\textwidth}
%DIFDELCMD <   \includegraphics[width=\textwidth]{figure/basic-sound-frequency.png}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Two waveforms showing a difference in frequency between the two signals.}}
  %DIFAUXCMD
%DIFDELCMD < \label{fig:basic-sound-frequency}
%DIFDELCMD < \end{subfigure}
%DIFDELCMD < %%%
%DIF < 
%DIFDELCMD < \\[2ex]
%DIFDELCMD < \begin{center}
%DIFDELCMD < \begin{subfigure}{0.5\textwidth}
%DIFDELCMD <   \includegraphics[width=\textwidth]{figure/basic-sound-phase.png}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Two waveforms showing a difference in phase between the two signals.}}
  %DIFAUXCMD
%DIFDELCMD < \label{fig:basic-sound-phase}
%DIFDELCMD < \end{subfigure}
%DIFDELCMD < \end{center}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Waveforms demonstrating difference in amplitude, frequency, and phase.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:basic-sound-wave}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend This latter characteristic - phase, while important when dealing with interacting waves from multiple sources, is often not taken into \DIFaddbegin \DIFadd{account }\DIFaddend in speech science due to both its complexity and the fact that phase does not encode \DIFdelbegin \DIFdel{any }\DIFdelend \DIFaddbegin \DIFadd{critical }\DIFaddend speech information.  The human auditory system primarily makes use of the other two characteristics of sound - amplitude and frequency.

\DIFaddbegin \begin{figure}[H]
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/basic-sound-amplitude.png}
  \caption{\DIFaddFL{Two waveforms showing a difference in amplitude between the two signals.}}
  \label{fig:basic-sound-amplitude}
\end{subfigure}
\qquad
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/basic-sound-frequency.png}
  \caption{\DIFaddFL{Two waveforms showing a difference in frequency between the two signals.}}
  \label{fig:basic-sound-frequency}
\end{subfigure}
%DIF > 
\\[2ex]
\begin{center}
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/basic-sound-phase.png}
  \caption{\DIFaddFL{Two waveforms showing a difference in phase between the two signals.}}
  \label{fig:basic-sound-phase}
\end{subfigure}
\end{center}
\caption{\DIFaddFL{Waveforms demonstrating difference in amplitude, frequency, and phase.}}
\label{fig:basic-sound-wave}
\end{figure}


\DIFaddend \subsection{Overview of Anatomy and Physiology of the Peripheral Auditory System}

Prior to discussing the acoustic structure of speech, it is important to become familiar with the basic peripheral auditory system.  The peripheral auditory system is generally grouped into three primary categories, the outer ear, the middle ear, and the inner ear (cf. Figure \ref{fig:ear-anatomy}).  The outer ear includes the pinna, the ear canal\DIFdelbegin \DIFdel{tube}\DIFdelend , and the tympanic membrane (ie. the eardrum).  Air-transmitted sound vibrations, ie. pressure fluctuations, enter the ear canal through the opening at the pinna.  These then travel along the canal to vibrate the tympanic membrane, which passes the energy to the middle ear.  The middle ear includes the ossicles within the middle ear cavity.  The ossicles \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{form }\DIFaddend a chain of three very small bones leading from the tympanic membrane to the cochlea. The external sound vibrations \DIFdelbegin \DIFdel{hit }\DIFdelend \DIFaddbegin \DIFadd{impose an oscillating force on }\DIFaddend the tympanic membrane, \DIFdelbegin \DIFdel{are passed along the ossicle }\DIFdelend \DIFaddbegin \DIFadd{setting it into motion. This results in displacement at the ossicular }\DIFaddend chain, which then \DIFdelbegin \DIFdel{sends these vibrations to }\DIFdelend \DIFaddbegin \DIFadd{transforms the vibrations into a traveling wave passing along the basilar membrane within the fluid-filled cochlea of }\DIFaddend the inner ear (\cite{rosen:91}).

\DIFdelbegin %DIFDELCMD < \begin{figure}[h]
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \begin{figure}[H]
\DIFaddendFL \centering
  \DIFdelbeginFL %DIFDELCMD < \includegraphics{figure/ear_anatomy.png}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=.75\textwidth]{figure/ear-diagram.png}
  \DIFaddendFL \caption{A diagram of the peripheral auditory system, including the outer ear, middle ear, and inner ear\DIFdelbeginFL \DIFdelFL{, up to the auditory nerve}\DIFdelendFL .\DIFdelbeginFL \DIFdelFL{(Image from \mbox{%DIFAUXCMD
\cite{martin:12}
}%DIFAUXCMD
)}\DIFdelendFL }
  \label{fig:ear-anatomy}
\end{figure}

The inner ear is composed of the cochlea, \DIFaddbegin \DIFadd{and }\DIFaddend the semicircular canals \DIFdelbegin \DIFdel{(and vestibule), and the }\DIFdelend \DIFaddbegin \DIFadd{and }\DIFaddend auditory and vestibular nerves \DIFaddbegin \DIFadd{(the latter three are not pictured)}\DIFaddend .  The semicircular canals, \DIFdelbegin \DIFdel{vestibule, }\DIFdelend and vestibular nerve don't play a part in audition (their primary function regards balance sensitivity).  The cochlea, a hard `shell' filled with fluid\DIFaddbegin \DIFadd{, }\DIFaddend receives the vibrations passed \DIFdelbegin \DIFdel{along through }\DIFdelend \DIFaddbegin \DIFadd{to it by way of }\DIFaddend the middle ear ossicles.\DIFdelbegin \footnote{\DIFdel{The peripheral auditory system uses gas, solid, and liquid media to transmit acoustic vibrations.}}  %DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdelend %DIF > \footnote{The peripheral auditory system uses gas, solid, and liquid media to transmit acoustic vibrations.}  
The vibrations are passed into the fluid \DIFaddbegin \DIFadd{of the }\DIFaddend cochlea and travel along \DIFdelbegin \DIFdel{the lengthof the cochlea, transmitting acoustic }\DIFdelend \DIFaddbegin \DIFadd{its length, transmitting hydro-mechanical }\DIFaddend energy to cells \DIFaddbegin \DIFadd{on the basilar membrane }\DIFaddend that are able to detect the \DIFdelbegin \DIFdel{amplitude of the vibrations}\DIFdelend \DIFaddbegin \DIFadd{displacement of the fluid by means of sensory membranes.  The basilar membrane serves as the base on which these sensory cells are fixed}\DIFaddend .  Depending on the location of \DIFdelbegin \DIFdel{a cell along the length of the cochlea, it will pick up a different frequency of sound}\DIFdelend \DIFaddbegin \DIFadd{the sensory cells along the basilar membrane, they respond to a different frequency of `sound', ie. fluid pressure fluctuation.  This placement of sensory cells `tuned' to detect a particular frequency band is called tonotopic organization.  This essentially allows the cochlea to perform a Fourier Transform - which extracts all the different frequency components from a wave - of the sound that is transmitted to it}\DIFaddend .  The auditory nerve carries electrical impulses from these \DIFaddbegin \DIFadd{sensory }\DIFaddend cells into the auditory cortex of the brain \DIFaddbegin \DIFadd{(\mbox{%DIFAUXCMD
\cite{rosen:91,celesia:15}
}%DIFAUXCMD
)}\DIFaddend . 

Of interest to this present study is the \DIFdelbegin \DIFdel{relatively simple }\DIFdelend outer ear.  Typically, as described above, vibrations from the air will enter the ear canal through the opening at the pinna. Frequently, these vibrations take the form of human speech.

%However, vibrations from one's own speech are also transmitted via the bone, cartilage, and tissue of the head.  Regardless of source, sound vibrations entering into the ear canal will be altered by the shape of the ear canal, described more below.


\subsection{Acoustic Structure of Human Speech}

The structure of human speech draws from a collection of individual sounds (phonemes) that are strung together, encoding words, sentences, and more abstractly, meaning.  Each human language uses a subset of all possible phonemes.  Each phoneme is either considered `voiced' or `unvoiced'.  The acoustic properties of sounds in these two categories differ greatly \DIFaddbegin \DIFadd{(\mbox{%DIFAUXCMD
\cite{ladefoged:14}
}%DIFAUXCMD
)}\DIFaddend .


%DIF > 
\DIFaddbegin \begin{wrapfigure}{l}{0.5\textwidth}
\centering
  \includegraphics[width=0.5\textwidth]{figure/spctrm5k.png}
  \caption{\DIFadd{Spectrum of the middle of an /I/ vowel.  Each `hump' is a separate narrow band of frequency, called a harmonic.}}
  \label{fig:spctrm5k}
\end{wrapfigure}
%DIF > 
\DIFaddend Voiced speech is composed of narrow bands of acoustic energy, called harmonics, located along a frequency spectrum (cf. \DIFdelbegin \DIFdel{fig }\DIFdelend \DIFaddbegin \DIFadd{Fig }\DIFaddend \ref{fig:spctrm5k}).  In this sense, speech is considered a `complex' sound, because it is composed of \DIFdelbegin \DIFdel{multiple, simultaneous frequencies.  %DIF < 
}%DIFDELCMD < \begin{wrapfigure}{l}{0.5\textwidth}
%DIFDELCMD < \centering
%DIFDELCMD <   \includegraphics[width=0.5\textwidth]{figure/spctrm5k.png}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Spectrum of the middle of an /I/ vowel.  Each `spike' is a separate narrow band of frequency, called a harmonic.}}
  %DIFAUXCMD
%DIFDELCMD < \label{fig:spctrm5k}
%DIFDELCMD < \end{wrapfigure}
%DIFDELCMD < %%%
%DIF < 
\DIFdelend \DIFaddbegin \DIFadd{energy at multiple frequencies.  }\DIFaddend Certain harmonics will be dampened by the vocal tract, leaving others relatively unfiltered.  A group of neighboring harmonics containing more energy than other harmonics are called formants.  The location, shape, and transition over time of these formants (among other more minor features) \DIFdelbegin \DIFdel{are what }\DIFdelend encodes speech information for voiced sounds.  This can be \DIFdelbegin \DIFdel{easily }\DIFdelend visualized in a spectrogram (cf. Fig. \ref{fig:spctgrm_citizen}).


\DIFdelbegin %DIFDELCMD < \begin{figure}[h!]
%DIFDELCMD < \begin{subfigure}{0.95\textwidth}
%DIFDELCMD <   %%%
\DIFdelend \DIFaddbegin \begin{figure}[H]
\begin{subfigure}{\textwidth}
  \DIFaddendFL \centering
  \DIFdelbeginFL %DIFDELCMD < \includegraphics[width=0.95\textwidth]{figure/spctgrm1k.png}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=0.85\textwidth]{figure/spctgrm1k.png}
  \DIFaddendFL \caption{Zoomed to the 0-1kHz range for better visualization of harmonics.}
  \label{fig:spctgrm_citizen_1k}
\end{subfigure}%
\hfill
\begin{subfigure}{0.95\textwidth}
  \centering
  \DIFdelbeginFL %DIFDELCMD < \includegraphics[width=0.95\textwidth]{figure/spctgrm5k.png}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=0.85\textwidth]{figure/spctgrm5k.png}
  \DIFaddendFL \caption{Zoomed to a more standard 0-5kHz range.}
  \label{fig:spctgrm_citizen_5k}
\end{subfigure}
\caption{Narrow-band spectrogram of the word `citizens' with phonetic transcription above.  Frequency is on the y-axis, time is on the x-axis, and amplitude is shown in grayscale on the graph; the darker an area of the graph, the greater the amplitude.}
\label{fig:spctgrm_citizen}
\end{figure}

For unvoiced speech, the information used to recognize and categorize the speech sound is likely found in \DIFdelbegin \DIFdel{either }\DIFdelend the turbulent frication generally centered in higher frequencies (cf. Fig. \ref{fig:spctgrm_s}\DIFaddbegin \DIFadd{)}\DIFaddend , although some of the information can be found in lower frequencies \DIFdelbegin \DIFdel{), }\DIFdelend or found in \DIFdelbegin \DIFdel{the voiced informationin the }\DIFdelend \DIFaddbegin \DIFadd{surrounding voiced information, such as formant }\DIFaddend transitions into and out of the sound (\cite{halle:57,lindblom:63,stevens:78,willi:17}).  Generally, most speech information used by the human auditory system can be found in frequencies below 5.0-8.0 kHz.
%DIF < 
\DIFaddbegin 

\DIFaddend \begin{wrapfigure}{l}{0.5\textwidth}
\centering
  \includegraphics[width=0.5\textwidth]{figure/spctgrm_s.png}
  \caption{Spectrum of the initial /s/ in `citizens'. Zoomed to range of \DIFdelbegin \DIFdel{0-8kHz }\DIFdelend \DIFaddbegin \DIFadd{0-8 kHz }\DIFaddend for visualization of high frequency energy.}
  \label{fig:spctgrm_s}
\end{wrapfigure}
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend %DIF > 
The human auditory \DIFdelbegin \DIFdel{system }\DIFdelend \DIFaddbegin \DIFadd{cortex }\DIFaddend has the remarkable ability to (a) \DIFdelbegin \DIFdel{identify }\DIFdelend \DIFaddbegin \DIFadd{transduce }\DIFaddend these sounds, which often only last from tens to a few hundreds of milliseconds in duration, (b) partition the stream of sounds into their respective words, and (c) string the words together into a sentence and pull meaning from it - all in real time \DIFaddbegin \DIFadd{(\mbox{%DIFAUXCMD
\cite{celesia:15}
}%DIFAUXCMD
)}\DIFaddend .  Nevertheless there are occasionally recognition errors, which can occur anywhere along this auditory chain.  \DIFdelbegin \DIFdel{The `lowest' level in this chain that errors occur is the recognition and identification of sounds.  }\DIFdelend %DIF > The `lowest' level in this chain in which errors occur is the recognition and identification of sounds in the auditory cortex.  
There are a host of reasons why \DIFdelbegin \DIFdel{this }\DIFdelend \DIFaddbegin \DIFadd{these errors }\DIFaddend might occur, yet \DIFdelbegin \DIFdel{this focuses }\DIFdelend \DIFaddbegin \DIFadd{the studies presented throughout this dissertation focus }\DIFaddend on additive noise interfering with the speech signal \DIFdelbegin \DIFdel{and }\DIFdelend \DIFaddbegin \DIFadd{or }\DIFaddend signal distortion via passage of speech through a speaker's head.

\DIFdelbegin %DIFDELCMD < \subsection{Acoustics of Multiple Signals}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \subsection{Acoustics of Complex Signals}
\DIFaddend 

\DIFdelbegin \DIFdel{As previously mentioned, we as humans }\DIFdelend \DIFaddbegin \DIFadd{Humans }\DIFaddend use the amplitude and frequency of pressure fluctuations to perceive sound \DIFaddbegin \DIFadd{(\mbox{%DIFAUXCMD
\cite{rosen:91}
}%DIFAUXCMD
)}\DIFaddend . When sound travels through a medium from \DIFdelbegin \DIFdel{source }\DIFdelend \DIFaddbegin \DIFadd{Source }\DIFaddend \textit{A}, there is nothing that prevents these pressure fluctuations of \DIFdelbegin \DIFdel{source }\DIFdelend \DIFaddbegin \DIFadd{Source }\DIFaddend \textit{A} from acoustically mixing with the pressure fluctuations originating from \DIFdelbegin \DIFdel{source }\DIFdelend \DIFaddbegin \DIFadd{Source }\DIFaddend \textit{B}.  For example, in Figure \ref{fig:sound-wave-addition}\DIFdelbegin \DIFdel{source }\DIFdelend \DIFaddbegin \DIFadd{, Source }\DIFaddend \textit{A} produces a simple wave with a frequency of 100 Hz.  Source \textit{B} produces a simple wave with a frequency of 200 Hz.  If the waves from the two sources reach each other and overlap, a single wave \DIFdelbegin \DIFdel{results that looks like that }\DIFdelend \DIFaddbegin \DIFadd{is produced which would look similar to the one }\DIFaddend in Figure \ref{fig:sound-wave-addition-combined}.  This sound wave now has two components - a tone at 100 Hz and a tone at 200 Hz.

\DIFdelbegin %DIFDELCMD < \begin{figure}[h!]
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \begin{figure}[H]
\DIFaddendFL \begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/basic-sound-wave.png}
  \caption{A basic sound wave from source \textit{A} at a frequency of 100 Hz.}
  \label{fig:sound-wave-A}
\end{subfigure}
\qquad
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/sound-wave-addition-200hz.png}
  \caption{A basic sound wave from source \textit{B} at a frequency of 200 Hz, with half the amplitude of the simple sound wave from source \textit{A}.}
  \label{fig:sound-wave-B}
\end{subfigure}
%
\\[2ex]
\begin{center}
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/sound-wave-addition-combined.png}
  \caption{The resulting complex wave from the combination of the wave from source \textit{A} and source \textit{B}.}
  \label{fig:sound-wave-addition-combined}
\end{subfigure}
\end{center}
\caption{Demonstration of the combination of two waves of different frequency and amplitude.}
\label{fig:sound-wave-addition}
\end{figure}

As previously mentioned, phase does not play a significant role in human audition, \DIFdelbegin \DIFdel{but can affect a wave resulting }\DIFdelend \DIFaddbegin \DIFadd{yet is a factor in the formation of a wave }\DIFaddend from the addition of multiple sound sources.  \DIFdelbegin \DIFdel{Say that source }\DIFdelend \DIFaddbegin \DIFadd{If Source }\DIFaddend \textit{B} \DIFdelbegin \DIFdel{produces instead a wave }\DIFdelend \DIFaddbegin \DIFadd{(cf. Figure \ref{fig:wave-out-of-phase}) produces, instead, a wave of }\DIFaddend the same frequency and amplitude as the wave from \DIFdelbegin \DIFdel{source }\DIFdelend \DIFaddbegin \DIFadd{Source }\DIFaddend \textit{A}, but they are completely `out of phase', ie. the pressure value of the waves at any given time is in direct opposition\DIFdelbegin \DIFdel{(cf. Figure \ref{fig:wave-out-of-phase})}\DIFdelend .  If these waves \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{were }\DIFaddend combined, it \DIFdelbegin \DIFdel{results }\DIFdelend \DIFaddbegin \DIFadd{would result }\DIFaddend in a complete elimination of sound (cf. Fig. \ref{fig:wave-out-of-phase-added}).

\DIFdelbegin %DIFDELCMD < \begin{figure}[h!]
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \begin{figure}[H]
\DIFaddendFL \begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/wave-out-of-phase.png}
  \caption{Two sound waves with frequency of 100 Hz and the same amplitude, completely out of phase.}
  \label{fig:wave-out-of-phase}
\end{subfigure}
\qquad
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/wave-out-of-phase-added.png}
  \caption{The sound wave resulting from the combination of the two out of phase waves in Fig. \ref{fig:wave-out-of-phase}.}
  \label{fig:wave-out-of-phase-added}
\end{subfigure}
\caption{Demonstration of the combination of two completely out-of-phase waves with the same amplitude and frequency.}
\end{figure}

It is not very often that two waves of the exact same frequency and amplitude, with exactly opposing phase, meet in such a way to completely negate.  However, varying degrees of negation \DIFdelbegin \DIFdel{occur frequently }\DIFdelend \DIFaddbegin \DIFadd{and alteration occur constantly }\DIFaddend due to phase \DIFaddbegin \DIFadd{and interacting sound waves}\DIFaddend .  For example, if the 100 Hz wave from Figure \ref{fig:sound-wave-addition} were combined with a 200Hz wave with a slightly shifted phase, a different wave would be produced, seen in Figure \ref{fig:sound-combined-shifted-phase}.  

\DIFdelbegin %DIFDELCMD < \begin{figure}[h!]
%DIFDELCMD < \begin{subfigure}{0.5\textwidth}
%DIFDELCMD <   \includegraphics[width=\textwidth]{figure/wave-out-of-phase.png}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Two sound waves with frequency of 100 Hz and the same amplitude, completely out of phase.}}
  %DIFAUXCMD
%DIFDELCMD < \label{fig:wave-out-of-phase2}
%DIFDELCMD < \end{subfigure}
%DIFDELCMD < \qquad
%DIFDELCMD < \begin{subfigure}{0.5\textwidth}
%DIFDELCMD <   \includegraphics[width=\textwidth]{figure/sound-wave-addition-200hz-shifted.png}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{The sound wave resulting from the combination of the two out of phase waves in Fig. \ref{fig:wave-out-of-phase}.}}
  %DIFAUXCMD
%DIFDELCMD < \label{fig:wave-addition-200hz-shifted}
%DIFDELCMD < \end{subfigure}
%DIFDELCMD < %%%
%DIF < 
%DIFDELCMD < \begin{center}
%DIFDELCMD < \begin{subfigure}{0.5\textwidth}
%DIFDELCMD <   \includegraphics[width=\textwidth]{figure/sound-combined-shifted-phase.png}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{The sound wave resulting from the combination of the two out of phase waves in Fig. \ref{fig:wave-out-of-phase}.}}
  %DIFAUXCMD
%DIFDELCMD < \label{fig:sound-combined-shifted-phase}
%DIFDELCMD < \end{subfigure}
%DIFDELCMD < \end{center}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Demonstration of the combination of the same two waves as in Fig. \ref{fig:sound-wave-addition}, where the phase of the 200 Hz wave in Fig. \ref{fig:sound-wave-B} was shifted slightly (cf. Fig. \ref{fig:wave-addition-200hz-shifted})}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:sound-shifted-phase}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend The combination of waves from multiple sound sources increases greatly in complexity as the number of sources increases, and the \DIFaddbegin \DIFadd{as }\DIFaddend sounds originating from \DIFdelbegin \DIFdel{the sources are }\DIFdelend \DIFaddbegin \DIFadd{a single source is itself }\DIFaddend complex (ie. containing multiple frequency elements), such as speech.  Speech rarely occurs in isolation from from all external\DIFaddbegin \DIFadd{, interacting }\DIFaddend sound, yet we are still \DIFaddbegin \DIFadd{able }\DIFaddend to largely understand speech in everyday environments; for example\DIFaddbegin \DIFadd{, }\DIFaddend it is generally easy for humans to understand the speech of an interlocutor while sitting on a bench at a park.

The auditory system is \DIFdelbegin \DIFdel{actually quite skilled at identifying }\DIFdelend \DIFaddbegin \DIFadd{well tuned to identify }\DIFaddend separate sources, even complex ones, like speech. Despite the shifted phase in Figure \ref{fig:sound-shifted-phase}, the human auditory system would still be able to detect and identify two separate waves.  While it undoubtedly plays a part, the differences in phase of combined signals does not normally completely negate a signal, nor render it unintelligible.  It is for this reason, and the complexity of phase calculations, that most efforts to remove \DIFdelbegin \DIFdel{speech from noise }\DIFdelend \DIFaddbegin \DIFadd{noise from speech }\DIFaddend ignore the phase component.

\DIFaddbegin \begin{figure}[H]
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/wave-out-of-phase.png}
  \caption{\DIFaddFL{Two sound waves with frequency of 100 Hz and the same amplitude, completely out of phase.}}
  \label{fig:wave-out-of-phase2}
\end{subfigure}
\qquad
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/sound-wave-addition-200hz-shifted.png}
  \caption{\DIFaddFL{The sound wave resulting from the combination of the two out of phase waves in Fig. \ref{fig:wave-out-of-phase}.}}
  \label{fig:wave-addition-200hz-shifted}
\end{subfigure}
%DIF > 
\begin{center}
\begin{subfigure}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/sound-combined-shifted-phase.png}
  \caption{\DIFaddFL{The sound wave resulting from the combination of the two out of phase waves in Fig. \ref{fig:wave-out-of-phase}.}}
  \label{fig:sound-combined-shifted-phase}
\end{subfigure}
\end{center}
\caption{\DIFaddFL{Demonstration of the combination of the waves in Fig. \ref{fig:sound-wave-addition}, where the phase of the 200 Hz wave in Fig. \ref{fig:sound-wave-B} was shifted slightly (cf. Fig. \ref{fig:wave-addition-200hz-shifted})}}
\label{fig:sound-shifted-phase}
\end{figure}



\DIFaddend \subsection{Difficulties of Speech in Noise}\label{sec:snr-difficult}

Nevertheless, there are still situations in which it is difficult to parse speech in noise.  This is most often due to signals with energy at similar frequencies that overlap.  The greater the amplitude of a signal at frequencies overlapping with those of speech, the more difficult the speech will be to understand.  This can be visualized in the spectrograms of the sentence ``A rich farm is rare in this sandy waste.'' in Figure \ref{fig:signal-SNR-intro}.  As demonstrated before in Figure \ref{fig:signal-SNR-intro-high}, the amplitude, \DIFdelbegin \DIFdel{or loudness, }\DIFdelend of the noise is well below that of speech, which would likely be easily understand by a listener.  Figure \ref{fig:signal-SNR-intro-low} \DIFdelbegin \DIFdel{has }\DIFdelend \DIFaddbegin \DIFadd{depicts }\DIFaddend a much greater noise amplitude level compared to the amplitude level of speech and consequently would likely be more difficult for a listener to understand.

This relationship between speech and any background noise is called the signal to noise \DIFdelbegin \DIFdel{ration }\DIFdelend \DIFaddbegin \DIFadd{ratio }\DIFaddend (SNR).  A complex signal with a higher signal to noise ratio (cf. Fig \ref{fig:signal-SNR-intro-high}) is generally easier to understand, because the amplitude of the speech (the `signal' of interest) is much greater than that of the noise.  Consequently a lower SNR (cf. Fig. \ref{fig:signal-SNR-intro-low}) results in speech that is more difficult to understand, because the amplitude of the speech is close to - or below - the amplitude of the background noise.  \DIFaddbegin \DIFadd{Noise occurring a similar frequency region to the desired speech signal is said to `mask' the speech; this is described in more detail in Chapter \ref{chapter3} (\mbox{%DIFAUXCMD
\cite{mattys:12}
}%DIFAUXCMD
).
}\DIFaddend 

This poses a problem for human listeners \DIFaddbegin \DIFadd{in very low SNR environments}\DIFaddend , but generally is more difficult to deal with \DIFdelbegin \DIFdel{in }\DIFdelend \DIFaddbegin \DIFadd{for }\DIFaddend automatic speech recognition (ASR) \DIFaddbegin \DIFadd{systems}\DIFaddend , since the \DIFdelbegin \DIFdel{electronic device }\DIFdelend \DIFaddbegin \DIFadd{computational system }\DIFaddend does not contain a highly-skilled, built-in auditory cortex.  There are a number of ways which have been proposed to deal with noise in a speech signal, both for human and automatic speech recognition.  These will be discussed further in Chapters \ref{chapter3} and \ref{chapter4}.

\section{Overview of Dissertation}\label{ch1:diss-overview}

This report aims to explore a novel method of human speech perception and automatic speech recognition (ASR) in noisy environments.  The method proposes that speech be recorded from the inside of the ear canal of the speaker, slightly transformed, and sent to the human listener or the computer receiver for recognition.  Collecting speech from the ear allows for usage of the human skull and adjacent tissues to passively filter out the noisy environment, leaving only - or mostly - the human speech carrying the intended message.  

The intention of this study is to determine a) if recording human speech from the inside of the ear canal can significantly reduce background noise in a signal, b) if intelligible speech, suitable for communication, can be collected from the inside of the ear canal, c) if humans find speech recorded from the ear canal more intelligible than speech in noisy conditions, and finally d) if ASR systems are able to recognize speech recorded in the ear canal with greater accuracy than speech recorded in noise.

Since \DIFdelbegin \DIFdel{currently there is }\DIFdelend \DIFaddbegin \DIFadd{at the time, there was }\DIFaddend no established corpus of data that \DIFdelbegin \DIFdel{contain }\DIFdelend \DIFaddbegin \DIFadd{contains }\DIFaddend speech recorded from the inside of the ear canal together with speech simultaneously recorded from the mouth in noisy environments, it was necessary to record \DIFaddbegin \DIFadd{sounds from }\DIFaddend speakers in this environment and create a new corpus.  The theory behind the acoustics of recording speech from the ear canal, as well as the process for developing this corpus are described in Chapter \ref{chapter2}, along with a discussion of the recorded speech.  Chapter \ref{chapter3} will outline a human perception experiment, tasking listeners with the transcription of various sentences of speech recorded \DIFaddbegin \DIFadd{both }\DIFaddend at the mouth in noise\DIFdelbegin \DIFdel{in noise, and recorded from the ear }\DIFdelend \DIFaddbegin \DIFadd{, and from a sealed ear canal}\DIFaddend .  Chapter \ref{chapter4} describes the use of this same speech with an ASR system, and its recognition performance.  Chapter \ref{chapter5} will summarize the previous chapters and engage in an overall discussion of the implications of the results, the limitations of the present experiments and methods, and suggestions for future research direction.


