%\Sexpr{set_parent(â€˜/Users/mwilli/Documents/Spring_2017/Dissertation_Document/Dissertation_Working_Directory_Draft/Dissertation_Main.Rnw')}

<<chunk_options, echo=FALSE>>=
# This is where we set basic knitr options.
opts_chunk$set(echo=TRUE, message=FALSE, warning = FALSE, cache = FALSE, comment="")
options(width=75) # This sets how wide the R printout can be.
@

 <<setup-child, include=FALSE>>=
 set_parent('/home/sam/Dissertation/Dissertation_Template/Dissertation_Main_Template/Dissertation_Main.Rnw')
@

<<load_libraries, echo=FALSE>>=
library(tidyr)
library(dplyr)
library(ggplot2)
library(lme4)
library(lsmeans)
library(car)
library(pbkrtest)
library(xtable)
library(cowplot)
library(plyr)
@

\chapter{Human Speech Perception of Ear-Recorded speech\label{chapter4}}


\section{Introduction}

In order to judge the usefulness and intelligibility of the modified sounds recorded at the ear, it is necessary to run a human perception task on the recorded sounds.  Of primary interest is whether the speech recorded at the ear in a noisy condition a) has resulted in a sufficient reduction in the ambient noise level, and b) is markedly more intelligible that speech recorded at the \textit{mouth} in a noisy condition.

\section{Background}

I need to demonstrate:
  - Explaining the acoustic structure of speech in noise
  - Demonstrating why speech in noise is difficult for humans to understand (mechanically? neurologically?)
  - Show the variability in ability to understand speech in noise by different speakers.
  
\subsection{Acoustics of Speech in Noise}


\subsection{Difficulty of Speech Perception in Noise}


\section{Experiment 2: Human Speech Perception in Noise}



\subsection{Stimuli}
\label{chap4:methods:stimuli}

After analysing the results from a few pilot runs, it was deemed that the speech collected at the mouth in the noisy background described in Chapter \ref{chapter2} was too easily recognizeable for humna listeners; i.e. the SNR ratio was not low enough, as explained in Section \ref{ch2:limitations}.  To remedy this problem, two additional participants (one male, one female) were recorded following the procedure in the first task (cf. Chapter \ref{chapter2}).  The list of stimuli was increased to 80 sentences (eight Harvard Sentence lists\footnote{This included the previous three lists, 14, 28, and 57, as well as lists 21, 29, 37, 53, and 68. These additional sentences were pseudo-randomly chosen, as were the original three sentences, to contain words that would be readily recognizeable by the participant population.}) to provide more reaction data from this present experiment.  

To increase the SNR, the directional microphone was pointed away from the mouth of the participant, and directed toward the loudspeaker (see fig. \ref{blah}).  This, of course, results in some of the limitations outlined in section \ref{blah} in Chapter 2; for example, simply pointing the mouth microphone toward the loudspeaker, rather than increasing the noise, ignores the fact that the noise level inside the ear might increase as well.  Given the alternatives outlined in, this was seen as the best available option.  Figures \ref{fig:spectNewMouthNoise} and \ref{fig:spectNewEarNoise} show the new noisy and ear recorded speech, respectively.

Furthermore, due to the issue of the lack of noise, only one noise level condition was used - the highest available noise level (80 dB).  All previously used background noises were used for this data collection task as well.


\subsection{Design}
\label{chap4:methods:design}

The experiment had three factors - gender of speaker by microphone location by noise type - resulting in a 2x2x6 experiment.  There were two genders, two mic locations (recording at the ear, and at the mouth), and six noise types (bus, cafe, pedestrian, street, factory, and no noise (clean)).  Since the ability to understand speech in noise is quite variable between individuals, the design of this experiment was a within-subjects experiment.  This meant that each of the 2x2x6 (ie. 24) conditions needed to be seen by each participant.  The sessions that were re-recorded constituted 80 sentences, which allowed for three sentences in each of the 24 conditions, totalling 72 sentences used in the experiment.  The eight remaining sentences were used as a ``training'' set, intended to get the participants used to the task itself, rather than to acclimate them the type of speech that they would hear.

Since any given speaker could not hear the same sentence twice without introducing a confound, and since each sentence occurred in each of the 24 conditions, this necessitated the use of 24 co-balanced lists to ensure that each sentence was heard in every condition.  For example, sentences 1, 2, and 3 would occur in Factor #1 (e.g. female speaker, mic at the mouth, with bus background noise) in List #1 for Participant #1. Participant #2 would see List #2, which placed sentences 1, 2, and 3 in Factor #2 (e.g. female speaker, mic at the mouth, with cafe background noise).  For simplicity's sake, each grouping of three sentences would appear together in a given condition, and were not mixed up between the different co-balanced lists.  However, once the sentences are assigned to a particular condition, the order of presentation to the participants is randomized.

\subsection{Participants}

Twenty-four native speakers of English with normal hearing participated in the experiment. Each participant was placed into a separate co-balanced group, as specified in \ref{chap4:methods:design}.

\subsecion{Equipment}

The experiment was conducted in a soundbooth with a pair of over-the-ear headphones.  The experiment used in-house developed software, and participants answers were typed into a computer whose monitor could be seen from inside the soundbooth.

\subsection{Procedure}

The participant was seated in the sound booth in front of a keyboard and computer monitor, with a pair of headphones, and were given a set of instructions. They were told that they would hear each utterance only once, and what they would hear was comprised of real English words, but may not constitute a ``complete'' sentence.  They were forewarned that many of the sentences they heard would be noisy and difficult to understand. They were instructed to write all words they heard, even if what was heard did not make syntactic sense, or if the words were not adjacent (e.g. if only the first and last word of the sentence was heard). They would be timed, with 18 seconds to type their response starting from the beginning of the soundfile and that their answer would be saved as is if they ran into the time limit, preventing them fry typing more.

The participant was told that the first set of eight utterances they heard were part of a ``training'' set of eight utterances.  These were given to introduce the participants to the experiment and their task.  None of the utterances from the training set were used in the analysis.  Once completed with this initial set, participants were asked if they had any questions.  Afterwards they began the task in the soundbooth.  They would hear one of the sentences, and type their answer in a text box.  When finished with their answer, they would either click to advance to the next sentence, or, if they ran into the time limit, were prevented from modifying their answer, and were prompted to click another button to advance.  When finished with all 72 stimuli, the participant was given a brief questionnaire to fill out.  

After the experiment, the researcher would double check participant answers for correct spelling.  Only obvious errors were modified (e.g. `teh' to `the', `crakers' to `crackers', `mantle' to `mantel'), while ambiguous errors were left as-is (e.g. `blo' was not changed to `block', `finde' was not changed to `fine').  Numbers were also lexicalized (e.g. `30' to `thirty').  Punctuation was removed for ease of analysis and calculation of word error rate.  Exact responses are given by each participant can be found in Appendix F\ref{appendixF}.


\section{Results}



\section{Follow-up Pilot Experiments}

\subsection{Methods}


\section{Discussion}

*Some participants noted that the computer screen was a distraction to their ability to perceive the utterances.

<<Exp_1_Data, echo=FALSE, results='asis'>>=

#Visualize Exp.1 Data:
library(ez)
#Read in Exp.1 data:
mydata <- read.csv("/home/sam/Dissertation/combined_results.csv")
## ANOVA

# #ensure column names have the correct name (I think this just renames the columns in the data structure)
colnames(mydata)[1]="subj"
colnames(mydata)[2]="group"
colnames(mydata)[3]="sent_num"
colnames(mydata)[4]="sentence"
colnames(mydata)[5]="spkrGender"
colnames(mydata)[6]="noiseType"
colnames(mydata)[7]="micLoc"
colnames(mydata)[8]="wer"

#attach(mydata)

# This sets each column (that you specify) as a factor.  Failing to do this will result in wonky degrees of freedom
mydata$subj = as.factor(mydata$subj)
attach(mydata)

mydata$group = as.factor(mydata$group)
attach(mydata)

mydata$spkrGender = as.factor(mydata$spkrGender)
attach(mydata)

mydata$noiseType = as.factor(mydata$noiseType)
attach(mydata)

mydata$micLoc = as.factor(mydata$micLoc)
attach(mydata)

## ezANOVA

# This line removes any data in the "noise_type" column that equals zero.  This is not necessary, but helpful for my data.
mydata2 = mydata[which(mydata$noiseType!=0),]

# By subjects ANOVA
by_subj_ez_anova = ezANOVA(
	data = mydata2 	#data structure
	, dv = wer	#dependent variable
	, wid = subj	#column containing case identifier (e.g. subjs/items)
	, within = .(spkrGender,noiseType,micLoc) #(idependent variables/factors)
	)

# By items ANOVA (note that 'wid' has a different factor than by-subjects (i.e., the items column))
by_item_ez_anova = ezANOVA(
	data = mydata2
	, dv = wer
	, wid = sentence
	, within = .(spkrGender,noiseType,micLoc)
	)


# Don't use 'summary(ez_anova)'
# by_subj_ez_anova
# 
# by_item_ez_anova

# Exp1Data$Hypothesis <- revalue(Exp1Data$Hypothesis, c("LE"="Locus Equation", "RFDP"="Relative Formant Deflection Pattern"))
# 
# #Bar Graph with error bar Exp.1:
# ggplot(Exp1Data, aes(x=Hypothesis, y=Percent_Participant_Agreement, fill=Vowel_Context)) + 
#     stat_summary(fun.y = mean, geom = "bar", position="dodge") + 
#     stat_summary(fun.data = mean_se, geom = "errorbar", position="dodge") +ggtitle("Hypothesis Comparison") + labs(x=" Hypothesis ", y=" % Participant Agreement") +ylim(0,100)
# 
# #Stats with Mean and Standard deviation:
# Exp1_Avg <-aggregate(Percent_Participant_Agreement~Hypothesis+Vowel_Context, data=Exp1Data, FUN = function(x) c(mean = mean(x), sd = sd(x), n = length(x)))
# 
# Exp1_mydata <- do.call(data.frame, Exp1_Avg)
# #Exp1_mydata
# 
# #Stats with Mean and Standard deviation:
# library(doBy)
# tab <- summaryBy(Percent_Participant_Agreement ~ Hypothesis + Vowel_Context, data = Exp1Data, FUN = function(x) { c(m = mean(x), s = sd(x)) } )
# 
# library(xtable)
# tab2 <- xtable(tab, caption = "Demographic information")
# print(tab2)


@

