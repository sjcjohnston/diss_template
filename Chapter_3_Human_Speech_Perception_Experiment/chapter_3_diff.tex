% \documentclass[dissertation,copyright]{uathesis}
% %DIF LATEXDIFF DIFFERENCE FILE
% %DIF DEL chapter_3_old.tex   Wed Jul 26 23:38:36 2017
% %DIF ADD chapter_3.tex       Wed Jul 26 23:37:05 2017
% \usepackage[]{graphicx}\usepackage[]{color}
% %% maxwidth is the original width if it is less than linewidth
% %% otherwise use linewidth (to make sure the graphics do not exceed the margin)
% \makeatletter
% \def\maxwidth{ %
%   \ifdim\Gin@nat@width>\linewidth
%     \linewidth
%   \else
%     \Gin@nat@width
%   \fi
% }
% \makeatother
% 
% \definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
% \newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
% \newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
% \newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
% \newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
% \newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
% \newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
% \newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
% \newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
% \newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
% \let\hlipl\hlkwb
% 
% \usepackage{framed}
% \makeatletter
% \newenvironment{kframe}{%
%  \def\at@end@of@kframe{}%
%  \ifinner\ifhmode%
%   \def\at@end@of@kframe{\end{minipage}}%
%   \begin{minipage}{\columnwidth}%
%  \fi\fi%
%  \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
%  \colorbox{shadecolor}{##1}\hskip-\fboxsep
%      % There is no \\@totalrightmargin, so:
%      \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
%  \MakeFramed {\advance\hsize-\width
%    \@totalleftmargin\z@ \linewidth\hsize
%    \@setminipage}}%
%  {\par\unskip\endMakeFramed%
%  \at@end@of@kframe}
% \makeatother
% 
% \definecolor{shadecolor}{rgb}{.97, .97, .97}
% \definecolor{messagecolor}{rgb}{0, 0, 0}
% \definecolor{warningcolor}{rgb}{1, 0, 1}
% \definecolor{errorcolor}{rgb}{1, 0, 0}
% \newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX
% 
% \usepackage{alltt}
% \newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
% \newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
% \newcommand{\Sexpr}[1]{}       % will only be parsed by R
% 
% 
% %\documentclass[dissertation,CC-BY]{uathesis}
% %\documentclass[dissertation,CC-BY-SA]{uathesis}
% %documentclass[dissertation,CC-BY-ND]{uathesis}
% %\documentclass[thesis]{uathesis}
% %\documentclass[document]{uathesis}
% 
% % Package Usage
% % These are the packages that we need
% \usepackage{booktabs}
% \usepackage{graphicx}
% \usepackage{natbib}			% natbib is available on most systems, and is
% 					% terribly handy.
% 					
% %May need to remove! Trying to fix nocite{*} biblography problem:
% 					% If you want to use a different Bibliography package, 
% 					% you should be able to, just change this
% 					% and the \bibliographystyle command below.  Be warned
% 					% that you may need to do a little hacking to get
% 					% the REFERENCES item to show up in your TOC.
% 
% % Compatibility with the AASTEX package 
% % of the American Astronomical Society.
% %\usepackage{deluxetable}		% Allows use of AASTEX deluxe tables
% %\usepackage{aastex_hack}		% Allows other AASTEX functionality.
% 
% % These are other packages that you might find useful.
% % For controlling the fonts, see
% % http://www.math.uiuc.edu/~hartke/computer/latex/survey/survey.html
% % The following is a nice font set:
% %\usepackage{mathtime}			% Times for letters; Belleek math.
% %
% \usepackage{wrapfig}
% %DIF 90-98d90
% %DIF < 
% %DIF < \newenvironment{lydiawrapfigure}
% %DIF <  {%
% %DIF < %  \setlength{\intextsep}{0pt}% <--- Wrong!
% %DIF <   \setlength{\columnsep}{15pt}%
% %DIF <   \wrapfloat{figure}%
% %DIF <  }
% %DIF <  {\endwrapfloat}
% %DIF < 
% %DIF -------
% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage{tipa}
% \usepackage{color,soul}
% \usepackage{url}
% \usepackage{blindtext}
% \usepackage[inline]{enumitem}
% \usepackage{breakurl}
% \usepackage{mathtools}
% %DIF 108c99-126
% %DIF < \usepackage{amsmath}			% AMS Math (advanced math typesetting)
% %DIF -------
% \usepackage{amsmath} %DIF > 
% \usepackage{array} %DIF > 
% \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}} %DIF > 
% \newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}} %DIF > 
%  %DIF > 
% % \usepackage[normalem]{ulem} %DIF > 
% % \usepackage{xyling,comment}			% AMS Math (advanced math typesetting) %DIF > 
% % includecomment{delall} %DIF > 
% % %\excludecomment{delall} %DIF > 
% %  %DIF > 
% % %default: don't show edits %DIF > 
% %  %DIF > 
% % \newcommand{\add}[1]{#1} %DIF > 
% % \newcommand{\del}[1]{} %DIF > 
% % \newcommand{\delni}[1]{} %DIF > 
% % \newcommand{\addtab}{} %DIF > 
% %  %DIF > 
% % %block to show edits controlled by include/exclude-comment above %DIF > 
% % \begin{delall} %DIF > 
% % %added stuff %DIF > 
% % \renewcommand{\add}[1]{\textcolor{blue}{#1}} %DIF > 
% % %added table %DIF > 
% % \renewcommand{\addtab}{\color{blue}} %DIF > 
% % %deleted stuff %DIF > 
% % \renewcommand{\del}[1]{\textcolor{red}{\sout{#1}}} %DIF > 
% % \renewcommand{\delni}[1]{\noindent\textcolor{red}{\sout{#1}}} %DIF > 
% % \end{delall} %DIF > 
%  %DIF > 
% %DIF -------
% %\usepackage{lscape}			% Used for making fitting large tables in by putting them landscape
% %\usepackage{refs}			
% %
% % If you are using hyper-ref (recommended), this command must go after all 
% % other package inclusions (from the hyperref package documentation).
% % The purpose of hyperref is to make the PDF created extensively
% % cross-referenced.
% 
% %Also works! Change dvips to driverfallback=dvips.
% \usepackage[driverfallback=dvips,bookmarks,colorlinks=true,urlcolor=black,linkcolor=black,citecolor=black]{hyperref}
% 
% 
% %Works!
% %\usepackage[pdftex,bookmarks,colorlinks=true,urlcolor=black,linkcolor=black,citecolor=black]{hyperref}
% %HERE IS THE THING THAT NEEDS TO CHANGE TO GET LATEX TO WORK WITH RSTUDIO. USE pdftex instead of dvips.
% 
% % Set up some values.
% \completetitle{Working Title: An approach to automatic and human speech recognition using ear-recorded speech.}
% \fullname{Samuel John Charles Johnston}			% Grad college wants your full name here.
% \degreename{Doctor of Philosophy}	% Title of your degree.
% %DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
% %DIF UNDERLINE PREAMBLE %DIF PREAMBLE
% \RequirePackage[normalem]{ulem} %DIF PREAMBLE
% \RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
% \providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
% \providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
% %DIF SAFE PREAMBLE %DIF PREAMBLE
% \providecommand{\DIFaddbegin}{} %DIF PREAMBLE
% \providecommand{\DIFaddend}{} %DIF PREAMBLE
% \providecommand{\DIFdelbegin}{} %DIF PREAMBLE
% \providecommand{\DIFdelend}{} %DIF PREAMBLE
% %DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
% \providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
% \providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
% \providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
% \providecommand{\DIFaddendFL}{} %DIF PREAMBLE
% \providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
% \providecommand{\DIFdelendFL}{} %DIF PREAMBLE
% %DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF
% %DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
% %DIF HYPERREF PREAMBLE %DIF PREAMBLE
% \providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
% \providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
% %DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF
% 
% \begin{document}
% %set_parent(â€˜/Users/mwilli/Documents/Spring_2017/Dissertation_Document/Dissertation_Working_Directory_Draft/Dissertation_Main.Rnw')


 



\chapter{Human Speech Perception of Ear-Recorded speech\DIFdelbegin %DIFDELCMD < \label{chapter4}%%%
\DIFdelend \DIFaddbegin \label{chapter3}\DIFaddend }


\section{Introduction}\DIFaddbegin \label{chap3:introduction}
\DIFaddend 

In order to judge the usefulness and intelligibility of the modified sounds recorded at the ear, it is necessary to run a human perception task on the recorded sounds.  Of primary interest \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend whether the speech recorded at the ear in a noisy condition (a) \DIFdelbegin \DIFdel{has }\DIFdelend \DIFaddbegin \DIFadd{had }\DIFaddend resulted in a sufficient reduction in the ambient noise level, and (b) \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend markedly more intelligible that speech recorded at the \textit{mouth} in a noisy condition.

\section{Background}\DIFdelbegin %DIFDELCMD < \label{chap2:background}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{chap3:background}
\DIFaddend 

% I need to demonstrate:
%   - Explaining the acoustic structure of speech in noise
%   - Demonstrating why speech in noise is difficult for humans to understand (mechanically? neurologically?)
%   - Explain the average/expected performance of Humans when recognizing speech in noise
%   - Show the variability in ability to understand speech in noise by different speakers.
Human listeners' \DIFdelbegin \DIFdel{loss of the ability to understand human speech occurs }\DIFdelend \DIFaddbegin \DIFadd{inability to understand a speech utterance can occur }\DIFaddend from an information loss that \DIFdelbegin \DIFdel{can be due to }\DIFdelend \DIFaddbegin \DIFadd{might be caused by any of }\DIFaddend a host of factors.  For example, information can be lost in the domain of time (eg. an intermittent signal), \DIFdelbegin \DIFdel{or }\DIFdelend from loss of intensity (eg. due to distance from the source), as well as the \DIFaddbegin \DIFadd{from }\DIFaddend distortion of the source itself, such as a speech impediment (\cite{mattys:12}).  Of particular interest to the present study is the difficulty for human listeners to \DIFdelbegin \DIFdel{perfectly }\DIFdelend understand a speech signal due to \DIFaddbegin \DIFadd{excessive, }\DIFaddend additive background noise from sources other than the desired speech signal.

The ability of the human auditory system to hear and differentiate multiple sources of sound from a single pressure wave is often given the term \DIFdelbegin \DIFdel{``}\DIFdelend \DIFaddbegin \DIFadd{`}\DIFaddend auditory scene analysis\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{' }\DIFaddend (\cite{bregman:94}).  The term \DIFdelbegin \DIFdel{``scene analysis'' is }\DIFdelend \DIFaddbegin \DIFadd{`scene analysis' was }\DIFaddend borrowed from the visual domain, implying the separation of a \DIFdelbegin \DIFdel{``scene'' }\DIFdelend \DIFaddbegin \DIFadd{`scene' }\DIFaddend (be it auditory or visual) into its component objects (again, be they auditory or visual). For the purposes of this study, \DIFdelbegin \DIFdel{we will be discussing }\DIFdelend \DIFaddbegin \DIFadd{this background discusses the }\DIFaddend human auditory ability to find multiple sound sources from a temporal stream of air pressure fluctuations (ie. sound) reaching the tympanic membrane.

To visualize the auditory scene, note the waveform (ie. the graph of air pressure fluctuations) that reaches the tympanic membrane in Figure \ref{fig:animal_singlechannel}.  
%
\begin{wrapfigure}{r!}{0.5\textwidth}
\centering
  \includegraphics[width=0.45\textwidth]{figure/single-channel-animals.png}
  \caption{A waveform composed of multiple sound sources (cf. Fig. \ref{fig:animal_multichannel}).}
  \label{fig:animal_singlechannel}
\end{wrapfigure}
%
It is composed of all environmental sounds contributing to the air pressure fluctuation at the tympanic membrane\footnote{Note that this is for illustration purposes; the waveform will obviously look different when shaped by a given environment and the human ear canal before reaching the tympanic membrane.}.

However, using the framework of auditory scene analysis, the human auditory system is able to separate this input signal into its various sources, or \DIFdelbegin \DIFdel{``auditory objects''.  In effect, this would separate }\DIFdelend \DIFaddbegin \DIFadd{`auditory objects'.  The auditory system separates }\DIFaddend the above waveform into its actual component sources of human speech, and the sounds of a sheep, cow, and horse, seen in Figure \ref{fig:animal_multichannel}; ``The normal auditory system exhibits a remarkable ability to parse these complex scenes'' (\cite{middlebrooks:17}, 2).

Of course, there reaches a point at which the auditory system fails and can
%
\DIFdelbegin %DIFDELCMD < \begin{figure}[h]
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \begin{figure}[h!]
\DIFaddendFL \centering
  \DIFdelbeginFL %DIFDELCMD < \includegraphics[width=0.95\textwidth]{figure/multi-channel-animals_w-text.png}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=0.95\textwidth]{figure/animal_multichannel-w-text.png}
  \DIFaddendFL \caption{The four component waveforms (human speech, sheep, cow, horse), of the combined waveform seen in Figure \ref{fig:animal_singlechannel}.}
  \label{fig:animal_multichannel}
\end{figure}
%
 no longer differentiate all sources\DIFdelbegin \DIFdel{, or, more relevant to this paper, }\DIFdelend \DIFaddbegin \DIFadd{. Pertaining to the present research, there reaches a point at which the auditory system cannot }\DIFaddend recognize the information in a human speech signal when embedded with background noise from one or more additional sources.  The following section \DIFdelbegin \DIFdel{will describe in more depth }\DIFdelend \DIFaddbegin \DIFadd{reviews }\DIFaddend the acoustics of speech in noise \DIFaddbegin \DIFadd{in more depth }\DIFaddend .

\subsection{Acoustics of Speech in Noise}
\label{bkgrnd:speech_in_noise}

Speech in noise can be intuitively grouped into two components, the speech (more specifically the voice one is intending to hear) and the noise, called the \DIFdelbegin \DIFdel{``masking'' }\DIFdelend \DIFaddbegin \DIFadd{`masking' }\DIFaddend element.  Broadly, masking \DIFdelbegin \DIFdel{can be }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend defined as ``the process by which the threshold of hearing for one sound is raised by the presence of another'' (\cite{ansi:13}, 61).  This masking element is \DIFdelbegin \DIFdel{anything }\DIFdelend \DIFaddbegin \DIFadd{everything }\DIFaddend \textit{but} the voice\footnote{For the purposes of this paper, the term \DIFdelbegin \DIFdel{``}\DIFdelend \DIFaddbegin \DIFadd{`}\DIFaddend voice\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{' }\DIFaddend will be used throughout to refer to the singular speech source the listener desires to hear out of the masked signal.} (speech signal) that one is interested in, as it was intended to be heard.
%DIF < which, as indicated in Chapter 2\ref{chapter2}, could be of any form or loudness.
%DIF > which, as indicated in Chapter \ref{chapter2}, could be of any form or loudness.

The masking process can be broken down into two forms: energetic masking and informational masking.  Energetic masking occurs when the masking element shares the same temporal and frequency elements of the voice.  \DIFdelbegin \DIFdel{It can be thought of as if }\DIFdelend \DIFaddbegin \DIFadd{In a sense, }\DIFaddend the masked element and the voice \DIFdelbegin \DIFdel{are competing for ``space'' }\DIFdelend \DIFaddbegin \DIFadd{`compete' for `space' }\DIFaddend along the basilar membrane and then the auditory nerve (\cite{brungart:01}), but \DIFdelbegin \DIFdel{can also be considered to be competing }\DIFdelend \DIFaddbegin \DIFadd{they also compete }\DIFaddend for the listener's attention (ie. the listener must concentrate on ignoring the mask, and exclusively listening to the target, (\cite{mattys:12})).  Energetic masking is normally thought to occur primarily in the \DIFdelbegin \DIFdel{``lower '' }\DIFdelend \DIFaddbegin \DIFadd{`lower }\DIFaddend auditory processes, eg. the cochlea and auditory nerve, though this is not always the case, as described further below.  

Informational masking can be broadly thought of as difficulties relating to memory, linguistic processing, and the like, oftentimes generalized to speech-on-speech noise.  \cite{mattys:10} failed to find informational masking in a cross-linguistic task, and so it is possible that informational masking could be limited to situations in which the masking speech is intelligible. This type of masking is thought to occur primarily in the \DIFdelbegin \DIFdel{``}\DIFdelend \DIFaddbegin \DIFadd{`}\DIFaddend higher auditory processes\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{' }\DIFaddend in the brain.

An instance of both energetic and informational masking can be visualized in a diagram of overlapping speech presented in \cite{middlebrooks:17}, and seen in Figure \ref{fig:sos-masked-spctgrms}.
%
\DIFdelbegin %DIFDELCMD < \begin{figure}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \begin{figure}[h!]
\DIFaddendFL \centering
  \includegraphics[width=0.95\textwidth]{figure/speech-on-speech_masked_spectrograms.png}
  \caption{Diagrams of different spectograms. (A) The spectogram of two temporally overlapping spoken utterances. (B) The spectogram of the utterance ``three five three four three four two'' colored in blue (C) The spectogram of the sentence ``It's hard to understand two things at once.'' colored in red. (D) The overlap of the two spectograms (B) and (C), with the color green highlighting the areas of energy in frequency and time that overlap. }
  \label{fig:sos-masked-spctgrms}
\end{figure}
%
\DIFdelbegin \DIFdel{Say that utterance }\DIFdelend \DIFaddbegin \DIFadd{Utterance }\DIFaddend (C) in the figure is the desired \DIFdelbegin \DIFdel{``voice''}\DIFdelend \DIFaddbegin \DIFadd{`voice'}\DIFaddend , leaving utterance (B) the masking element.  In (D), one can see the voice (red), the areas of masking in which there is direct frequency and temporal overlap (green), and the remainder of the masking speech (blue). Of course this is never so nicely differentiated, and the resulting acoustic information that the auditory system \DIFdelbegin \DIFdel{gets }\DIFdelend \DIFaddbegin \DIFadd{receives }\DIFaddend can be seen in (A), in which no source is differentiated.  This \DIFdelbegin \DIFdel{could primarily be viewed as }\DIFdelend \DIFaddbegin \DIFadd{is primarily }\DIFaddend a form of energetic masking (competition for lower-level processing), though upper level processing is required to take meaning from the desired voice, which is masked informationally by the \DIFdelbegin \DIFdel{other, competing voice carrying }\DIFdelend \DIFaddbegin \DIFadd{competing voice with }\DIFaddend its own information.

The five different background noises used in the study described in Chapter \DIFdelbegin \DIFdel{2\ref{chapter2} primarily serve the purpose of energetic masking of }\DIFdelend \DIFaddbegin \DIFadd{\ref{chapter2} (data collection of speech recorded at the ear) energetically mask }\DIFaddend the voice in the signal.  A small (5 second) portion of the spectrogram of each sound can be seen in Figure \ref{fig:bkgrnd-noises}.  These sounds don \DIFdelbegin \DIFdel{'t }\DIFdelend \DIFaddbegin \DIFadd{not }\DIFaddend produce any competing linguistic informational content themselves which mask the desired voice (the `\DIFdelbegin \DIFdel{cafe}\DIFdelend \DIFaddbegin \DIFadd{caf}\'{e}\DIFaddend ' noise, seen in Figure \ref{fig:cafe-bkgrnd}, does contain speech babble, none of it intelligible), and so masking occurs by producing energy at the same time as - and in the same frequency range as - the recorded voice.

\begin{figure}[h!]
\begin{subfigure}{0.475\linewidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/spctgrm-bus-background.png}
  \caption{Bus background noise.}
  \label{fig:bus-bkgrnd}
\end{subfigure}
\qquad
\begin{subfigure}{0.475\linewidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/spctgrm-cafe-background.png}
  \caption{\DIFdelbeginFL \DIFdelFL{Cafe }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Caf}\'{e} \DIFaddendFL background noise.}
  \label{fig:cafe-bkgrnd}
\end{subfigure}%
%\hfill
\\[2ex]
\begin{subfigure}{0.475\linewidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/spctgrm-ped-background.png}
  \caption{Pedestrian background noise.}
  \label{fig:ped-bkgrnd}
\end{subfigure}
\qquad
\begin{subfigure}{0.475\linewidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/spctgrm-str-background.png}
  \caption{Street background noise.}
  \label{fig:str-bkgrnd}
\end{subfigure}%
%\hfill
\\[2ex]
\begin{center}
\begin{subfigure}{0.475\linewidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/spctgrm-fac-background.png}
  \caption{Factory background noise.}
  \label{fig:fac-bkgrnd}
\end{subfigure}
\end{center}
\caption{Example spectograms of the first five seconds of the background noise tracks. Most recorded sentences occurred within these temporal spans.}
\label{fig:bkgrnd-noises}
\end{figure}

\DIFdelbegin \DIFdel{Yet simply because a sound may be ``masked'' }\DIFdelend \DIFaddbegin \DIFadd{Simply because a voice may be `masked' by noise }\DIFaddend does not necessarily \DIFdelbegin \DIFdel{imply }\DIFdelend \DIFaddbegin \DIFadd{mean }\DIFaddend that the voice is not heard or understood.  \DIFdelbegin \DIFdel{There are }\DIFdelend \DIFaddbegin \DIFadd{The auditory system employs }\DIFaddend a number of methods \DIFdelbegin \DIFdel{used by the auditory system }\DIFdelend to overcome the masking and interpret the voice; this process is termed \DIFdelbegin \DIFdel{``}\DIFdelend \DIFaddbegin \DIFadd{the `}\DIFaddend release from masking\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{' }\DIFaddend (\cite{middlebrooks:17}).  One such \DIFdelbegin \DIFdel{proposed method , }\DIFdelend \DIFaddbegin \DIFadd{theorized method - }\DIFaddend the use of humans' \DIFdelbegin \DIFdel{built in binaural hearing , }\DIFdelend \DIFaddbegin \DIFadd{built-in binaural hearing - }\DIFaddend uses both ears to tease apart the different sources, \DIFdelbegin \DIFdel{utilizing }\DIFdelend \DIFaddbegin \DIFadd{exploiting }\DIFaddend the very small temporal difference that occurs when different sound sources reach each ear.  
In \DIFdelbegin \DIFdel{this example }\DIFdelend \DIFaddbegin \DIFadd{the example of binaural hearing}\DIFaddend , it is easy to see that energetic and informational masking are not strictly limited to \DIFdelbegin \DIFdel{masking separate ``lower'' and ``higher'' processes}\DIFdelend \DIFaddbegin \DIFadd{separate `lower' and `higher' processes, respectively }\DIFaddend (\cite{durlach:06}).
\DIFdelbegin \DIFdel{The use of binaural }\DIFdelend \DIFaddbegin \DIFadd{Binaural }\DIFaddend hearing is an example of \DIFdelbegin \DIFdel{utilizing a ``higher'' process as a release from }\DIFdelend \DIFaddbegin \DIFadd{a `higher' process `releasing' }\DIFaddend energetic masking, as it necessarily requires signals from both ears \DIFdelbegin \DIFdel{to be interpreted }\DIFdelend \DIFaddbegin \DIFadd{(ie. the joining of the signals from both auditory nerves) to be used }\DIFaddend (\cite{hirsh:48}). \DIFdelbegin \DIFdel{Binaural hearing is essentially making }\DIFdelend \DIFaddbegin \DIFadd{It makes }\DIFaddend use of the spatial directionality of the noise(s) from the listener to separate the different sources (\cite{bregman:94}).

There are many other \DIFdelbegin \DIFdel{proposed methods of release from }\DIFdelend \DIFaddbegin \DIFadd{methods theorized to be a part of the auditory system's ability to release }\DIFaddend energetic masking.  One involves making note of acoustic transitions: ``when [a] sound...changes its properties gradually, [it] is likely to be heard as a single changing sound.  However, when [it] changes...abruptly, [it] tends to be treated as a newly arriving sound, this tendency increasing with the abruptness of the change.'' (\cite{bregman:94}, 5).  The use of fundamental frequency (F0) has also been shown to be an effective tool, presumably to interpret the location of harmonics, and parse apart different sources (eg. two separate, simultaneous vowels with different F0s, (\cite{bird:97})). \DIFdelbegin \DIFdel{There are many other proposed methods to release masking that the auditory system uses, particularly among informational masking (\mbox{%DIFAUXCMD
\cite{middlebrooks:17}
}%DIFAUXCMD
) , but these are beyond the scope of this project}\DIFdelend %DIF > Other methods, particularly among informational masking (\cite{middlebrooks:17}), are beyond the scope of this project.
\DIFaddbegin 

\DIFadd{\mbox{%DIFAUXCMD
\cite{mattys:12}
}%DIFAUXCMD
briefly discussed the concept of `training' in the sense that one can learn to accommodate a particular adverse condition (eg. background noise, signal distortion, etc.) with practice in that area.  This, in theory, allows a listener to identify which methods of release from masking are most effective, and to practice the use of these methods within the specific adverse condition.  Learning is less effective in cases which the degradation is variable or unpredictable between trials, such as with unpredictable background noise}\DIFaddend .


\subsection{Performance of Human Recognition of Speech in Noise}

Eventually, \DIFdelbegin \DIFdel{however, }\DIFdelend with enough background noise and masking, the methods listed above for releasing the masking \DIFdelbegin \DIFdel{will }\DIFdelend fail and recognition \DIFdelbegin \DIFdel{will begin }\DIFdelend \DIFaddbegin \DIFadd{begins }\DIFaddend to break down.  Under the most simple conditions to measure - steady-state noise - \cite{ding:13} \DIFdelbegin \DIFdel{report }\DIFdelend \DIFaddbegin \DIFadd{reported }\DIFaddend that when listening to speech in noise, human \DIFdelbegin \DIFdel{self reported intelligibility ratings don't }\DIFdelend \DIFaddbegin \DIFadd{self-reported intelligibility ratings did not }\DIFaddend drop significantly until the SNR \DIFdelbegin \DIFdel{reaches }\DIFdelend \DIFaddbegin \DIFadd{reached }\DIFaddend approximately -3 dB, where intelligibility \DIFdelbegin \DIFdel{drops }\DIFdelend \DIFaddbegin \DIFadd{dropped }\DIFaddend to about 55\%\DIFdelbegin \DIFdel{, and it doesn't hit near floor level (}\DIFdelend \DIFaddbegin \DIFadd{;  it did not reach }\DIFaddend 0\% \DIFdelbegin \DIFdel{) until }\DIFdelend \DIFaddbegin \DIFadd{self-reported intelligibility until the signal had }\DIFaddend -9 dB SNR.

This subjective measure \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend backed by a study performed by \cite{gilbert:13}, who used the PRESTO corpus (\cite{garofolo:93}) to test sentence intelligibility among 121 native English speakers.  \cite{gilbert:13} found that - similar to \cite{ding:13} - the median score (at the 50th percentile) of speech with a -3dB SNR had about 55\% accuracy.  At +3 dB SNR, the median score increased to approximately 88\% accuracy.

\cite{ding:13} \DIFdelbegin \DIFdel{do however mention }\DIFdelend \DIFaddbegin \DIFadd{mentioned }\DIFaddend that there was great inter-speaker variation among the \DIFdelbegin \DIFdel{reported }\DIFdelend \DIFaddbegin \DIFadd{self-reported, }\DIFaddend subjective perception of intelligibility of an utterance; this \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend also supported by \cite{gilbert:13}'s results. \DIFdelbegin \DIFdel{They }\DIFdelend \DIFaddbegin \DIFadd{The latter }\DIFaddend showed that, averaging over all SNR conditions (-5, -3, 0, +3 dB), the variability between speaker's accuracy scores had a range of almost 36\% for a given item.  A retest performed with a subgroup of the original participants on the same dataset yielded a similar (~34\%) range of variability in accuracy.  

\cite{francis:10} \DIFdelbegin \DIFdel{discusses }\DIFdelend \DIFaddbegin \DIFadd{discussed }\DIFaddend how listening to speech in background noise places extra demands on working memory, as does listening to degraded speech (\cite{francis:09}).  The diversion of working memory to acoustic processing can be particularly detrimental to performance when \DIFdelbegin \DIFdel{simultaneously working }\DIFdelend \DIFaddbegin \DIFadd{a listener simultaneously works }\DIFaddend on other computation, \DIFdelbegin \DIFdel{e.g}\DIFdelend \DIFaddbegin \DIFadd{eg}\DIFaddend . syntactic and semantic parsing (\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{caplan:99}
}%DIFAUXCMD
), such as in }\DIFdelend \DIFaddbegin \DIFadd{eg. }\DIFaddend phrase or sentence recognition \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{caplan:99}
}%DIFAUXCMD
)}\DIFaddend . \cite{tamati:13} tested a group of high-performing hearers of speech in noise against a separate group of low-performing hearers \DIFaddbegin \DIFadd{(all had `normal' hearing) }\DIFaddend using several different working- and short-term memory tasks.  Not surprisingly, the group of listeners who are able to better hear speech in noise also perform statistically better on the working memory tasks.  Working- and short-term memory are by no means the only indicators of perceptual performance.


\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{mattys:12}
}%DIFAUXCMD
briefly discusses the concept of perceptual learning, which asserts that one can learn to accommodate a particular adverse condition (eg.  background noise, signal distortion, etc.  ) with practice in that area.  Learning will be less effective in cases which the degradation is variable or unpredictable between trials, such as with unpredictable background noise .  The speech with the background noises in this present study will be presented in a random order, and since the background noise varies, it cannot be assumed or predicted from one sentence to }\DIFdelend \DIFaddbegin \subsection{Summary}

\DIFadd{Speech in noisy environments presents a challenge for recognition.  Competing acoustic energy from different sources blends together into a single signal that reaches the ear.  The energy from these sources occurs at the same temporal and frequency locations, and can `compete' for space along the auditory pathway.  Energetic masking largely refers to the direct masking of the acoustic energy of the desired voice.  Informational masking largely occurs when there are multiple, intelligible voices that compete for attention.
}

\DIFadd{The auditory system employs many methods to release the desired voice from masking.  These include binaural hearing, the use of acoustic transitions, the use of pitch information, and many others.  Practice recognizing speech in a specific adverse condition allows a listener to improve their recognition accuracy when listening to that adverse condition in }\DIFaddend the \DIFdelbegin \DIFdel{next, and therefore likely will not be `learned' in this sense.
}\DIFdelend \DIFaddbegin \DIFadd{future.  Despite these tools of the auditory system, it is often the case that the background noise is too loud for them to work effectively.  Studies by \mbox{%DIFAUXCMD
\cite{ding:13}
}%DIFAUXCMD
and \mbox{%DIFAUXCMD
\cite{gilbert:13}
}%DIFAUXCMD
indicated the range of SNRs in which human speech perception begins to falter substantially.
}\DIFaddend 

\DIFdelbegin \DIFdel{Although there is expected to be a great amount of variability between subjects, }\DIFdelend \DIFaddbegin \DIFadd{They also described very high variability in the performance of of participants when listening to speech in noise; other research (eg. \mbox{%DIFAUXCMD
\cite{tamati:13}
}%DIFAUXCMD
) has suggested that the variability between the working-memory capacity of listeners may in part contribute to the variability in their ability to recognize speech in noise.
}

\DIFadd{The task presented in Section \ref{expt2} below aimed to compare listeners ability to understand speech in noise, recorded from the mouth, with their ability to understand speech recorded at the ear.  The author hypothesized that, despite missing information from }\DIFaddend the \DIFaddbegin \DIFadd{mid- to high-frequency portion of the spectrum, the ear-recorded speech would be sufficiently devoid of noise to provide a significant improvement in recognition over the noisy mouth-recorded speech.  After the primary experiment, two non-statistical follow-up investigations were conducted, taking advantage of the auditory system's ability to use pitch (F0) information and to benefit from prior exposure to a particular adverse condition.
}


%DIF > Experiment redo

%DIF >  Rediscuss these (learning directly above, cover briefly) in brief background when introducing secondary studies)
%DIF > PERCEPTUAL LEARNING AS IMPETUS FOR READING TRAINING TASK
%DIF > FREQUENCY as release of energetic masking AS IMPETUS FOR COMBINATION STIMULI


\section{Experiment 2: Human Speech Perception in Noise}
\label{expt2}

\DIFadd{The }\DIFaddend results from the \cite{ding:13} and \cite{gilbert:13} studies \DIFdelbegin \DIFdel{indicate that the }\DIFdelend \DIFaddbegin \DIFadd{indicated that the 50}\% \DIFadd{intelligibility threshold (ie. participants correctly identified 50}\% \DIFadd{of the speech) in their studies occurred at -3 db SNR.  The }\DIFaddend average SNR for the \DIFdelbegin \DIFdel{highest noise condition from the data collected and described in Chapter 2\ref{chapter2} }\footnote{\DIFdel{Some of the lowest 80 dB noise condition sentences yielded approximately +6dB SNR}} %DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{is over 9 dB SNR above }\DIFdelend \DIFaddbegin \DIFadd{80 dB noise condition in the Chapter \ref{chapter2} Section \ref{expt1} data collection experiment was +12 dB SNR for the noisy mouth-recorded speech.  This is 15 dB SNR }\textit{\DIFadd{above}} \DIFaddend the 50\% \DIFdelbegin \DIFdel{intelligibility threshold at -3 db SNR given by these two studies.  It }\DIFdelend \DIFaddbegin \DIFadd{threshold identified by \mbox{%DIFAUXCMD
\cite{ding:13}
}%DIFAUXCMD
and \mbox{%DIFAUXCMD
\cite{gilbert:13}
}%DIFAUXCMD
.  At this level of SNR, it }\DIFaddend is unlikely that listeners will encounter much masking \DIFdelbegin \DIFdel{in the collected noisy speech }\DIFdelend \DIFaddbegin \DIFadd{from the noise in the signal }\DIFaddend that won't be overcome.

After testing two pilot participants on the speech collected, \DIFdelbegin \DIFdel{it was }\DIFdelend \DIFaddbegin \DIFadd{the researcher }\DIFaddend deemed that the speech in the noisy background (described in Chapter \DIFdelbegin \DIFdel{2}\DIFdelend \ref{chapter2}) was too easily recognizable.  This conclusion was drawn because the average performance on noisy speech (using word error rate \DIFaddbegin \DIFadd{(WER)}\DIFaddend \footnote{The lower the error rate, the more accurate}) was at a very low 10\% using only the 80 dB noise condition; the average non-noisy mouth-recorded speech was only slightly more accurate at 7\% word error rate. The cause was likely that the SNR ratio was not low enough, \DIFdelbegin \DIFdel{as explained in Section \ref{ch2:limitations}}\DIFdelend \DIFaddbegin \DIFadd{explained in more detail in Section \ref{chap2:limitations}}\DIFaddend .  Due to this, two additional participants were rerun with lower SNRs \DIFdelbegin \DIFdel{, explained more in Section \ref{expt2}}\DIFdelend \DIFaddbegin \DIFadd{(cf. Section \ref{chap3:methods:stimuli})}\DIFaddend .

%DIF < Experiment redo
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  Rediscuss these (learning directly above, cover briefly) in brief background when introducing secondary studies)
%DIF < PERCEPTUAL LEARNING AS IMPETUS FOR READING TRAINING TASK
%DIF < FREQUENCY as release of energetic masking AS IMPETUS FOR COMBINATION STIMULI
%DIFDELCMD < 

%DIFDELCMD < \section{Experiment 3: Human Speech Perception in Noise}
%DIFDELCMD < \label{expt3}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend After additional stimuli were gathered from \DIFaddbegin \DIFadd{these }\DIFaddend two additional participants\DIFdelbegin \DIFdel{(explained below in Section \ref{chap4:methods:stimuli})}\DIFdelend , a human speech perception experiment was run on the data in order to better understand and compare the ability of the auditory system to accurately comprehend the speech with a noisy background and the [modified] speech distorted by passage through the speaker's head.  To act as a control, participants would also listen to the normal, clean speech.

\DIFdelbegin %DIFDELCMD < \subsection{Stimuli Generation}
%DIFDELCMD < \label{chap4:methods:stimuli}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \subsection{Stimulus Generation}
\label{chap3:methods:stimuli}
\DIFaddend 

To remedy the problem of the noisy speech being \textit{too} intelligible and having a high SNR, 
%DIF < 
\DIFdelbegin %DIFDELCMD < \begin{wrapfigure}{r}{0.5\textwidth}
%DIFDELCMD < \centering
%DIFDELCMD <   \includegraphics[width=0.45\textwidth]{figure/overallSetUp_new.png}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{This is the same setup as described in Chapter 2\ref{chapter2}, except that the mouth microphone is facing the loudspeaker, rather than the mouth.}}
  %DIFAUXCMD
%DIFDELCMD < \label{fig:overallSetUp_new}
%DIFDELCMD < \end{wrapfigure}
%DIFDELCMD < %%%
%DIF < 
\DIFdelend two additional participants (one male, one female) were recorded following the procedure in the \DIFdelbegin \DIFdel{first task (cf. Chapter 2\ref{chapter2})}\DIFdelend \DIFaddbegin \DIFadd{initial data collection experiment presented in Chapter \ref{chapter2}, Section \ref{expt1}}\DIFaddend .  The list of stimuli was increased to 80 sentences (eight Harvard Sentence lists\footnote{This included the previous three lists, 14, 28, and 57, as well as lists 21, 29, 37, 53, and 68. These additional lists were pseudo-randomly chosen, as were the original three lists, to contain words that would be readily recognizable by the participant population.}) to provide more reaction data from this present experiment.  

To \DIFdelbegin \DIFdel{increase }\DIFdelend \DIFaddbegin \DIFadd{decrease }\DIFaddend the SNR, \DIFaddbegin \DIFadd{during recording }\DIFaddend the directional microphone was pointed away from the mouth of the \DIFdelbegin \DIFdel{participant}\DIFdelend \DIFaddbegin \DIFadd{participants}\DIFaddend , and directed toward the loudspeaker (see Fig. \ref{fig:overallSetUp_new}).  This \DIFdelbegin \DIFdel{, of course, results }\DIFdelend \DIFaddbegin \DIFadd{differs from the options for remedying the problem of high SNR listed in Chapter \ref{chapter2} Section \ref{chap2:limitations}.  Of these options, (a) - having the speaker intentionally lower their spoken volume - and (b) - increasing the background noise - were deemed unreliable and impractical.  Option (c) - using an omnidirectional microphone - was not considered for this small data collection task due to the lack of availability of omni-directional microphones that fit the size and specification requirements.
}

\DIFadd{Regardless, pointing the directional microphone towards the loudspeaker still resulted }\DIFaddend in some of the limitations outlined in Section \DIFdelbegin \textit{\textbf{%DIFDELCMD < [%%%
\DIFdel{Chapter 2: Limitations}%DIFDELCMD < ]%%%
}%DIFAUXCMD
}%DIFAUXCMD
\DIFdel{\ref{ch2:limitations} in Chapter 2\ref{chap2}; for example, simply pointing the mouth microphone }\DIFdelend \DIFaddbegin \DIFadd{\ref{chap2:limitations} in Chapter \ref{chapter2}.  
%DIF > 
}\begin{wrapfigure}{r}{0.5\textwidth}
\centering
  \includegraphics[width=0.45\textwidth]{figure/overallSetUp_new.png}
  \caption{\DIFadd{This is the same setup as described in Chapter \ref{chapter2}, except that the mouth microphone is facing the loudspeaker, rather than the mouth.}}
  \label{fig:overallSetUp_new}
\end{wrapfigure}
%DIF > 
\DIFadd{It shared the same issue as the omnidirectional microphone, namely that pointing the mouth-microphone }\DIFaddend toward the loudspeaker, rather than increasing the noise, \DIFdelbegin \DIFdel{ignores }\DIFdelend \DIFaddbegin \DIFadd{ignored }\DIFaddend the fact that the noise level inside the ear canal \DIFdelbegin \DIFdel{might }\DIFdelend \DIFaddbegin \DIFadd{would likely }\DIFaddend increase as well with an increase in ambient noise.  Given the alternatives outlined in Section \DIFdelbegin \DIFdel{\ref{ch2:limitations}}\DIFdelend \DIFaddbegin \DIFadd{\ref{chap2:limitations}}\DIFaddend , this was seen as the best available option.  Figures \ref{fig:spectNewMouthNoise} and \ref{fig:spectNewEarNoise} show the new noisy \DIFdelbegin \DIFdel{and ear recorded }\DIFdelend \DIFaddbegin \DIFadd{mouth-recorded and ear-recorded }\DIFaddend speech, respectively.  %DIF < 
\DIFdelbegin %DIFDELCMD < \begin{figure}
%DIFDELCMD < \begin{subfigure}{0.5\textwidth}
%DIFDELCMD <   %%%
\DIFdelend \DIFaddbegin \DIFadd{These are presented along with Figures \ref{fig:spectOldMouthNoise} and \ref{fig:spectOldEarNoise}, the speech collected in the previous experiment in Chapter \ref{chapter2}, for comparison
}

\begin{figure}[h!]
\begin{subfigure}{0.45\textwidth}
  \DIFaddendFL \centering
  \includegraphics[width=0.9\textwidth]{figure/spectNewMouthNoise.png}
  \caption{New recording at the mouth with the microphone pointed toward the loudspeaker noise source.}
  \label{fig:spectNewMouthNoise}
\end{subfigure}
%DIF < 
\DIFdelbeginFL %DIFDELCMD < \begin{subfigure}{0.5\textwidth}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \qquad
\begin{subfigure}{0.45\textwidth}
  \DIFaddendFL \centering
  \includegraphics[width=0.9\textwidth]{figure/spectNewEarNoise.png}
  \caption{New recording at the ear; all recording conditions for the ear were the same as in the first group of recordings.}
  \label{fig:spectNewEarNoise}
\end{subfigure}
\DIFaddbeginFL \\[2ex]
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/spctgrmNarrowMthNoise_35.pdf}
  \DIFaddendFL \caption{\DIFdelbeginFL \DIFdelFL{The sentence ``A cramp is no small danger on a swim'' spoken by the male speaker, recorded at }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Recording from }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{mouth }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{first set of experiments }\DIFaddendFL (\DIFdelbeginFL \DIFdelFL{Fig. \ref{fig:spectNewMouthNoise}}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{in Chapter \ref{chapter2}}\DIFaddendFL )\DIFdelbeginFL \DIFdelFL{with ``cafe'' noise}\DIFdelendFL , \DIFdelbeginFL \DIFdelFL{and simultaneously at }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{with }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{ear (Fig. \ref{fig:spectNewEarNoise})}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{microphone pointed toward the speaker's mouth}\DIFaddendFL .}
  \DIFaddbeginFL \label{fig:spectOldMouthNoise}
\end{subfigure}
\qquad
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/spctgrmNarrowEarNoisePrempFiltPremp.pdf}
  \caption{\DIFaddFL{Recording at the ear from the first set of experiments (in Chapter \ref{chapter2}); all recording conditions for the ear were the same as in the second group of recordings.}}
  \label{fig:spectOldEarNoise}
\end{subfigure}
\caption{\DIFaddFL{The sentence ``A cramp is no small danger on a swim''. In Figs. \ref{fig:spectNewMouthNoise} and \ref{fig:spectNewEarNoise}, spoken by the male speaker, recorded at the mouth with 80 dB `bus' noise and simultaneously at the ear, respectively.  In Figs. \ref{fig:spectOldMouthNoise} and \ref{fig:spectOldEarNoise}, spoken by a female speaker, recorded at the mouth with 80 dB `bus' background noise and simultaneously at the ear, respectively.}}
\DIFaddendFL \end{figure}

\DIFdelbegin \DIFdel{Furthermore, due to the issue of the lack of noise in the noisy speech signals, only one noise level condition was used }\DIFdelend \DIFaddbegin \DIFadd{Note that unlike the previous data collection experiment (cf. Chapter \ref{chapter2}, Section \ref{expt1}), there was only one one noise }\textit{\DIFadd{level}} \DIFaddend - \DIFdelbegin \DIFdel{the highest available noise level (}\DIFdelend 80 \DIFdelbegin \DIFdel{dB)}\DIFdelend \DIFaddbegin \DIFadd{dB.  Data for the 60 and 70 dB noise level conditions were not collected for these two speakers}\DIFaddend .  All previously used background noise types \DIFaddbegin \DIFadd{(bus, caf}\'{e}\DIFadd{, pedestrian area, street, factory) }\DIFaddend were used for this data collection task as well.

\DIFaddbegin \DIFadd{The SNR was calculated for the male speaker using the code specified in Chapter \ref{chapter2} Section \ref{chap2:observations}.  This yielded an average value of slightly below +1 dB SNR for the noisy speech recorded with 80 dB background noise.  This was substantially lower than the 80 dB condition in the data collection experiment with an average SNR of +12 dB.  At that time, the female speaker's SNR was not calculated.  This was later (cf. Sections \ref{chap3:discussion}, \ref{chap3:future-research}) deemed to be an obvious mistake, as the female speaker's SNR was much higher than the male speaker's SNR at approximately +8 dB SNR.  It was likely that the female speaker spoke louder than the male speaker during the experiment, which resulted in the difference.
}


\DIFaddend \subsection{Design}
\DIFdelbegin %DIFDELCMD < \label{chap4:methods:design}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{chap3:methods:design}
\DIFaddend 

The experiment had three factors - gender of speaker\footnote{This \DIFdelbegin \DIFdel{has been }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend included to \DIFdelbegin \DIFdel{ensure }\DIFdelend \DIFaddbegin \DIFadd{test if }\DIFaddend there is \DIFdelbegin \DIFdel{no }\DIFdelend \DIFaddbegin \DIFadd{an }\DIFaddend effect of gender \DIFaddbegin \DIFadd{of the speaker }\DIFaddend on \DIFaddbegin \DIFadd{listeners' }\DIFaddend perception in noise or on transmission of speech through the head and into the ear canal, due to the difference in male and female vocal \DIFdelbegin \DIFdel{tracts}\DIFdelend \DIFaddbegin \DIFadd{tract and aural anatomy}\DIFaddend .} \textbf{x} microphone location \textbf{x} noise type - resulting in a 2x2x6 experiment.  There were two genders \DIFaddbegin \DIFadd{(female, male)}\DIFaddend , two mic locations (recording at the ear, \DIFdelbegin \DIFdel{and }\DIFdelend \DIFaddbegin \DIFadd{recording }\DIFaddend at the mouth), and six noise types (bus, \DIFdelbegin \DIFdel{cafe}\DIFdelend \DIFaddbegin \DIFadd{caf}\'{e}\DIFaddend , pedestrian, street, factory, and no noise (clean)).  Since the ability to understand speech in noise is quite variable between individuals \DIFaddbegin \DIFadd{(cf. \mbox{%DIFAUXCMD
\cite{ding:13,gilbert:13}
}%DIFAUXCMD
)}\DIFaddend , the design of this experiment was a within-subjects experiment.  This meant that each of the 2x2x6 (ie. 24) conditions needed to be \DIFdelbegin \DIFdel{seen }\DIFdelend \DIFaddbegin \DIFadd{heard }\DIFaddend by each participant.  The sessions that were re-recorded utilized 80 distinct sentences, which allowed for three sentences to appear in each of the 24 conditions, totaling 72 sentences used in the experiment.  The eight remaining sentences were used as a \DIFdelbegin \DIFdel{``training'' set , }\DIFdelend \DIFaddbegin \DIFadd{`training' set }\DIFaddend intended to get the participants used to the task itself, rather than to acclimate them to the type of speech that they would hear.

Since \DIFdelbegin \DIFdel{any given speaker }\DIFdelend \DIFaddbegin \DIFadd{a given participant }\DIFaddend could not hear the same sentence twice without introducing a \DIFaddbegin \DIFadd{training }\DIFaddend confound, and since each sentence was recorded in each of the 24 conditions\footnote{72 distinct sentences * 24 conditions = 1728 total sentence recordings}, this necessitated the use of 24 \DIFdelbegin \DIFdel{co-balanced }\DIFdelend \DIFaddbegin \DIFadd{counter-balanced }\DIFaddend groups to ensure that each \DIFdelbegin \DIFdel{sentence was heard in every condition }\DIFdelend \DIFaddbegin \DIFadd{one of the 1728 sentences was heard }\DIFaddend by at least one \DIFdelbegin \DIFdel{speaker}\DIFdelend \DIFaddbegin \DIFadd{participant listener}\DIFaddend .  For example, \DIFdelbegin \DIFdel{sentences }\DIFdelend \DIFaddbegin \DIFadd{items (sentences) }\DIFaddend 1, 2, and 3 \DIFdelbegin \DIFdel{would occur in Factor Combination }\DIFdelend \DIFaddbegin \DIFadd{occurred in Condition }\DIFaddend \#1 (\DIFdelbegin \DIFdel{e.g}\DIFdelend \DIFaddbegin \DIFadd{eg}\DIFaddend . female speaker, mic at the mouth, with bus background noise) in \DIFdelbegin \DIFdel{co-balanced }\DIFdelend \DIFaddbegin \DIFadd{counter-balanced }\DIFaddend group \#1 for Participant \#1. Participant \#2 \DIFdelbegin \DIFdel{would see co-balanced }\DIFdelend \DIFaddbegin \DIFadd{saw counter-balanced }\DIFaddend group \#2, which placed \DIFdelbegin \DIFdel{sentences }\DIFdelend \DIFaddbegin \DIFadd{items (sentences) }\DIFaddend 1, 2, and 3 in \DIFdelbegin \DIFdel{Factor Combination }\DIFdelend \DIFaddbegin \DIFadd{Condition }\DIFaddend \#2 (\DIFdelbegin \DIFdel{e.g}\DIFdelend \DIFaddbegin \DIFadd{eg}\DIFaddend . female speaker, mic at the mouth, with \DIFdelbegin \DIFdel{cafe }\DIFdelend \DIFaddbegin \DIFadd{caf}\'{e} \DIFaddend background noise).  \DIFdelbegin \DIFdel{For simplicity's sake, each grouping of three sentences would appear }\DIFdelend \DIFaddbegin \DIFadd{Each three-item (sentence) grouping always appeared }\DIFaddend together in a \DIFdelbegin \DIFdel{given condition, and were not mixed up between the different co-balanced groups. }\DIFdelend \DIFaddbegin \DIFadd{condition, ie. item (sentence) groupings always appeared together in a counter-balanced group.  
}

\DIFaddend However, once the sentences \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{were }\DIFaddend assigned to a particular condition, the order of presentation to \DIFdelbegin \DIFdel{the participants is randomized}\DIFdelend \DIFaddbegin \DIFadd{each participants was randomized; each participant received a different random stimulus order.  Due to the randomized order, it was more difficult for participants to predict the noise type from one sentence to the next, which was intended to slow down the rate at which participants became familiarized with the noise types}\DIFaddend .

\subsection{Participants}
\DIFaddbegin \label{chap3:methods:participants}
\DIFaddend 

Twenty-four native speakers of English with self-reported normal hearing participated in the experiment. Each participant was placed into a separate \DIFdelbegin \DIFdel{co-balanced }\DIFdelend \DIFaddbegin \DIFadd{counter-balanced }\DIFaddend group, as specified above in Section \DIFdelbegin \DIFdel{\ref{chap4:methods:design}}\DIFdelend \DIFaddbegin \DIFadd{\ref{chap3:methods:design}.  Due to resource constraints that prevented the use of 48 or more participants, each counter-balanced group only had one participant}\DIFaddend .

\subsection{Equipment}
\DIFaddbegin \label{chap3:methods:equipment}
\DIFaddend 

The experiment was conducted in a soundbooth with a pair of over-the-ear headphones.  The experiment interface utilized in-house developed software\DIFdelbegin \DIFdel{, and participants' answers were typed into a textbox in this program, on a computer }\DIFdelend \DIFaddbegin \DIFadd{.  The software played the stimulus for the listeners and provided a textbox for them to type their answers.  It placed a time limit on their response, and locked in their answer after 18 seconds if the participant had not already advanced to the next stimulus. The software recorded the participants' answer to each stimulus in a log file.  A computer placed outside the soundbooth, }\DIFaddend whose monitor could be seen from inside the soundbooth\DIFaddbegin \DIFadd{, was used to run the software and displayed the textbox for entering responses}\DIFaddend .

\subsection{Procedure}
\DIFdelbegin %DIFDELCMD < \label{hsp-main-procedure}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{chap3:methods:procedure}
\DIFaddend 

The participant was seated in the sound booth in front of a keyboard and computer monitor\DIFaddbegin \footnote{\DIFadd{The computer monitor was outside the soundbooth, and did not produce noise interfering with the task.}} \DIFaddend with a pair of headphones, and was given a set of instructions. They were told that they would hear each utterance only once, and \DIFdelbegin \DIFdel{what they would hear was comprised of }\DIFdelend \DIFaddbegin \DIFadd{that each utterance contained only }\DIFaddend real English words\DIFdelbegin \DIFdel{, but may not constitute a ``complete'' sentence.  They were forewarned }\DIFdelend \DIFaddbegin \DIFadd{; they were told it was possible for a sentence to not be a `complete' sentence.  The researcher forewarned them }\DIFaddend that many of the sentences they would hear would be noisy and difficult to understand. They were instructed to write all words they heard, even if what was heard did not make syntactic sense, or if the words were not adjacent (\DIFdelbegin \DIFdel{e.g}\DIFdelend \DIFaddbegin \DIFadd{eg}\DIFaddend . if only the first and last \DIFdelbegin \DIFdel{word }\DIFdelend \DIFaddbegin \DIFadd{words }\DIFaddend of the sentence \DIFdelbegin \DIFdel{was }\DIFdelend \DIFaddbegin \DIFadd{were }\DIFaddend heard). They \DIFdelbegin \DIFdel{would be timed, with }\DIFdelend \DIFaddbegin \DIFadd{were given }\DIFaddend 18 seconds to type their response\DIFdelbegin \DIFdel{starting from the beginning of the sound file and that their answer would be saved as-is if they ran into the time limit, preventing them from typing more}\DIFdelend \DIFaddbegin \DIFadd{, beginning at the onset of the utterance audio, before their answer was locked in}\DIFaddend .

The participant was told that the first set of eight utterances they heard were part of a \DIFdelbegin \DIFdel{``training'' set of eight utterances }\DIFdelend \DIFaddbegin \DIFadd{`training' set }\DIFaddend intended to familiarize them with the task\DIFdelbegin \footnote{\DIFdel{The same eight sentences were heard by every participant}}%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{. The same eight sentences were heard by every participant}\DIFaddend .  None of the utterances from the training set were used in the analysis.  Once \DIFdelbegin \DIFdel{completed with this }\DIFdelend \DIFaddbegin \DIFadd{they they finished the }\DIFaddend initial set, participants were \DIFdelbegin \DIFdel{asked }\DIFdelend \DIFaddbegin \DIFadd{able to ask }\DIFaddend if they had any questions.  Afterwards\DIFaddbegin \DIFadd{, }\DIFaddend they began the primary task in the soundbooth.  They would hear one of the sentences, and type their answer in a text box.  When finished with their answer, they would either click to advance to the next sentence, or, if they ran into the time limit, \DIFaddbegin \DIFadd{they }\DIFaddend were prevented from modifying their answer\DIFdelbegin \DIFdel{, and were prompted }\DIFdelend \DIFaddbegin \DIFadd{.  The software prompted them }\DIFaddend to click another button to advance\DIFdelbegin \DIFdel{.  When finished with all 72 stimuli, the participant was given a brief questionnaire to fill out}\DIFdelend \DIFaddbegin \DIFadd{, and did not advance automatically}\DIFaddend .

After the experiment, the researcher \DIFdelbegin \DIFdel{would double check }\DIFdelend \DIFaddbegin \DIFadd{double checked the }\DIFaddend participant answers for correct spelling.  Only obvious errors were modified (\DIFdelbegin \DIFdel{e.g}\DIFdelend \DIFaddbegin \DIFadd{eg}\DIFaddend . `teh' to `the', `crakers' to `crackers', `mantle' to `mantel'), while ambiguous errors were left as-is (\DIFdelbegin \DIFdel{e.g}\DIFdelend \DIFaddbegin \DIFadd{eg}\DIFaddend . `blo' was not changed to `block', `finde' was not changed to `fine').  Numbers were also lexicalized (\DIFdelbegin \DIFdel{e.g}\DIFdelend \DIFaddbegin \DIFadd{eg}\DIFaddend . `30' to `thirty').  Punctuation was removed for ease of analysis and calculation of word error rate.  Exact responses given by each participant can be found in Appendix F\ref{appendixF}.


\section{Results}
\DIFdelbegin %DIFDELCMD < \label{ch4:results}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{chap3:results}
\DIFaddend 



The word error rate (WER) for each (spell-checked) response for each participant was calculated. The code for the WER calculation can be found in Appendix F\ref{appendix?}

A 3-way, within-subjects ANOVA was performed with the collected data - 72 sentences from each of the 24 participants. Factors included the gender \DIFaddbegin \DIFadd{(}\DIFaddend of the speaker \DIFdelbegin \DIFdel{(of the stimulus) with two levels - maleand female- the location of the recording microphone with two levels - }\DIFdelend \DIFaddbegin \DIFadd{of the pre-recorded stimulus; male, female), the recording (mic) location (}\DIFaddend at the mouth\DIFdelbegin \DIFdel{and }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend at the ear\DIFdelbegin \DIFdel{- and the }\DIFdelend \DIFaddbegin \DIFadd{), and }\DIFaddend background noise type \DIFdelbegin \DIFdel{with six levels - }\DIFdelend \DIFaddbegin \DIFadd{(}\DIFaddend no noise, bus\DIFdelbegin \DIFdel{noise, cafe noise, pedestrian noise, streetnoise, and factorynoise.  There was no significant 3-way interaction between speaker gender, noise type, and mic location, as can be seen in the by-subjects ANOVA }\DIFdelend \DIFaddbegin \DIFadd{, caf}\'{e}\DIFadd{, pedestrian area, street, factory) - 2}\textbf{\DIFadd{x}}\DIFadd{2}\textbf{\DIFadd{x}}\DIFadd{6 design.  WER was the dependent variable.  An ANOVA was chosen to test for statistical significance over an LME due to the large number of conditions in the study; the reported results from the ANOVA were much more concise, whereas the LME would have required a substantial amount of re-leveling comparison tests.  
}

\DIFadd{By-subjects ($S$) and by-items }\DIFaddend (\DIFdelbegin \DIFdel{Table \ref{tab:anova1_by_subj}) and the }\DIFdelend \DIFaddbegin \DIFadd{$I$) ANOVAs were performed (cf. Table \ref{tab:anova1}). Mauchley's Test for Sphericity}\footnote{\DIFadd{Sphericity assumes that the variance of the differences between all pairs of conditions is the same; sphericity is violated when this is not the case.}} \DIFadd{was conducted, both for the by-subjects and }\DIFaddend by-items \DIFdelbegin \DIFdel{ANOVA (Table \ref{tab:anova1_by_item}}\DIFdelend \DIFaddbegin \DIFadd{ANOVAs.  Significant sphericity violations were found for the main effect of noise type, and the interaction of speaker gender and noise type.  These were corrected for using a Greenhouse-Geisser test, but they resulted in no change to statistical significance.  The values below incorporate the corrections.
}

%DIF >  ($F_1$($by_subj_ez_anova$ANOVA$DFn[6]$,$by_subj_ez_anova$ANOVA$DFd[6]$) = $by_subj_ez_anova$ANOVA$F[6]$, $p>0.1$; $F_1$($by_items_ez_anova$ANOVA$DFn[6]$,$by_items_ez_anova$ANOVA$DFd[6]$) = $by_items_ez_anova$ANOVA$F[6]$, $p>0.05$)

\DIFadd{The main effects of all three factors were significant (cf. Table \ref{tab:anova1}}\DIFaddend ).  Two, two-way interactions were significant\DIFdelbegin \DIFdel{, }\DIFdelend \DIFaddbegin \DIFadd{: }\DIFaddend speaker gender \textbf{x} mic location, and noise-type \textbf{x} mic location. The two way interaction between speaker gender and noise type was not significant\DIFdelbegin \DIFdel{.  The main effects of all three factors were also significant (cf. Tables \ref{tab:anova1_by_subj} and \ref{tab:anova1_by_item})}\DIFdelend \DIFaddbegin \DIFadd{, nor was the three-way interaction between speaker gender, mic location, and noise type}\DIFaddend .


% latex table generated in R 3.4.1 by xtable 1.8-2 package
%DIF <  Sun Jul  9 18:16:31 2017
%DIF >  Wed Jul 26 23:37:03 2017
\begin{table}[ht]
\centering
\DIFdelbeginFL %DIFDELCMD < \begin{tabular}{lrrrrl}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \begin{tabular}{llrrrrl}
  \DIFaddendFL \hline
\DIFaddbeginFL \DIFaddFL{ANOVA }& \DIFaddendFL Effect & DFn & DFd & F & p & \DIFdelbeginFL \DIFdelFL{p$<$.05 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{* }\DIFaddendFL \\ 
  \hline
\DIFaddbeginFL \DIFaddFL{S }& \DIFaddendFL speaker\_gender & \DIFdelbeginFL \DIFdelFL{1.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{23.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{23 }\DIFaddendFL & 6.69 & 0.02 & * \\ 
  \DIFdelbeginFL \DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }%DIFDELCMD < & %%%
\DIFdelFL{5.00 }%DIFDELCMD < & %%%
\DIFdelFL{115.00 }%DIFDELCMD < & %%%
\DIFdelFL{84.83 }%DIFDELCMD < & %%%
\DIFdelFL{0.00 }%DIFDELCMD < & %%%
\DIFdelFL{* }%DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelFL{mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }%DIFDELCMD < & %%%
\DIFdelFL{1.00 }%DIFDELCMD < & %%%
\DIFdelFL{23.00 }%DIFDELCMD < & %%%
\DIFdelFL{155.07 }%DIFDELCMD < & %%%
\DIFdelFL{0.00 }%DIFDELCMD < & %%%
\DIFdelFL{* }%DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }%DIFDELCMD < & %%%
\DIFdelFL{5.00 }%DIFDELCMD < & %%%
\DIFdelFL{115.00 }%DIFDELCMD < & %%%
\DIFdelFL{0.55 }%DIFDELCMD < & %%%
\DIFdelFL{0.74 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }\DIFaddendFL & \DIFdelbeginFL %DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelendFL speaker\_gender \DIFdelbeginFL \DIFdelFL{:mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }\DIFdelendFL & \DIFdelbeginFL \DIFdelFL{1.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{23.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{71 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{18.53 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{4.01 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.05 }\DIFaddendFL & * \\ 
  \DIFaddbeginFL \DIFaddFL{S }& \DIFaddendFL noise\_type \DIFdelbeginFL \DIFdelFL{:mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }\DIFdelendFL & \DIFdelbeginFL \DIFdelFL{5.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{5 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{115.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{115 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{55.53 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{84.83 }\DIFaddendFL & 0.00 & * \\ 
  \DIFdelbeginFL \DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype:mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }%DIFDELCMD < & %%%
\DIFdelFL{5.00 }%DIFDELCMD < & %%%
\DIFdelFL{115.00 }%DIFDELCMD < & %%%
\DIFdelFL{1.61 }%DIFDELCMD < & %%%
\DIFdelFL{0.16 }%DIFDELCMD < &  \\ 
%DIFDELCMD <    \hline
%DIFDELCMD < \end{tabular}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{ANOVA for by-subjects analysis of the three-factor, within-subjects experiment.}} 
%DIFAUXCMD
%DIFDELCMD < \label{tab:anova1_by_subj}
%DIFDELCMD < \end{table}
%DIFDELCMD < %%%
%DIF <  latex table generated in R 3.4.1 by xtable 1.8-2 package
%DIF <  Sun Jul  9 18:16:32 2017
%DIFDELCMD < \begin{table}[ht]
%DIFDELCMD < \centering
%DIFDELCMD < \begin{tabular}{lrrrrl}
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{Effect }%DIFDELCMD < & %%%
\DIFdelFL{DFn }%DIFDELCMD < & %%%
\DIFdelFL{DFd }%DIFDELCMD < & %%%
\DIFdelFL{F }%DIFDELCMD < & %%%
\DIFdelFL{p }%DIFDELCMD < & %%%
\DIFdelFL{p$<$.05 }%DIFDELCMD < \\ 
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender }%DIFDELCMD < & %%%
\DIFdelFL{1.00 }%DIFDELCMD < & %%%
\DIFdelFL{71.00 }%DIFDELCMD < & %%%
\DIFdelFL{4.01 }%DIFDELCMD < & %%%
\DIFdelFL{0.05 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{* }%DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelendFL noise\_type & \DIFdelbeginFL \DIFdelFL{5.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{5 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{355.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{355 }\DIFaddendFL & 103.50 & 0.00 & * \\ 
  \DIFaddbeginFL \DIFaddFL{S }& \DIFaddendFL mic\_location & \DIFdelbeginFL \DIFdelFL{1.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{71.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{23 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{354.53 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{155.07 }\DIFaddendFL & 0.00 & * \\ 
  \DIFdelbeginFL \DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }%DIFDELCMD < & %%%
\DIFdelFL{5.00 }%DIFDELCMD < & %%%
\DIFdelFL{355.00 }%DIFDELCMD < & %%%
\DIFdelFL{0.52 }%DIFDELCMD < & %%%
\DIFdelFL{0.76 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }\DIFaddendFL & \DIFdelbeginFL %DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:}\DIFdelendFL mic\_location & \DIFdelbeginFL \DIFdelFL{1.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{71.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{71 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{21.36 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{354.53 }\DIFaddendFL & 0.00 & * \\ 
  \DIFdelbeginFL \DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype:mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }%DIFDELCMD < & %%%
\DIFdelFL{5.00 }%DIFDELCMD < & %%%
\DIFdelFL{355.00 }%DIFDELCMD < & %%%
\DIFdelFL{71.03 }%DIFDELCMD < & %%%
\DIFdelFL{0.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{S }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{* }%DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelendFL speaker\_gender:noise\_type \DIFdelbeginFL \DIFdelFL{:mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }\DIFdelendFL & \DIFdelbeginFL \DIFdelFL{5.00 }%DIFDELCMD < & %%%
\DIFdelFL{355.00 }%DIFDELCMD < & %%%
\DIFdelFL{1.86 }%DIFDELCMD < & %%%
\DIFdelFL{0.10 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{5 }\DIFaddendFL & \DIFdelbeginFL %DIFDELCMD < \\ 
%DIFDELCMD <    \hline
%DIFDELCMD < \end{tabular}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{ANOVA for by-items analysis of the three-factor, within-subjects experiment.}} 
%DIFAUXCMD
%DIFDELCMD < \label{tab:anova1_by_item}
%DIFDELCMD < \end{table}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelFL{Mauchley's Test for Sphericity}\footnote{\DIFdelFL{Sphericity assumes that the variance between levels of a factor are the same; sphericity is violated when this is not the case.}} %DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdelFL{was conducted, both for the by-subjects and by-items ANOVAs.  Significant sphericity violations were found for the main effect of noise type, and the interaction of speaker gender and noise type, as can be seen in Tables \ref{tab:anova1_subj_sph_test} and \ref{tab:anova1_item_sph_test}. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  latex table generated in R 3.4.1 by xtable 1.8-2 package
%DIF <  Sun Jul  9 18:16:32 2017
%DIFDELCMD < \begin{table}[ht]
%DIFDELCMD < \centering
%DIFDELCMD < \begin{tabular}{lrrl}
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{Effect }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{115 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{W }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.55 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{p }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.74 }\DIFaddendFL &  \DIFdelbeginFL \DIFdelFL{p$<$.05 }\DIFdelendFL \\ 
  \DIFdelbeginFL %DIFDELCMD < \hline
%DIFDELCMD < %%%
\DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }%DIFDELCMD < & %%%
\DIFdelFL{0.21 }%DIFDELCMD < & %%%
\DIFdelFL{0.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{* }%DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelendFL speaker\_gender:noise\_type & \DIFdelbeginFL \DIFdelFL{0.61 }%DIFDELCMD < & %%%
\DIFdelFL{0.73 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{5 }\DIFaddendFL & \DIFdelbeginFL %DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype:mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{355 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.39 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.52 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.13 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.76 }\DIFaddendFL &  \\ 
  \DIFaddbeginFL \DIFaddFL{S }& \DIFaddendFL speaker\_gender:\DIFdelbeginFL \DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype:}\DIFdelendFL mic\_location & \DIFdelbeginFL \DIFdelFL{0.67 }%DIFDELCMD < & %%%
\DIFdelFL{0.87 }%DIFDELCMD < &  \\ 
%DIFDELCMD <    \hline
%DIFDELCMD < \end{tabular}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Sphericity test for the by-subjects ANOVA.}} 
%DIFAUXCMD
%DIFDELCMD < \label{tab:anova1_subj_sph_test}
%DIFDELCMD < \end{table}
%DIFDELCMD < %%%
%DIF <  latex table generated in R 3.4.1 by xtable 1.8-2 package
%DIF <  Sun Jul  9 18:16:32 2017
%DIFDELCMD < \begin{table}[ht]
%DIFDELCMD < \centering
%DIFDELCMD < \begin{tabular}{lrrl}
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{Effect }%DIFDELCMD < & %%%
\DIFdelFL{W }%DIFDELCMD < & %%%
\DIFdelFL{p }%DIFDELCMD < & %%%
\DIFdelFL{p$<$.05 }%DIFDELCMD < \\ 
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }%DIFDELCMD < & %%%
\DIFdelFL{0.78 }%DIFDELCMD < & %%%
\DIFdelFL{0.26 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL %DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{23 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.64 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{18.53 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.01 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.00 }\DIFaddendFL & * \\ 
  \DIFdelbeginFL \DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype:mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }%DIFDELCMD < & %%%
\DIFdelFL{0.85 }%DIFDELCMD < & %%%
\DIFdelFL{0.66 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }\DIFaddendFL & \DIFdelbeginFL %DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelendFL speaker\_gender:\DIFdelbeginFL \DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype:}\DIFdelendFL mic\_location & \DIFdelbeginFL \DIFdelFL{0.86 }%DIFDELCMD < & %%%
\DIFdelFL{0.70 }%DIFDELCMD < &  \\ 
%DIFDELCMD <    \hline
%DIFDELCMD < \end{tabular}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Sphericity test for the by-items ANOVA.}} 
%DIFAUXCMD
%DIFDELCMD < \label{tab:anova1_item_sph_test}
%DIFDELCMD < \end{table}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelFL{The corrections for sphericity were performed using a Greenhouse-Geisser test, for both by-subjects (Table \ref{tab:anova1_subj_sph_corr}) and by-items (Table \ref{tab:anova1_item_sph_corr}) ANOVAs.  Neither resulted in a change to any prior finding.
}%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  latex table generated in R 3.4.1 by xtable 1.8-2 package
%DIF <  Sun Jul  9 18:16:32 2017
%DIFDELCMD < \begin{table}[ht]
%DIFDELCMD < \centering
%DIFDELCMD < \begin{tabular}{lrrl}
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{Effect }%DIFDELCMD < & %%%
\DIFdelFL{GGe }%DIFDELCMD < & %%%
\DIFdelFL{p}%DIFDELCMD < [%%%
\DIFdelFL{GG}%DIFDELCMD < ] %%%
\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{p}%DIFDELCMD < [%%%
\DIFdelFL{GG}%DIFDELCMD < ]%%%
\DIFdelFL{$<$.05 }%DIFDELCMD < \\ 
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{71 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.61 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{21.36 }\DIFaddendFL & 0.00 & * \\ 
  \DIFdelbeginFL \DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }%DIFDELCMD < & %%%
\DIFdelFL{0.86 }%DIFDELCMD < & %%%
\DIFdelFL{0.71 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{S }\DIFaddendFL & \DIFdelbeginFL %DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelendFL noise\_type:mic\_location & \DIFdelbeginFL \DIFdelFL{0.77 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{5 }& \DIFaddFL{115 }& \DIFaddFL{55.53 }\DIFaddendFL & 0.00 & * \\ 
  \DIFdelbeginFL \DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }& \DIFaddendFL noise\_type:mic\_location & \DIFdelbeginFL \DIFdelFL{0.87 }%DIFDELCMD < & %%%
\DIFdelFL{0.17 }%DIFDELCMD < &  \\ 
%DIFDELCMD <    \hline
%DIFDELCMD < \end{tabular}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Sphericity corrections for the by-subjects ANOVA.}} 
%DIFAUXCMD
%DIFDELCMD < \label{tab:anova1_subj_sph_corr}
%DIFDELCMD < \end{table}
%DIFDELCMD < %%%
%DIF <  latex table generated in R 3.4.1 by xtable 1.8-2 package
%DIF <  Sun Jul  9 18:16:32 2017
%DIFDELCMD < \begin{table}[ht]
%DIFDELCMD < \centering
%DIFDELCMD < \begin{tabular}{lrrl}
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{Effect }%DIFDELCMD < & %%%
\DIFdelFL{GGe }%DIFDELCMD < & %%%
\DIFdelFL{p}%DIFDELCMD < [%%%
\DIFdelFL{GG}%DIFDELCMD < ] %%%
\DIFdelendFL \DIFaddbeginFL \DIFaddFL{5 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{p}%DIFDELCMD < [%%%
\DIFdelFL{GG}%DIFDELCMD < ]%%%
\DIFdelFL{$<$.05 }%DIFDELCMD < \\ 
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{355 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.91 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{71.03 }\DIFaddendFL & 0.00 & * \\ 
  \DIFaddbeginFL \DIFaddFL{S }& \DIFaddendFL speaker\_gender:noise\_type\DIFdelbeginFL %DIFDELCMD < & %%%
\DIFdelFL{0.87 }%DIFDELCMD < & %%%
\DIFdelFL{0.73 }%DIFDELCMD < &  \\ 
%DIFDELCMD <   %%%
\DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype}\DIFdelendFL :mic\_location & \DIFdelbeginFL \DIFdelFL{0.94 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{5 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{115 }& \DIFaddFL{1.61 }& \DIFaddFL{0.16 }\DIFaddendFL &  \DIFdelbeginFL \DIFdelFL{* }\DIFdelendFL \\ 
  \DIFaddbeginFL \DIFaddFL{I }& \DIFaddendFL speaker\_gender:noise\_type:mic\_location & \DIFdelbeginFL \DIFdelFL{0.95 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{5 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.11 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{355 }& \DIFaddFL{1.86 }& \DIFaddFL{0.10 }\DIFaddendFL &  \\ 
   \hline
\end{tabular}
\caption{\DIFdelbeginFL \DIFdelFL{Sphericity corrections }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{ANOVA }\DIFaddendFL for \DIFdelbeginFL \DIFdelFL{the }\DIFdelendFL by-subjects \DIFaddbeginFL \DIFaddFL{(S) and by-items (I) analysis of the three-factor, within-subjects experiment. `}\DIFaddendFL ANOVA\DIFaddbeginFL \DIFaddFL{' indicates by-subject (S) or by-item (I) analysis, `Effect' lists the factor(s) tested, `DFn' and `DFd' refer to the degrees of freedom numerator and denominator, `F' is the F-score, `p' is the p-value, and `*' marks a significant p-value}\DIFaddendFL .} 
\DIFdelbeginFL %DIFDELCMD < \label{tab:anova1_item_sph_corr}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \label{tab:anova1}
\DIFaddendFL \end{table}



\DIFdelbegin \DIFdel{Noting the sphericity violations involving the noise type condition, the data was viewed in the box plots in Figures \ref{fig:anova1_noise_boxplot}, \ref{fig:anova1_noiseXspkr_boxplot}, and \ref{fig:anova1_noiseXmic_boxplot}.
When viewing the simple effects of noise in Figure \ref{fig:anova1_noise_boxplot}, it is apparent (and intuitive) that the no-noise condition differs distinctly from the other noise types.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{This holds true when viewing the interaction of speaker gender }\textbf{\DIFdel{x}} %DIFAUXCMD
\DIFdel{noise type in 
}\DIFdelend %DIF >  Mauchley's Test for Sphericity\footnote{Sphericity assumes that the variance of the differences between all pairs of conditions is the same; sphericity is violated when this is not the case.} was conducted, both for the by-subjects and by-items ANOVAs.  Significant sphericity violations were found for the main effect of noise type, and the interaction of speaker gender and noise type, as can be seen in Tables \ref{tab:anova1_subj_sph_test} and \ref{tab:anova1_item_sph_test}. 
% 
\DIFdelbegin %DIFDELCMD < \begin{wrapfigure}{l!}{0.5\textwidth}
%DIFDELCMD < 

%DIFDELCMD < \includegraphics[width=\maxwidth]{figure/boxplot_noise-1} 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Boxplot displaying the average word error rate (WER) averaged over each participant for every noise type. WER is the variable on the y-axis, and noise type is on the x-axis.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:anova1_noise_boxplot}
%DIFDELCMD < \end{wrapfigure}
%DIFDELCMD < %%%
\DIFdelend %DIF >  <<ANOVA1_sphericity_test, echo=FALSE, results='asis'>>=
%DIF >  print(xtable(by_subj_ez_anova$`Mauchly's Test for Sphericity`, label="tab:anova1_subj_sph_test", caption="Sphericity test for the by-subjects ANOVA."), include.rownames=FALSE)
%DIF >  print(xtable(by_item_ez_anova$`Mauchly's Test for Sphericity`, label="tab:anova1_item_sph_test", caption="Sphericity test for the by-items ANOVA."), include.rownames=FALSE)
%DIF >  by_subj_ez_anova$`Sphericity Corrections`$HFe <- NULL
%DIF >  by_item_ez_anova$`Sphericity Corrections`$HFe <- NULL
%DIF >  by_subj_ez_anova$`Sphericity Corrections`$`p[HF]` <- NULL
%DIF >  by_item_ez_anova$`Sphericity Corrections`$`p[HF]` <- NULL
%DIF >  by_subj_ez_anova$`Sphericity Corrections`$`p[HF]<.05` <- NULL
%DIF >  by_item_ez_anova$`Sphericity Corrections`$`p[HF]<.05` <- NULL
%DIF >  @
% 
\DIFdelbegin \DIFdel{Figure \ref{fig:anova1_noiseXspkr_boxplot} and the interaction of }\DIFdelend %DIF >  The corrections for sphericity were performed using a Greenhouse-Geisser test, for both by-subjects (Table \ref{tab:anova1_subj_sph_corr}) and by-items (Table \ref{tab:anova1_item_sph_corr}) ANOVAs.  Neither resulted in a change to any prior finding.
%DIF >  
%DIF >  <<ANOVA1_sphericity_corr, echo=FALSE, results='asis'>>=
%DIF >  print(xtable(by_subj_ez_anova$`Sphericity Corrections`, label="tab:anova1_subj_sph_corr", caption="Sphericity corrections for the by-subjects ANOVA."), include.rownames=FALSE)
%DIF >  print(xtable(by_item_ez_anova$`Sphericity Corrections`, label="tab:anova1_item_sph_corr", caption="Sphericity corrections for the by-subjects ANOVA."), include.rownames=FALSE)
%DIF >  @
%DIF >  Noting the sphericity violations involving the noise type condition, the data was viewed in the box plots in Figures \ref{fig:anova1_noise_boxplot}, \ref{fig:anova1_noiseXspkr_boxplot}, and \ref{fig:anova1_noiseXmic_boxplot}.
%DIF >  When viewing the simple effects of noise in Figure \ref{fig:anova1_noise_boxplot}, it is apparent (and intuitive) that the no-noise condition differs distinctly from the other noise types.
%DIF >  
%DIF >  This holds true when viewing the interaction of speaker gender \textbf{x} noise type in 
%DIF >  %
%DIF >  \begin{wrapfigure}{l!}{0.5\textwidth}
%DIF >  <<boxplot_noise, echo=FALSE, results='asis'>>=
%DIF >  par(mar=c(7,5,0,0))
%DIF >  boxplot(wer~noise_type,col=c("white","lightgray"),mydata, las=2, names=c("No Noise","Bus","Cafe","Pedestrian","Street","Factory"),ylab="WER", cex.axis=1.5, cex.lab=2.0)
%DIF >  @
%DIF >  \caption{Boxplot displaying the average word error rate (WER) averaged over each participant for every noise type. WER is the variable on the y-axis, and noise type is on the x-axis.}
%DIF >  \label{fig:anova1_noise_boxplot}
%DIF >  \end{wrapfigure}
%DIF >  %
%DIF >  Figure \ref{fig:anova1_noiseXspkr_boxplot} and the interaction of noise type \textbf{x} mic location in Figure \ref{fig:anova1_noiseXmic_boxplot}.  The conditions in which there is no noise present differ noticeably from those with noise; this can visually be seen even in the condition in which the speech was recorded at the ear. This is likely the root of the sphericity violations.
%DIF >  
%DIF >  %\begin{wrapfigure}{L}{1\textwidth}
%DIF >  \begin{figure}[h!]
%DIF >  <<boxplot_noiseXspkr, echo=FALSE, results='asis'>>=
%DIF >  boxplot(wer~noise_type*speaker_gender,col=c("white","lightgray"),mydata, names=c("Female\nNo Noise","Female\nBus","Female\nCafe","Female\nPedestrian","Female\nStreet","Female\nFactory","Male\nNo Noise","Male\nBus","Male\nCafe","Male\nPedestrian","Male\nStreet","Male\nFactory"),ylab="WER",las=2)
%DIF >  @
%DIF >  \caption{Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the speaker gender. WER is the variable on the y-axis, and noise type by speaker gender is on the x-axis.}
%DIF >  \label{fig:anova1_noiseXspkr_boxplot}
%DIF >  \end{figure}
%DIF >  
%DIF >  \begin{figure}[h!]%{L}{\textwidth}
%DIF >  <<boxplot_noiseXmic, echo=FALSE, results='asis'>>=
%DIF >  boxplot(wer~noise_type*mic_location,col=c("white","lightgray"),mydata, names=c("Ear\nNo Noise","Ear\nBus","Ear\nCafe","Ear\nPedestrian","Ear\nStreet","Ear\nFactory","Mouth\nNo Noise","Mouth\nBus","Mouth\nCafe","Mouth\nPedestrian","Mouth\nStreet","Mouth\nFactory"),ylab="WER",las=2)
%DIF >  @
%DIF >  \caption{Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location. WER is the variable on the y-axis, and noise type by mic location is on the x-axis.}
%DIF >  \label{fig:anova1_noiseXmic_boxplot}
%DIF >  \end{figure}
\DIFaddbegin 


\DIFadd{The factor of speaker gender was split, and two two-way ANOVA were calculated for each level of speaker gender (female, male) with factors of }\DIFaddend noise type \DIFaddbegin \DIFadd{and microphone location.  Both by-subjects and by-items ANOVA, for both female and male speakers, found the noise type }\DIFaddend \textbf{x} \DIFdelbegin \DIFdel{mic location in Figure \ref{fig:anova1_noiseXmic_boxplot}.  The conditions in which there is no noise present differs noticeably from those with noise ; this can visually be seen even in the condition in which the speech was recorded at the ear.  This is likely the root of the sphericity violations}\DIFdelend \DIFaddbegin \DIFadd{microphone location interaction significant and both main effects of noise type and microphone location significant, with $p<0.005$ in all conditions.  The simple effects of microphone location were calculated, for each combination of speaker gender and noise type, and all were found to be significant, with $p<0.005$ in all conditions.  The simple effects of noise type were calculated.  For all female gender conditions (mic at the ear and at the mouth), and for the male gender condition with the microphone at the mouth, all effects are significant with $p<0.005$.  However, for the male gender condition with the microphone at the ear, there was no significant effect of noise, including the `no noise' condition ($F_{subj}(5,115)=2.01, p>0.05$; $F_{item}(5,355)=2.25, p<0.05$)}\DIFaddend .

%DIF < \begin{wrapfigure}{L}{1\textwidth}
\DIFdelbegin %DIFDELCMD < \begin{figure}[h!]
%DIFDELCMD < 

%DIFDELCMD < \includegraphics[width=\maxwidth]{figure/boxplot_noiseXspkr-1} 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the speaker gender. WER is the variable on the y-axis, and noise type by speaker gender is on the x-axis.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:anova1_noiseXspkr_boxplot}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < \begin{figure}[h!]%%%
%DIF < {L}{\textwidth}
%DIFDELCMD < 

%DIFDELCMD < \includegraphics[width=\maxwidth]{figure/boxplot_noiseXmic-1} 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location. WER is the variable on the y-axis, and noise type by mic location is on the x-axis.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:anova1_noiseXmic_boxplot}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Since there is a statistical difference in the main effect of noise, and since the \DIFdelbegin \DIFdel{``no noise'' condition is very apparently different }\DIFdelend \DIFaddbegin \DIFadd{`no noise' condition is expected to contain different WERs }\DIFaddend from the other noise conditions, another ANOVA was calculated with the \DIFdelbegin \DIFdel{``no noise'' }\DIFdelend \DIFaddbegin \DIFadd{`no noise' }\DIFaddend level of noise type removed.  This was to test for statistical difference only in noise conditions containing actual noise (and any resulting interaction). This modified ANOVA is a \DIFdelbegin \DIFdel{two by two by five }\DIFdelend \DIFaddbegin \DIFadd{2 }\textbf{\DIFadd{x}} \DIFadd{2 }\textbf{\DIFadd{x}} \DIFadd{5 }\DIFaddend design, with the noise type factor only having 5 levels (with the \DIFdelbegin \DIFdel{``no noise'' }\DIFdelend \DIFaddbegin \DIFadd{`no noise' }\DIFaddend condition removed from the noise type factor).  \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The results are similar, with no significant three-way interaction, and - as before - two significant two-way interactions of speaker gender }\textbf{\DIFdel{x}} %DIFAUXCMD
\DIFdel{mic location and noise-type }\textbf{\DIFdel{x}} %DIFAUXCMD
\DIFdel{mic location (cf. Tables \ref{tab:anova2_by_subj} and \ref{tab:anova2_by_item}, respectively).
There are }\DIFdelend \DIFaddbegin \DIFadd{As before, the values below include Greenhouse-Geisser corrections for Sphericity violations for noise type, but this resulted in no change to any significant values.
}

\DIFadd{There are significant }\DIFaddend main effects of noise type and mic location \DIFaddbegin \DIFadd{(cf. Table \ref{tab:anova2})}\DIFaddend .  The main effect of speaker gender \DIFdelbegin \DIFdel{differs from the first ANOVA in that it }\DIFdelend is only significant in the by-subjects ANOVA, but is not significant in the by-items ANOVAs.  \DIFaddbegin \DIFadd{There are two significant two-way interactions of speaker gender }\textbf{\DIFadd{x}} \DIFadd{mic location and noise type }\textbf{\DIFadd{x}} \DIFadd{mic location, but no significant interaction of speaker gender }\textbf{\DIFadd{x}} \DIFadd{noise type, and no significant three-way interaction.  
}\DIFaddend 

% latex table generated in R 3.4.1 by xtable 1.8-2 package
%DIF <  Sun Jul  9 18:16:32 2017
%DIF >  Wed Jul 26 23:37:04 2017
\begin{table}[ht]
\centering
\DIFdelbeginFL %DIFDELCMD < \begin{tabular}{lrrrrl}
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \begin{tabular}{llrrrrl}
  \DIFaddendFL \hline
\DIFaddbeginFL \DIFaddFL{ANOVA }& \DIFaddendFL Effect & DFn & DFd & F & p & \DIFdelbeginFL \DIFdelFL{p$<$.05 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{* }\DIFaddendFL \\ 
  \hline
\DIFaddbeginFL \DIFaddFL{S }& \DIFaddendFL speaker\_gender & \DIFdelbeginFL \DIFdelFL{1.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{23.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{23 }\DIFaddendFL & 5.26 & 0.03 & * \\ 
  \DIFdelbeginFL \DIFdelFL{noise}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }& \DIFaddFL{speaker}\DIFaddendFL \\SUBSCRIPTNB{\DIFdelbeginFL \DIFdelFL{t}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{g}\DIFaddendFL }\DIFdelbeginFL \DIFdelFL{ype }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{ender }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{4.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{92.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{71 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{5.95 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{3.46 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.07 }\DIFaddendFL &  \DIFdelbeginFL \DIFdelFL{* }\DIFdelendFL \\ 
  \DIFdelbeginFL \DIFdelFL{mic}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{S }& \DIFaddFL{noise}\DIFaddendFL \\SUBSCRIPTNB{\DIFdelbeginFL \DIFdelFL{l}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{t}\DIFaddendFL }\DIFdelbeginFL \DIFdelFL{ocation }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{ype }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{1.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{4 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{23.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{92 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{215.08 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{5.95 }\DIFaddendFL & 0.00 & * \\ 
  \DIFdelbeginFL \DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }& \DIFaddendFL noise\_type & \DIFdelbeginFL \DIFdelFL{4.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{4 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{92.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{284 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.60 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{6.92 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.66 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.00 }\DIFaddendFL & \DIFaddbeginFL \DIFaddFL{* }\DIFaddendFL \\ 
  \DIFdelbeginFL \DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{S }& \DIFaddendFL mic\_location & \DIFdelbeginFL \DIFdelFL{1.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{23.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{23 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{16.68 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{215.08 }\DIFaddendFL & 0.00 & * \\ 
  \DIFdelbeginFL \DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype:}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }& \DIFaddendFL mic\_location & \DIFdelbeginFL \DIFdelFL{4.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{92.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{71 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{6.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{515.73 }\DIFaddendFL & 0.00 & * \\ 
  \DIFaddbeginFL \DIFaddFL{S }& \DIFaddendFL speaker\_gender:noise\_type \DIFdelbeginFL \DIFdelFL{:mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }\DIFdelendFL & \DIFdelbeginFL \DIFdelFL{4.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{4 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{92.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{92 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{1.14 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.60 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.34 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.66 }\DIFaddendFL &  \\ 
  \DIFdelbeginFL %DIFDELCMD < \hline
%DIFDELCMD < \end{tabular}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{ANOVA for by-subjects analysis of the three-factor, within-subjects experiment. The ``no noise'' condition was removed from the noise type factor, resulting in a 2x2x5 design.}} 
%DIFAUXCMD
%DIFDELCMD < \label{tab:anova2_by_subj}
%DIFDELCMD < \end{table}
%DIFDELCMD < %%%
%DIF <  latex table generated in R 3.4.1 by xtable 1.8-2 package
%DIF <  Sun Jul  9 18:16:32 2017
%DIFDELCMD < \begin{table}[ht]
%DIFDELCMD < \centering
%DIFDELCMD < \begin{tabular}{lrrrrl}
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{Effect }%DIFDELCMD < & %%%
\DIFdelFL{DFn }%DIFDELCMD < & %%%
\DIFdelFL{DFd }%DIFDELCMD < & %%%
\DIFdelFL{F }%DIFDELCMD < & %%%
\DIFdelFL{p }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{p$<$.05 }%DIFDELCMD < \\ 
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelendFL speaker\_gender\DIFdelbeginFL %DIFDELCMD < & %%%
\DIFdelFL{1.00 }%DIFDELCMD < & %%%
\DIFdelFL{71.00 }%DIFDELCMD < & %%%
\DIFdelFL{3.46 }%DIFDELCMD < & %%%
\DIFdelFL{0.07 }%DIFDELCMD < &  \\ 
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \DIFaddFL{:}\DIFaddendFL noise\_type & \DIFdelbeginFL \DIFdelFL{4.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{4 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{284.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{284 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{6.92 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.55 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.70 }\DIFaddendFL &  \DIFdelbeginFL \DIFdelFL{* }\DIFdelendFL \\ 
  \DIFdelbeginFL \DIFdelFL{mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }%DIFDELCMD < & %%%
\DIFdelFL{1.00 }%DIFDELCMD < & %%%
\DIFdelFL{71.00 }%DIFDELCMD < & %%%
\DIFdelFL{515.73 }%DIFDELCMD < & %%%
\DIFdelFL{0.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{S }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{* }%DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelendFL speaker\_gender:\DIFdelbeginFL \DIFdelFL{noise}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{mic}\DIFaddendFL \\SUBSCRIPTNB{\DIFdelbeginFL \DIFdelFL{t}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{l}\DIFaddendFL }\DIFdelbeginFL \DIFdelFL{ype }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{ocation }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{4.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{284.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{23 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.55 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{16.68 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.70 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.00 }\DIFaddendFL & \DIFaddbeginFL \DIFaddFL{* }\DIFaddendFL \\ 
  \DIFaddbeginFL \DIFaddFL{I }& \DIFaddendFL speaker\_gender:mic\_location & \DIFdelbeginFL \DIFdelFL{1.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{71.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{71 }\DIFaddendFL & 20.62 & 0.00 & * \\ 
  \DIFaddbeginFL \DIFaddFL{S }& \DIFaddendFL noise\_type:mic\_location & \DIFdelbeginFL \DIFdelFL{4.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{4 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{284.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{92 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{7.14 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{6.00 }\DIFaddendFL & 0.00 & * \\ 
  \DIFdelbeginFL \DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }& \DIFaddendFL noise\_type:mic\_location & \DIFdelbeginFL \DIFdelFL{4.00 }%DIFDELCMD < & %%%
\DIFdelFL{284.00 }%DIFDELCMD < & %%%
\DIFdelFL{1.35 }%DIFDELCMD < & %%%
\DIFdelFL{0.25 }%DIFDELCMD < &  \\ 
%DIFDELCMD <    \hline
%DIFDELCMD < \end{tabular}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{ANOVA for by-items analysis of the three-factor, within-subjects experiment. The ``no noise'' condition was removed from the noise type factor, resulting in a 2x2x5 design.}} 
%DIFAUXCMD
%DIFDELCMD < \label{tab:anova2_by_item}
%DIFDELCMD < \end{table}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelFL{Again, Mauchley's Test for Sphericity was conducted, resulting only in a significant sphericity violation in the by-subjects ANOVA for noise (cf. Tables \ref{tab:anova2_subj_sph_test} and \ref{tab:anova2_item_sph_test}).  This was corrected with a Greenhouse-Geisser test, which resulted, again, in no changes to the determinations of statistical significance indicated in the ANOVAs in Tables \ref{tab:anova2_by_subj} and \ref{tab:anova2_by_item}.
%DIF <  latex table generated in R 3.4.1 by xtable 1.8-2 package
%DIF <  Sun Jul  9 18:16:32 2017
}%DIFDELCMD < \begin{table}[ht]
%DIFDELCMD < \centering
%DIFDELCMD < \begin{tabular}{lrrl}
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{Effect }%DIFDELCMD < & %%%
\DIFdelFL{W }%DIFDELCMD < & %%%
\DIFdelFL{p }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{5 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{p$<$.05 }%DIFDELCMD < \\ 
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{355 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.26 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{71.03 }\DIFaddendFL & 0.00 & * \\ 
  \DIFdelbeginFL \DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }%DIFDELCMD < & %%%
\DIFdelFL{0.76 }%DIFDELCMD < & %%%
\DIFdelFL{0.74 }%DIFDELCMD < &  \\ 
%DIFDELCMD <   %%%
\DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype:mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }%DIFDELCMD < & %%%
\DIFdelFL{0.58 }%DIFDELCMD < & %%%
\DIFdelFL{0.23 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{S }\DIFaddendFL & \DIFdelbeginFL %DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelendFL speaker\_gender:noise\_type:mic\_location & \DIFdelbeginFL \DIFdelFL{0.82 }%DIFDELCMD < & %%%
\DIFdelFL{0.89 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{4 }\DIFaddendFL & \DIFdelbeginFL %DIFDELCMD < \\ 
%DIFDELCMD <    \hline
%DIFDELCMD < \end{tabular}
%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{Sphericity test for the by-subjects ANOVA with the ``no noise'' condition removed.}} 
%DIFAUXCMD
%DIFDELCMD < \label{tab:anova2_subj_sph_test}
%DIFDELCMD < \end{table}
%DIFDELCMD < %%%
%DIF <  latex table generated in R 3.4.1 by xtable 1.8-2 package
%DIF <  Sun Jul  9 18:16:32 2017
%DIFDELCMD < \begin{table}[ht]
%DIFDELCMD < \centering
%DIFDELCMD < \begin{tabular}{lrrl}
%DIFDELCMD <   \hline
%DIFDELCMD < %%%
\DIFdelFL{Effect }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{92 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{W }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1.14 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{p }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.34 }\DIFaddendFL &  \DIFdelbeginFL \DIFdelFL{p$<$.05 }\DIFdelendFL \\ 
  \DIFdelbeginFL %DIFDELCMD < \hline
%DIFDELCMD < %%%
\DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype }%DIFDELCMD < & %%%
\DIFdelFL{0.87 }%DIFDELCMD < & %%%
\DIFdelFL{0.36 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{I }\DIFaddendFL & \DIFdelbeginFL %DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelendFL speaker\_gender:noise\_type\DIFdelbeginFL %DIFDELCMD < & %%%
\DIFdelFL{0.88 }%DIFDELCMD < & %%%
\DIFdelFL{0.43 }%DIFDELCMD < &  \\ 
%DIFDELCMD <   %%%
\DIFdelFL{noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype}\DIFdelendFL :mic\_location & \DIFdelbeginFL \DIFdelFL{0.93 }%DIFDELCMD < & %%%
\DIFdelFL{0.86 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{4 }\DIFaddendFL & \DIFdelbeginFL %DIFDELCMD < \\ 
%DIFDELCMD <   %%%
\DIFdelFL{speaker}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{g}%DIFDELCMD < }%%%
\DIFdelFL{ender:noise}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{t}%DIFDELCMD < }%%%
\DIFdelFL{ype:mic}%DIFDELCMD < \\%%%
\DIFdelFL{SUBSCRIPTNB}%DIFDELCMD < {%%%
\DIFdelFL{l}%DIFDELCMD < }%%%
\DIFdelFL{ocation }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{284 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{0.98 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{1.35 }\DIFaddendFL & \DIFdelbeginFL \DIFdelFL{1.00 }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{0.25 }\DIFaddendFL &  \\ 
   \hline
\end{tabular}
\caption{\DIFdelbeginFL \DIFdelFL{Sphericity test }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{ANOVA }\DIFaddendFL for \DIFdelbeginFL \DIFdelFL{the }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{by-subjects (S) and }\DIFaddendFL by-items \DIFdelbeginFL \DIFdelFL{ANOVA with }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{(I) analysis of }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{``}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{three-factor, within-subjects experiment; the level of `}\DIFaddendFL no noise\DIFdelbeginFL \DIFdelFL{'' condition }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{' has been }\DIFaddendFL removed \DIFaddbeginFL \DIFaddFL{from the noise type factor. `ANOVA' indicates by-subject (S) or by-item (I) analysis, `Effect' lists the factor(s) tested, `DFn' and `DFd' refer to the degrees of freedom numerator and denominator, `F' is the F-score, `p' is the p-value, and `*' marks a significant p-value}\DIFaddendFL .} 
\DIFdelbeginFL %DIFDELCMD < \label{tab:anova2_item_sph_test}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \label{tab:anova2}
\DIFaddendFL \end{table}


%DIF < (cf. Tables \ref{tab:anova2_subj_sph_corr} and \ref{tab:anova2_item_sph_corr}).
\DIFaddbegin \DIFadd{As before, the factor of gender was split, and two two-way ANOVAs were run for the factors of noise type and microphone location, at each level of speaker gender (female, male).  The ANOVA with the male data has significant main effects of noise and microphone location, with $p<0.005$ in all conditions, but there was no significant interaction of noise type and microphone location ($F_{subj}(4,92)=1.54, p>0.1$; $F_{item}(4,284)=2.11, p>0.05$).  The ANOVA for the female data contained a significant main effect of microphone location ($F_{subj}(1,23)=107.31, p<0.001$; $F_{item}(1,71)=130.26, p<0.001$) and a significant interaction of microphone location }\textbf{\DIFadd{x}} \DIFadd{noise type ($F_{subj}(4,92)=5.2, p<0.005$; $F_{item}(4,284)=3.51, p<0.001$), but no main effect of noise type ($F_{subj}(4,92)=2.63, p>0.05$; $F_{item}(4,284)=2.66, p<0.05$).
}\DIFaddend 

%DIF <  # <<ANOVA2_sphericity_corr, echo=FALSE, results='asis'>>=
%DIF <  # print(xtable(by_subj_ez_anova2$`Sphericity Corrections`, label="tab:anova2_subj_sph_corr", caption="Sphericity corrections for the by-subjects ANOVA with the ``no noise'' condition removed."), include.rownames=FALSE)
%DIF <  # print(xtable(by_item_ez_anova2$`Sphericity Corrections`, label="tab:anova2_item_sph_corr", caption="Sphericity corrections for the by-items ANOVA with the ``no noise'' condition removed."), include.rownames=FALSE)
%DIF <  # @
\DIFaddbegin \DIFadd{The simple effects for microphone location were calculated, and as before, all were significant with $p<0.005$.  The simple effects of noise were calculated, and there were significant effects of noise type for both levels of gender with the microphone at the mouth, $p<0.005$.  However, there were no significant effects of noise (again, this test excludes the `no noise' condition) for either gender when the microphone was placed in the ear (female: ($F_{subj}<1; F_{item}<1$); male: ($F_{subj}(4,92)=1.06, p>0.1$; $F_{item}(4,284)=1.10, p>0.1$)).
}


%DIF >  
%DIF >  Again, Mauchley's Test for Sphericity was conducted, resulting only in a significant sphericity violation in the by-subjects ANOVA for noise (cf. Tables \ref{tab:anova2_subj_sph_test} and \ref{tab:anova2_item_sph_test}).  This was corrected with a Greenhouse-Geisser test, which resulted, again, in no changes to the determinations of statistical significance indicated in the ANOVAs in Tables \ref{tab:anova2_by_subj} and \ref{tab:anova2_by_item}.
%DIF >  <<ANOVA2_sphericity_test, echo=FALSE, results='asis'>>=
%DIF >  print(xtable(by_subj_ez_anova2$`Mauchly's Test for Sphericity`, label="tab:anova2_subj_sph_test", caption="Sphericity test for the by-subjects ANOVA with the `no noise' condition removed."), include.rownames=FALSE)
%DIF >  print(xtable(by_item_ez_anova2$`Mauchly's Test for Sphericity`, label="tab:anova2_item_sph_test", caption="Sphericity test for the by-items ANOVA with the `no noise' condition removed."), include.rownames=FALSE)
%DIF >  by_subj_ez_anova2$`Sphericity Corrections`$HFe <- NULL
%DIF >  by_item_ez_anova2$`Sphericity Corrections`$HFe <- NULL
%DIF >  by_subj_ez_anova2$`Sphericity Corrections`$`p[HF]` <- NULL
%DIF >  by_item_ez_anova2$`Sphericity Corrections`$`p[HF]` <- NULL
%DIF >  by_subj_ez_anova2$`Sphericity Corrections`$`p[HF]<.05` <- NULL
%DIF >  by_item_ez_anova2$`Sphericity Corrections`$`p[HF]<.05` <- NULL
%DIF >  @
%DIF >  
%DIF >  
%DIF >  %(cf. Tables \ref{tab:anova2_subj_sph_corr} and \ref{tab:anova2_item_sph_corr}).
%DIF >  
%DIF >  % # <<ANOVA2_sphericity_corr, echo=FALSE, results='asis'>>=
%DIF >  % # print(xtable(by_subj_ez_anova2$`Sphericity Corrections`, label="tab:anova2_subj_sph_corr", caption="Sphericity corrections for the by-subjects ANOVA with the ``no noise'' condition removed."), include.rownames=FALSE)
%DIF >  % # print(xtable(by_item_ez_anova2$`Sphericity Corrections`, label="tab:anova2_item_sph_corr", caption="Sphericity corrections for the by-items ANOVA with the ``no noise'' condition removed."), include.rownames=FALSE)
%DIF >  % # @
\DIFaddend 




\section{Discussion}\DIFaddbegin \label{chap3:discussion}
\DIFaddend 

% Points to hit here:
% X- Ear speech is more easily recognized than noisy speech from the mouth (print bar graph of mic loc simple effects)
% X- There is still a main effect of noise even when 'no noise' is removed, print bar graph w/ no 'no noise' and discuss simple effects
% X- Touch on 'near-significance' of speaker gender, how it isn't as near significant as it appears - likely an SNR effect (print graph).
% - Discuss noiseXmic interation data, re-print graph
% X-- Mention differences in mouth noise performance which come out in this interaction, medians ranging from approximately 60% (factory) to near 90% (bus); reference noise spectograms, give possible explanation
% X-- Non-noisy mouth speech is still the most easily recognized - median of zero - but non-noisy ear-speech does well for itself with a median nearing 15% WER.
% X-- There still appears to be a noticeable difference between ear-no noise and ear-noisy speech (some noise must get through), but no noticeable difference between noises within ear-speech category; nevertheless, median noisy ear-speech recognition appears to hover near 30% WER.
% X-- Comment on how this is evidence for ''predictable'' noise that is (more or less) non-varying between noise types.
% X- Look at longitudinal effects
% -- Surveys - add in later
% X- Move on to limitations
% -- Mention not all sound files had normalized amplitude - some were quite louder than others, which may be an explanation for some of the observed variability.
% -- Mention the computer screen distraction.


The primary hypotheses from Chapter \DIFdelbegin \DIFdel{2}\DIFdelend \ref{chapter2} included (a) that the signal recorded from the ear, pre-emphasized, filtered, and pre-emphasized again, would be intelligible by human listeners, and (b) that it would be more intelligible than speech with a noisy background.  The results in Section \DIFdelbegin \DIFdel{\ref{ch4:results} }\DIFdelend \DIFaddbegin \DIFadd{\ref{chap3:results} }\DIFaddend above show a statistical difference between the WERs of the sentence transcriptions of the speech recorded \DIFdelbegin \DIFdel{from }\DIFdelend \DIFaddbegin \DIFadd{in every simple effect of microphone location in every condition, ie. recognition of speech recorded form }\DIFaddend the ear canal \DIFdelbegin \DIFdel{and the speech recorded in front of the mouth }\DIFdelend \DIFaddbegin \DIFadd{is statistically different than the recognition of speech recorded at the mouth in the tested conditions}\DIFaddend .  This can be seen more clearly in the \DIFdelbegin \DIFdel{graph of the simple effects of microphone location, in Figure \ref{fig:mic_loc_simple}.  The }\DIFdelend \DIFaddbegin \DIFadd{graphs containing all levels of data, Figures \ref{fig:female-split} and \ref{fig:male-split}.  The noisy (non-clean) }\DIFaddend speech recorded at the ear has a significantly lower transcription \DIFdelbegin \DIFdel{word error rate than the }\DIFdelend \DIFaddbegin \DIFadd{WER than the noisy }\DIFaddend speech recorded at the mouth, \DIFdelbegin \DIFdel{collapsing }\DIFdelend \DIFaddbegin \DIFadd{splitting }\DIFaddend over all noise conditions\DIFdelbegin \DIFdel{(this holds with or without clean speech). These primary hypotheses seem to have been validated.  }%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{A statistical interaction of speaker gender }\textbf{\DIFdel{x}} %DIFAUXCMD
\DIFdel{mic location is found.  
Looking at a boxplot of this interaction in Figure \ref{fig:spkr-genXmic-loc}, it is apparent that the two genders have different effects on microphone location .  Based on this plot, it would appear that the female's ear-recorded speech offers less intelligibility benefit over the speech recorded at the mouth, while the male voice has more of a benefit.  This interaction seems to exist primarily because listeners are able to more accurately recognize the speech in noise when spoken by the female, rather than the male.
%DIF < 
}%DIFDELCMD < \begin{wrapfigure}{R}{0.5\textwidth}
%DIFDELCMD < 

%DIFDELCMD < \includegraphics[width=\maxwidth]{figure/Mic-location_simple-1} 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Simple effects of Microphone Location.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:mic_loc_simple}
%DIFDELCMD < \end{wrapfigure}
%DIFDELCMD < %%%
%DIF < 
\DIFdel{However, while it is possible that gender itself is causing this effect, it should also noted that - in the instance of these two particular speakers - gender is confounded with level SNR.  The average female speaker's SNR in the speech recorded at mouth in noisy conditions was higher than the male speaker's SNR}\footnote{\textbf{\DIFdel{The only noise condition was 80 dB; the female speaker averaged less than +1 dB SNR and the male speaker averaged over +8 dB SNR, using the SNR calculator described in Section }\textit{%DIFDELCMD < [%%%
\DIFdel{Chapter 2: Ear Recorded Speech: Discussion}%DIFDELCMD < ]%%%
}%DIFAUXCMD
\DIFdel{\ref{chap2:discussion} and found in Appendix E\ref{appendixE}}}%DIFAUXCMD
}%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdelend .  \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{This likely contributed to the observed statistical difference.  If the SNR is high (as it was for the female's speech), the human auditory system can utilize the methods it has to ``release the masking'' in an effective manner.  Listeners therefore perform better on speech with richer frequency information (even if there is a bit of noise), than more distorted, ``muffled'' speech.  When the reverse is true and the speech in noise has a lower SNR (as it was in the male's speech), it cannot as easily be ``released from the masking'' by the auditory system.  Thus, speech that is slightly distorted - but has clearer harmonic and formant information - can be more easily understood.  The likelihood of the statistical difference seen being caused by SNR level also makes sense, as the major difference observed in Figure \ref{fig:spkr-genXmic-loc} occurs between the two genders' speech that is recorded at the mouth (in noisy conditions).  Unfortunately, this is only speculation, as with the given data, gender is confounded with SNR.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{There is also a main effect of noise.  It is obvious that the speech recorded with no background noise (particularly at the mouth) would be easier to transcribe and recognize than the speech recorded with background noise.  However, when the level of `no noise' within the noise-type factor is removed, the statistical difference within this condition remains.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{A closer look at the main effect of noise-type, excluding the level of `no noise', can be seen in Figure \ref{fig:noise-type_non-no-noise_main}.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The difference here is not nearly as stark as that seen within the microphone location distinction in Figure \ref{fig:mic_loc_simple}.  Even still, there is a observable difference, particularly between the bus background noise (with the highest relative WER) and the factory background noise (with the lowest relative WER).
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Referring back to Figure \ref{fig:bkgrnd-noises}, containing the spectograms of the background noises in Section \ref{bkgrnd:speech_in_noise}, it appears that the bus noise (cf. Figure \ref{fig:fac-bkgrnd}) contains bands in the frequency spectrum that contain higher amplitude.
This may adversely affect a person's ability to parse the harmonics and/or formants from the desired speech. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Again referring to Figure \ref{fig:bkgrnd-noises}, it is unclear, however, why the cafe background noise, among the other noises, has a relatively lower WER, since it also contains many more prominent bands of frequency from speaker babble. 
Similarly, it is difficult to observe much difference between the factory background noise and the pedestrian background noise, despite the pedestrian noise having a higher upper bound.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{To see if any more apparent differences can be found, Figure \ref{fig:noiseXmic2} shows the noise type factor split between the two levels of mic location, displaying what was originally seen above in Figure \ref{fig:anova1_noiseXmic_boxplot}.  %DIF < 
}%DIFDELCMD < \begin{wrapfigure}{l}{0.5\textwidth}
%DIFDELCMD < 

%DIFDELCMD < \includegraphics[width=\maxwidth]{figure/spkr-gender_mic-location_interaction-1} 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Interaction between speaker gender and microphone location.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:spkr-genXmic-loc}
%DIFDELCMD < \end{wrapfigure}
%DIFDELCMD < %%%
%DIF < 
\DIFdel{When only looking at the noise as it occurs in the mouth-recorded condition, the differences between the levels of noise become much more stark.  In particular, the difference between the bus noise (high WER) and factory noise (relatively lower WER) greatly expands. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{This distinction reaches as low as a median of approximately 60}%DIFDELCMD < \% %%%
\DIFdel{WER for the factory background noise conditions, and a median of nearly 90}%DIFDELCMD < \% %%%
\DIFdel{WER for the bus background noise.  The variance within noise conditions also differs, with the bus noise containing less variance (upper quartile 100}%DIFDELCMD < \% %%%
\DIFdel{WER, lower quartile ~68}%DIFDELCMD < \% %%%
\DIFdel{WER) than the factory noise condition (upper quartile ~90}%DIFDELCMD < \% %%%
\DIFdel{WER, lower quartile ~30}%DIFDELCMD < \% %%%
\DIFdel{WER).  The remaining noise conditions are more similar to one another, and fall in between the two, with median WERs hovering near 80}%DIFDELCMD < \%%%%
\DIFdel{.  The bus noise, as seen in Figure \ref{fig:bckgrnd-noises}, seems to contain very prominent frequency bands within the frequency range that is important for speech; it is possible that this plays a role in its noticeably higher WER.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The differences between the transcription WERs of the cafe, pedestrian, and street noises have expanded slightly, but still hover fairly close together in between the bus and factory performances. No explanation is proposed for the reason for these apparent divergences}\DIFdelend \DIFaddbegin \DIFadd{There is a significant simple effect of microphone location at the level of `no noise', as well, the effect goes in the opposite direction, as the clean mouth-recorded speech is easier to understand than the ear-recorded speech.  These primary hypotheses seem to have been validated}\DIFaddend .

Out of all condition combinations in \DIFdelbegin \DIFdel{Figure \ref{fig:noiseXmic2}, the WER-front-runner }\DIFdelend \DIFaddbegin \DIFadd{Figures \ref{fig:female-split} and \ref{fig:male-split}, the WER front-runner }\DIFaddend is quite clearly the speech recorded at the mouth with no background noise.  There was never any doubt that this would be the case, as speech with \DIFdelbegin \DIFdel{relatively little background noise }\DIFdelend \DIFaddbegin \DIFadd{a }\textit{\DIFadd{relatively}} \DIFadd{high SNR }\DIFaddend is the sort of speech \DIFdelbegin \DIFdel{from which learners acquire their language model, and }\DIFdelend whereby most communication occurs \DIFaddbegin \DIFadd{(ie. not with consistent 80 dB background noise, or from significantly low-passed speech)}\DIFaddend .  The median WER is, unsurprisingly, 0\%, though there is some variance from perfect perception; some errors do occasionally occur. %DIF < 
\DIFdelbegin %DIFDELCMD < \begin{wrapfigure}{l}{0.5\textwidth}
%DIFDELCMD < 

%DIFDELCMD < \includegraphics[width=\maxwidth]{figure/Noise-type_simple-1} 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Simple effects of Noise Type, excluding the level of `no noise'.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:noise-type_non-no-noise_main}
%DIFDELCMD < \end{wrapfigure}
%DIFDELCMD < %%%
%DIF < 
\DIFdelend The ear-recorded speech in a clean environment manages to also achieve a respectable transcription WER median of approximately 15\% \DIFaddbegin \DIFadd{for both the male- and female-spoken stimuli}\DIFaddend , with a lower quartile boundary at 0\% WER and an upper quartile boundary of \DIFaddbegin \DIFadd{approximately }\DIFaddend 35\% WER.


\DIFaddbegin \begin{figure}[h!]

\includegraphics[width=\maxwidth]{figure/All-Factors-Female-1} 

\caption{\DIFaddFL{Comparison of ear-recorded and mouth-recorded speech at every level of noise type, for the female-spoken stimuli.  White bars signify ear-recorded speech, and grey bars signify mouth-recorded speech.}}\label{fig:female-split}
\end{figure}

\begin{figure}[h!]

\includegraphics[width=\maxwidth]{figure/All-Factors-Male-1} 

\caption{\DIFaddFL{Comparison of ear-recorded and mouth-recorded speech at every level of noise type, for the male-spoken stimuli.  White bars signify ear-recorded speech, and grey bars signify mouth-recorded speech.}}\label{fig:male-split}
\end{figure}

%DIF >  0Discuss Mic vs Ear results basic

\DIFaddend Despite being more similar than their mouth recorded counterparts, there is still a noticeable difference between the ear recorded speech with no noise, and ear recorded speech in noisy conditions.  Most noise conditions recorded from the ear achieve a lower quartile boundary near 0\%, but the upper quartile boundary for most nears 60\% WER.  The median WER for noise conditions generally falls at or slightly below 30\%. 

Even though there is a higher WER than the ear-recorded no-noise condition, the ear-recorded speech in noise is quite consistent across noise categories.  This is very different from the mouth-recorded speech in noise, which vary considerably between noise conditions.    
\DIFaddbegin 

%DIF >  1Discuss significant interaction of noise and mic, and significant simple effect of noise for all mic locs except ear for male
\DIFadd{There was also a statistical interaction of noise type }\textbf{\DIFadd{x}} \DIFadd{microphone location, and this remained even when the `clean' condition was removed from the dataset, except the interaction disappears when only the male-spoken data observed after being split by gender.  In looking at the simple effect of noise in for the mouth-recorded level of microphone location, there is a statistical difference for both genders, both when the `clean' condition is left in the analysis as well as when it is removed.  This is both clear in Figures \ref{fig:female-split} and \ref{fig:male-split} and intuitive; different types of noise with different qualities will affect recognition in different ways.  This can be contrasted with the simple effects of noise for both genders at the ear (excluding the `clean' condition); there is no statistical difference in recognition (WER) across the levels of noise when the the speech was recorded from the ear.  }\DIFaddend This indicates that while a noise presence hampers transcription ability and increases WER in ear-recorded conditions, the varying qualities of the different background noises were dampened to the point of having a \DIFdelbegin \DIFdel{starkly }\DIFdelend \DIFaddbegin \DIFadd{significantly }\DIFaddend lesser effect than that which occurs in the mouth-recorded speech.

\DIFaddbegin \DIFadd{For the male-spoken data, this trend continued when the `clean' condition was included in the analysis.  There was no simple effect of noise, no statistical difference between any of the noise levels, when the speech was recorded at the ear.  This difference in the recognition of the female-spoken speech indicates that some noise must be present in the ear-recorded signal in order to elicit a statistical difference in the simple effects of noise when the `clean' speech was included versus when it was not.  It is worth noting that the female speaker's hair introduced a potential compromise to the seal of the earmuffs used in the experiment (cf. Appendix A\ref{appendixA}). Emphasizing what is written above, there is still no difference of noise type when the `clean' condition is removed from the analysis.
}

\DIFadd{Regarding the simple effects of noise at the mouth, as previously mentioned, there are statistical effects found for both genders, regardless of whether `clean' speech is included in the analysis.  The exact reasons why performance differs for different levels of noise is not a primary concern of this study.  The different levels were primarily included to test the above effect, }\textit{\DIFadd{whether}} \DIFadd{performance differs with noise type when speech is recorded at the mouth (it does), and whether it differs for different noise types when recorded at the ear (it does not).   Visually, in Figures \ref{fig:female-split} and \ref{fig:male-split}, the bus noise condition appears to be the most difficult background noise from which to parse speech, and the factory background noise appears to be the easiest.  Further analysis of these particular differences are not pursued.
}



%DIF >  4Talking about Speaker Gender X Mic Loc interaction, and different dB SNR.  Highlight the differences in the plots above, especially the clean condition
\DIFadd{A statistical interaction of speaker gender }\textbf{\DIFadd{x}} \DIFadd{mic location is found in both ANOVAs, with the `clean' noise condition and without.  
Looking at the boxplots in Figures \ref{fig:female-split} and \ref{fig:male-split}, it can be visualized that gender affects the recognition of speech at each of the recording locations differently (in the ear, in front of the mouth).  Based on this plot, it would appear that the female ear-recorded speech offers less intelligibility benefit over the speech recorded at the mouth, while the male voice has more of a benefit. It was previously mentioned that one possibility for the drop in performance of the female, ear-recorded speech was that there was a potential earmuff seal compromise due to the participant's hair.  
}

\DIFadd{It is also very possible that ear-recorded speech by females does not result in as high of recognition performance as that of male-speech.  One simple possibility is that there are fewer harmonics in female-speech than in male-speech below the lowpass filter cutoff.  It may also be due to differences in the anatomy of the head, although it is unlikely due to ear canal size, as the size of the outer ear canal recorded for both speakers was rather similar (male speaker: 1.4 mL; female speaker: 1.6 mL)}\footnote{\DIFadd{Cf. Appendix A\ref{appendixA}}}\DIFadd{.
%DIF > 
%DIF >  \begin{wrapfigure}{R}{0.5\textwidth}
%DIF >  <<Mic-location_simple, echo=FALSE, results='asis'>>=
%DIF >  par(mar=c(4,5,0,0))
%DIF >  boxplot(wer~mic_location,col=c("white","lightgray"),mydata, names=c("Ear-Recorded","Mouth-Recorded"),ylab="WER", cex.axis=1.5, cex.lab=2.0)
%DIF >  @
%DIF >  \caption{Simple effects of Microphone Location.}
%DIF >  \label{fig:mic_loc_simple}
%DIF >  \end{wrapfigure}
%DIF > 
}

\DIFadd{There is also a difference between both genders for speech recorded at the mouth.
However, recall that - in the instance of these two particular speakers - gender is confounded with SNR.  The average female speaker's SNR for noisy, mouth-recorded speech was higher than the male speaker's SNR for noisy, mouth-recorded speech.  As previously mentioned in Section \ref{chap3:methods:design}, the only noise level used for the stimuli in this task was 80 dB; the male speaker averaged less than +1 dB SNR and the female speaker averaged over +8 dB SNR, using the SNR calculation script mentioned previously in Section \ref{chap2:observations} and found in Appendix E\ref{appendixE}.
}

\DIFadd{This difference in SNR likely partially contributed to the observed statistical interaction.  If the SNR is high (as it was for the noisy, mouth-recorded female speech), the human auditory system can utilize the methods it has to `release the masking' in an effective manner.  The likelihood that the observed statistical difference was caused by the two speakers' differing SNRs makes sense, as the major difference in listener performance between the two speaker genders occurs within the noisy, mouth-recorded speech (cf. Figs. \ref{fig:female-split} and \ref{fig:male-split}).  Unfortunately, as gender is confounded with SNR, this can only be speculated.
}



%DIF >  A closer look at the main effect of noise-type, excluding the level of `no noise', can be seen in Figure \ref{fig:noise-type_non-no-noise_main}.
%DIF >  
%DIF >  The difference here is not nearly as stark as that seen within the microphone location distinction in Figure \ref{fig:mic_loc_simple}.  Still, there is a observable difference, particularly between the bus background noise (with the highest relative WER) and the factory background noise (with the lowest relative WER).
%DIF >  
%DIF >  Referring back to Figure \ref{fig:bkgrnd-noises}, containing the spectograms of the background noises in Section \ref{bkgrnd:speech_in_noise}, it appears that the bus noise (cf. Figure \ref{fig:fac-bkgrnd}) contains bands in the frequency spectrum that contain higher amplitude.
%DIF >  This may adversely affect a person's ability to parse the harmonics and/or formants from the desired speech. 
%DIF >  
%DIF >  Again referring to Figure \ref{fig:bkgrnd-noises}, it is unclear, however, why the caf\'{e} background noise, among the other noises, has a relatively lower WER, since it also contains many more prominent bands of frequency from speaker babble. 
%DIF >  Similarly, it is difficult to observe much difference between the factory background noise and the pedestrian background noise, despite the pedestrian noise having a higher upper bound.
%DIF >  
%DIF >  To see if any more apparent differences can be found, Figure \ref{fig:noiseXmic2} shows the noise type factor split between the two levels of mic location, displaying what was originally seen above in Figure \ref{fig:anova1_noiseXmic_boxplot}.  
%DIF > 
%DIF >  \begin{wrapfigure}{l}{0.5\textwidth}
%DIF >  <<spkr-gender_mic-location_interaction, echo=FALSE, results='asis'>>=
%DIF >  par(mar=c(8,5,0,0))
%DIF >  boxplot(wer~mic_location*speaker_gender,col=c("white","lightgray"),mydata2, names=c("Female\nEar-Rec.","Female\nMouth-Rec.","Male\nEar-Rec.","Male\nMouth-Rec."),ylab="WER", cex.axis=1.5, cex.lab=2.0, las=2)
%DIF >  @
%DIF >  \caption{Interaction between speaker gender and microphone location.}
%DIF >  \label{fig:spkr-genXmic-loc}
%DIF >  \end{wrapfigure}
%DIF > 
%DIF >  When only looking at the noise as it occurs in the mouth-recorded condition, the differences between the levels of noise become much more stark.  In particular, the difference between the bus noise (high WER) and factory noise (relatively lower WER) greatly expands. 

%DIF >  This distinction reaches as low as a median of approximately 60\% WER for the factory background noise conditions, and a median of nearly 90\% WER for the bus background noise.  The variance within noise conditions also differs, with the bus noise containing less variance (upper quartile 100\% WER, lower quartile ~68\% WER) than the factory noise condition (upper quartile ~90\% WER, lower quartile ~30\% WER).  The remaining noise conditions are more similar to one another, and fall in between the two, with median WERs hovering near 80\%.  The bus noise, as seen in Figure \ref{fig:bkgrnd-noises}, seems to contain very prominent frequency bands within the frequency range that is important for speech; it is possible that this plays a role in its noticeably higher WER.
%DIF >  
%DIF >  The differences between the transcription WERs of the caf\'{e}, pedestrian, and street noises have expanded slightly, but still hover fairly close together in between the bus and factory performances. No explanation is proposed for the reason for these apparent divergences.


%DIF >  %
%DIF >  \begin{wrapfigure}{l}{0.5\textwidth}
%DIF >  <<Noise-type_simple, echo=FALSE, results='asis'>>=
%DIF >  par(mar=c(7,5,0,0))
%DIF >  noiseless_data <- subset(mydata, noise_type!=0)
%DIF >  #par(mar=c(6,5,2,2))
%DIF >  boxplot(wer~droplevels(noise_type),col=c("white","lightgray"),noiseless_data, names=c("Bus","Cafe","Pedestrian","Street","Factory"),ylab="WER",las=2, cex.axis=1.5, cex.lab=2.0)
%DIF >  @
%DIF >  \caption{Simple effects of Noise Type, excluding the level of `no noise'.}
%DIF >  \label{fig:noise-type_non-no-noise_main}
%DIF >  \end{wrapfigure}
%DIF >  %




\DIFaddend Since the ability to recognize ear-recorded speech, even in noise, is quite consistent \DIFaddbegin \DIFadd{across different noise types}\DIFaddend , the conditions are right for the auditory system to \DIFdelbegin \DIFdel{`perceptually learn' }\DIFdelend \DIFaddbegin \DIFadd{be `trained' on }\DIFaddend the distorted ear-recorded speech, as discussed in Section \DIFdelbegin \DIFdel{\ref{chap2:background} }\DIFdelend \DIFaddbegin \DIFadd{\ref{chap3:background} }\DIFaddend and by \cite{mattys:12}, among others.  \DIFdelbegin \DIFdel{This}\DIFdelend \DIFaddbegin \DIFadd{Training}\DIFaddend , in theory, would increase the learners' recognition of the ear-recorded speech with additional exposure, further increasing the WER improvement seen with ear-recorded speech.


\DIFdelbegin %DIFDELCMD < \begin{figure}[h!]
%DIFDELCMD < 

%DIFDELCMD < \includegraphics[width=\maxwidth]{figure/boxplot_noiseXmic2-1} 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location. WER is the variable on the y-axis, and noise type by mic location is on the x-axis.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:noiseXmic2}
%DIFDELCMD < \end{figure}
%DIFDELCMD < %%%
\DIFdelend %DIF >  \begin{figure}[h!]
%DIF >  <<boxplot_noiseXmic2, echo=FALSE, results='asis'>>=
%DIF >  boxplot(wer~noise_type*mic_location,col=c("white","lightgray"),mydata, names=c("Ear\nNo Noise","Ear\nBus","Ear\nCafe","Ear\nPedestrian","Ear\nStreet","Ear\nFactory","Mouth\nNo Noise","Mouth\nBus","Mouth\nCafe","Mouth\nPedestrian","Mouth\nStreet","Mouth\nFactory"),ylab="WER",las=2)
%DIF >  @
%DIF >  \caption{Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location. WER is the variable on the y-axis, and noise type by mic location is on the x-axis.}
%DIF >  \label{fig:noiseXmic2}
%DIF >  \end{figure}

To visualize whether the performance of participants generally improves over time, scatterplots graph participant's chronological performance with mouth-recorded and ear-recorded speech over the course of the experiment in Figures \ref{fig:linear_performance_m} and \ref{fig:linear_performance_e}. 
%
\DIFdelbegin %DIFDELCMD < \begin{figure}[t]%%%
\DIFdelend \DIFaddbegin \begin{figure}[ht]\DIFaddendFL %{L}{0.5\textwidth}
\begin{subfigure}{0.47\textwidth}

\includegraphics[width=\maxwidth]{figure/line_graph_chrono_m-1} 

\caption{Scatterplot of all participants' WER values for responses to speech recorded at the mouth \textbf{and} in noise.}
\label{fig:linear_performance_m}
\end{subfigure}
%\hfill
\begin{subfigure}{0.47\textwidth}%{L}{0.5\textwidth}

\includegraphics[width=\maxwidth]{figure/line_graph_chrono-1} 

\caption{Scatterplot of all participants' WER values for responses to speech recorded at the ear.}
\label{fig:linear_performance_e}
\end{subfigure}
\caption{The x-axis is the order of the responses; eg. \DIFdelbeginFL \DIFdelFL{``}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{`}\DIFaddendFL 1 \DIFdelbeginFL \DIFdelFL{'' }\DIFdelendFL on the x-axis is the first response given by the participants.  The x-axis only corresponds to order of response, and does not indicate the specific noise type or gender of the speaker.  A line was fitted to the data using linear regression.}
\end{figure}
%
Linear regression models were fit onto the mouth-recorded data (slope=$-0.0048728$, $R^2$=$0.0175763$, $p<0.001$), and the ear-recorded data (slope=$-0.0016076$, $R^2$=$0.003146$, $p>0.05$). 

It is important to note the linear scales both \DIFaddbegin \DIFadd{for both }\DIFaddend axes, particularly the y-axis, as an explanation for why the slope values themselves are so small (ie. the y-axis range is from \textit{0.0} to \textit{1.2}).  The primary take away from both graphs and both fitted regression models in Figures \ref{fig:linear_performance_m} and \ref{fig:linear_performance_e} is that participants' recognition ability seems to improve statistically over the course of the experiment for mouth-recorded (noisy) speech, but not ear-recorded speech.  It could be possible that there are statistical gains by the noisy speech simply due to greater room for improvement, but it could also be due to the presence of high frequency speech information in the signal \DIFdelbegin \DIFdel{. The }\DIFdelend \DIFaddbegin \DIFadd{which leads to better recognition. The mouth-recorded }\DIFaddend speech beneath the noise is \DIFdelbegin \DIFdel{``normal''}\DIFdelend \DIFaddbegin \DIFadd{`normal'}\DIFaddend , containing full frequency information that the participant would be \DIFdelbegin \DIFdel{used }\DIFdelend \DIFaddbegin \DIFadd{accustomed }\DIFaddend to listening for, and if participants are able to use \DIFdelbegin \DIFdel{perceptual learning to take }\DIFdelend \DIFaddbegin \DIFadd{any training received to }\DIFaddend release the noise mask, this improvement over time is the expected effect.  However, as already seen, the overall performance on noisy, mouth recorded speech still falls well below that of ear-recorded speech, even at the \DIFaddbegin \DIFadd{chronological }\DIFaddend end of the experiment \DIFaddbegin \DIFadd{(cf. Figs. \ref{fig:linear_performance_m} and \ref{fig:linear_performance_e})}\DIFaddend .

%Of particular interest for the goals of this study, is the fact that - although very modest - there is improvement in recognition of ear-recorded speech that appears to correlate with an increase in exposure time to the ear-recorded speech.  That is, as more ear-recorded speech is heard, participants see modest gains in their ability to correctly recognize and transcribe it.  
One potential reason that there may not have been a statistical \DIFdelbegin \DIFdel{perceptual learning }\DIFdelend \DIFaddbegin \DIFadd{`training' }\DIFaddend effect for ear-recorded speech over the course of the experiment is that the participant was not given feedback or the correct answer.  It has been shown (\cite{davis:05}) that such feedback can improve and speed up the \DIFdelbegin \DIFdel{perceptual learning }\DIFdelend \DIFaddbegin \DIFadd{training }\DIFaddend process when listeners are able to correctly identify what was said.  This will be discussed further in Section \DIFdelbegin \DIFdel{\ref{ch4:follow-up-expts} }\DIFdelend \DIFaddbegin \DIFadd{\ref{chap3:follow-up-expts} }\DIFaddend below, and a follow up investigation will be conducted pertaining to this.


\DIFdelbegin %DIFDELCMD < \subsection{Follow-up Investigations}
%DIFDELCMD < \label{ch4:follow-up-expts}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \section{Follow-up Investigations}
\label{chap3:follow-up-expts}
\DIFaddend 

To \DIFdelbegin \DIFdel{expound }\DIFdelend \DIFaddbegin \DIFadd{expand }\DIFaddend on the previous study, two additional investigations were performed to give insights into possible future research directions.  The impetus for the first investigation was \cite{bird:97}, which demonstrated that fundamental frequency (F0) is a tool used by the auditory system to separate a desired source from masking noise.  This follow up \DIFdelbegin \DIFdel{proposes }\DIFdelend \DIFaddbegin \DIFadd{proposed }\DIFaddend to recombine the very clear, lower frequencies from the ear-recorded speech with the \DIFdelbegin \DIFdel{``noisy'' }\DIFdelend \DIFaddbegin \DIFadd{`noisy' }\DIFaddend upper frequencies recorded at the mouth.  

The hypothesis \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend that the auditory system \DIFdelbegin \DIFdel{will }\DIFdelend \DIFaddbegin \DIFadd{would }\DIFaddend use the clear fundamental frequency harmonic information in the lower frequencies to extract the upper harmonics out of the noise.  Accuracy \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend predicted to improve over that of the low-pass filtered, \DIFdelbegin \DIFdel{``muffled''}\DIFdelend \DIFaddbegin \DIFadd{`muffled'}\DIFaddend , ear speech (which many participants subjectively observed to be annoying and difficult to understand), as more high frequency information will be present and available for listeners' auditory systems.  This speech \DIFdelbegin \DIFdel{will sacrifice }\DIFdelend \DIFaddbegin \DIFadd{sacrificed }\DIFaddend the advantage of being completely or nearly \DIFdelbegin \DIFdel{``}\DIFdelend \DIFaddbegin \DIFadd{`}\DIFaddend noise-free\DIFdelbegin \DIFdel{''}\DIFdelend \DIFaddbegin \DIFadd{'}\DIFaddend , to sound more natural.  Additionally, since the ear-recorded speech \DIFdelbegin \DIFdel{consists }\DIFdelend \DIFaddbegin \DIFadd{consisted }\DIFaddend of very clean harmonics, it \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend hypothesized that this speech, combined with the higher frequency mouth recorded speech in the \DIFdelbegin \DIFdel{noise-free condition, will perform equal }\DIFdelend \DIFaddbegin \DIFadd{`clean' condition, would perform equally }\DIFaddend to its mouth-recorded counterpart in the \DIFdelbegin \DIFdel{noise-free }\DIFdelend \DIFaddbegin \DIFadd{`clean' }\DIFaddend condition. This \DIFdelbegin \DIFdel{will be referred to as the ``}\DIFdelend \DIFaddbegin \DIFadd{is referred to throughout as the `}\DIFaddend F0\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{' }\DIFaddend investigation.

The second investigation was based on the concept of \DIFdelbegin \DIFdel{``perceptual learning'' }\DIFdelend \DIFaddbegin \DIFadd{`training' }\DIFaddend discussed earlier.  This \DIFdelbegin \DIFdel{presumes }\DIFdelend \DIFaddbegin \DIFadd{presumed }\DIFaddend that the auditory system can learn to adapt to understand speech in a degraded signal better \DIFdelbegin \DIFdel{over time}\DIFdelend \DIFaddbegin \DIFadd{with experience}\DIFaddend .  According to \cite{mattys:12}, significant learning \DIFdelbegin \DIFdel{can occur }\DIFdelend \DIFaddbegin \DIFadd{has occurred }\DIFaddend with even a small number of training trials.  \cite{davis:05} \DIFdelbegin \DIFdel{demonstrate }\DIFdelend \DIFaddbegin \DIFadd{demonstrated }\DIFaddend that during training, successful recognition of a degraded signal\DIFdelbegin \DIFdel{will help one recognize a similar signal }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend more than unsuccessful recognition of a degraded signal\DIFdelbegin \DIFdel{.  
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{, helped participants recognize a similar signal later on.  
}\DIFaddend Based on the implications of these findings, a short story was read \DIFdelbegin \DIFdel{and was }\DIFdelend \DIFaddbegin \DIFadd{aloud and }\DIFaddend recorded from inside the ear canal\DIFdelbegin \DIFdel{for the participants in this follow up study to listen to prior to completing the experiment itself.  This will serve as a brief ``training '' }\DIFdelend \DIFaddbegin \DIFadd{; this served as training }\DIFaddend for participants in \DIFdelbegin \DIFdel{preparation for the actual task}\DIFdelend \DIFaddbegin \DIFadd{a follow-up investigation}\DIFaddend .  Since the type of distortion from the ear-recorded signal \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend regular and predictable, it \DIFdelbegin \DIFdel{is hypothesized that perceptual learning will take place, and }\DIFdelend \DIFaddbegin \DIFadd{was hypothesized that }\DIFaddend those who have listened to the training story \DIFdelbegin \DIFdel{will }\DIFdelend \DIFaddbegin \DIFadd{would }\DIFaddend perform better on ear-recorded speech than those who had not \DIFaddbegin \DIFadd{received training }\DIFaddend (ie. those in the primary study). This \DIFdelbegin \DIFdel{will be referred to as the ``perceptual learning'' or ``training'' }\DIFdelend \DIFaddbegin \DIFadd{is referred to throughout as the `training' }\DIFaddend investigation.

\DIFdelbegin %DIFDELCMD < \subsection{``F0'' Investigation Methods}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{These two investigations did not contain a full (24) number of subjects to place each participant into a counter-balanced group due to resource limitations.  Therefore no statistical analysis was conducted for either investigation.  In light of this, these could be thought of as `pilot' studies for future research.  Because there were no individual statistical results, and because the two studies were primarily of interest when compared with the results from the primary study, the methods sections (Sections \ref{F0-methods} and \ref{training-methods}) for both studies are consecutive, and any discussion of results is withheld until Section \ref{chap3:glob_discussion}.
}

%DIF >  for the participants in this follow-up investigation to listen to prior to completing the experiment itself. This speech will be modified in the same manner as the ear-recorded speech in the primary study (pre-emphasized, lowpass filtered 0 to 2.5 kHz with a 500 Hz slope, and pre-emphasized).  This recording did not combine the ear-recorded speech with the higher frequencies of the mouth recorded speech (as in the `F0' investigation above).  This brief story will serve as a training for participants in preparation for the actual task. 


\section{`F0' Investigation Methods}
\DIFaddend \label{F0-methods}

\DIFaddbegin \subsection{Stimuli}\label{F0-stimuli}

\DIFaddend The stimuli used for this investigation consisted of the \DIFdelbegin \DIFdel{exact same sentences }\DIFdelend \DIFaddbegin \DIFadd{same 80 Harvard Sentences }\DIFaddend produced by the \DIFdelbegin \DIFdel{exact same speakers }\DIFdelend \DIFaddbegin \DIFadd{same speakers in the primary study above (Section \ref{expt3})}\DIFaddend .
No modification was performed to the sentences recorded at the mouth.  For the sentences recorded at the ear, the same modifications as before (pre-emphasis, lowpass filtering \DIFdelbegin \footnote{\DIFdel{Lowpass filtered allowing 0-2500Hz, with a 500 Hz slope}}%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{from 0 to 2.5 kHz with a 500 Hz slope}\DIFaddend , and a second pre-emphasis) were performed, but afterwards, the simultaneously recorded speech from the mouth was filtered and combined with the ear-recorded speech.  The speech from the mouth was bandpass filtered between \DIFdelbegin \DIFdel{3000Hz and 8000Hz}\DIFdelend \DIFaddbegin \DIFadd{3.0 kHz and 8.0 kHz}\DIFaddend , with a 500Hz slope.  
%DIF < 
\DIFdelbegin %DIFDELCMD < \begin{wrapfigure}{r}{0.5\textwidth}
%DIFDELCMD < \centering
%DIFDELCMD <   \includegraphics[width=0.45\textwidth]{figure/combined-signal.png}
%DIFDELCMD <   %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{A spectrogram of the sentence ``A cramp is no small danger on a swim''.  The low-pass filtered ear-recorded signal was combined with the simultaneous }%DIFDELCMD < [%%%
\DIFdel{noisy}%DIFDELCMD < ] %%%
\DIFdel{mouth signal, which was bandpass filtered at a higher frequency.}}
  %DIFAUXCMD
%DIFDELCMD < \label{fig:combined-signal}
%DIFDELCMD < \end{wrapfigure}
%DIFDELCMD < %%%
%DIF < 
\DIFdelend \DIFaddbegin 

\DIFaddend This allowed for an overlap of the frequencies from the mouth-recorded speech and the lowpass filtered ear-recorded speech.  The two signals were converted to a stereo signal, and then combined into a mono signal.  This resulted in relatively clean speech below approximately 2.7 kHz, and noisy speech above approximately 2.7 kHz, as seen in Figure \ref{fig:combined-signal}.
%DIF > 
\DIFaddbegin \begin{wrapfigure}{r}{0.5\textwidth}
\centering
  \includegraphics[width=0.45\textwidth]{figure/combined-signal_labeled.png}
  \caption{\DIFadd{A spectrogram of the sentence ``A cramp is no small danger on a swim''.  The low-pass filtered ear-recorded signal (0 - 2.5 kHz with a 500 Hz slope) was combined with the simultaneous }[\DIFadd{noisy}] \DIFadd{mouth signal, which was bandpass filtered at a higher frequency.  The arrow indicates the location of overlap, above which the mouth-recorded signal is dominant, and below which the ear-recorded signal is dominant}}
  \label{fig:combined-signal}
\end{wrapfigure}
%DIF > 
\DIFaddend 

\DIFaddbegin \subsection{Participants}
\DIFaddend There were five native speakers of English with self-reported normal hearing who participated in this investigation.  
\DIFdelbegin \DIFdel{The design and }\DIFdelend \DIFaddbegin 

\subsection{Procedure}
\DIFadd{The }\DIFaddend procedure of this task was exactly the same as the initial perception experiment \DIFdelbegin \DIFdel{, save the alteration in modifications performed on the ear-recorded stimuli, described }\DIFdelend \DIFaddbegin \DIFadd{(Section \ref{chap3:methods:procedure}), with the exception of the stimulus modification, described in Section \ref{F0-stimuli} }\DIFaddend above.


\DIFdelbegin %DIFDELCMD < \subsection{``F0'' Results and Discussion}
%DIFDELCMD < \label{ch4:F0_discussion}
%DIFDELCMD < %%%
\DIFdelend %DIF >  \subsection{`F0' Results and Discussion}
%DIF >  \label{chap3:F0_discussion}

%\begin{wrapfigure}{L}{0.75\textwidth}



% \caption{Using the data from the five participants who performed the experiment using the speech in which the higher frequencies were added back in from the noisy mouth-recorded speech.  Boxplot displaying the average word error rate (WER) averaged over each participant for every noise type. WER is the variable on the y-axis, and noise type is on the x-axis.}
% \label{fig:F0_noise_boxplot}
% \end{wrapfigure}

% \begin{figure}%{L}{\textwidth}
% <<F0_boxplot_noiseXspkr, echo=FALSE, results='asis'>>=
% boxplot(wer~noise_type*speaker_gender,col=c("white","lightgray"),F0data, names=c("Female\nNo Noise","Female\nBus","Female\nCafe","Female\nPedestrian","Female\nStreet","Female\nFactory","Male\nNo Noise","Male\nBus","Male\nCafe","Male\nPedestrian","Male\nStreet","Male\nFactory"),ylab="WER",las=2)
% @
% \caption{Using the data from the five participants who performed the experiment using the speech in which the higher frequencies were added back in from the noisy mouth-recorded speech.  Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the speaker gender. WER is the variable on the y-axis, and noise type by speaker gender is on the x-axis.}
% \label{fig:F0_noiseXspkr_boxplot}
% \end{figure}

\DIFdelbegin %DIFDELCMD < \begin{figure}[h!]%%%
%DIF < {L}{\textwidth}
%DIFDELCMD < 

%DIFDELCMD < \includegraphics[width=\maxwidth]{figure/F0_boxplot_noiseXmic-1} 
%DIFDELCMD < %%%
\DIFdelend %DIF >  \begin{figure}[h!]%{L}{\textwidth}
%DIF >  <<F0_boxplot_noiseXmic, echo=FALSE, results='asis'>>=
%DIF >  par(mar=c(6,4,0,0))
%DIF >  boxplot(wer~noise_type*mic_location,col=c("white","lightgray"),F0data, names=c("Ear\nNo Noise","Ear\nBus","Ear\nCafe","Ear\nPedestrian","Ear\nStreet","Ear\nFactory","Mouth\nNo Noise","Mouth\nBus","Mouth\nCafe","Mouth\nPedestrian","Mouth\nStreet","Mouth\nFactory"),ylab="WER",las=2)
%DIF >  @
%DIF >  \caption{Using the data from the five participants who performed the task using the speech in which the higher frequencies were added back in from the noisy mouth-recorded speech.  Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location. WER is the variable on the y-axis, and noise type by mic location is on the x-axis.}
%DIF >  \label{fig:F0_noiseXmic_boxplot}
%DIF >  \end{figure}
%DIF >  
%DIF >  Since there were only five participants in this follow-up investigation, no actual statistics were run to test for significance.  The results here are to be used in a basic comparison of possible directions these altered methods may be able to take future research.
%DIF >  
%DIF >  The boxplot in Figure \ref{fig:F0_noiseXmic_boxplot} demonstrates the interaction of most interest, that of noise-type and microphone location, as identified by the ANOVAs performed on the primary experiment above.  No change was made to any of the presented mouth-recorded speech signals, so, as is observed, one would not expect there to be any change to listeners' ability to recognize these signals (outside of expected inter-speaker variation).
%DIF >  
%DIF >  It is interesting to note that the primary difference between the clean ear-recorded speech and the `noisy' ear-recorded speech appears to be the extent of the variation in the upper quartile and whiskers.  Based on what is seen here, if this follow up study were to be extended carried out in a full study, one would expect there to be a dramatic difference between the noisy speech recorded at the mouth, and the speech recorded at the ear.
%DIF >  Further discussion concerning the comparison of these results with those of the primary study and the `training' study can be found below in Section \ref{chap3:glob_discussion}.
%DIF >  %
%DIF >  \begin{wrapfigure}{L}{0.5\textwidth}
%DIF >  <<F0spkr-gender_mic-location_interaction, echo=FALSE, results='asis'>>=
%DIF >  boxplot(wer~speaker_gender*mic_location,col=c("white","lightgray"),F0data, names=c("Female\nEar-Recorded","Female\nMouth-Recorded","Male\nEar-Recorded","Male\nMouth-Recorded"),ylab="WER")
%DIF >  @
%DIF >  \caption{Interaction between speaker gender and microphone location.}
%DIF >  \label{fig:F0_spkr-genXmic-loc}
%DIF >  \end{wrapfigure}
%DIF >  %


\DIFdelbegin %DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Using the data from the five participants who performed the task using the speech in which the higher frequencies were added back in from the noisy mouth-recorded speech.  Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location. WER is the variable on the y-axis, and noise type by mic location is on the x-axis.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:F0_noiseXmic_boxplot}
%DIFDELCMD < \end{figure}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \section{`Training' Investigation Methods}
\label{training-methods}
\DIFaddend 

\DIFdelbegin \DIFdel{Since there were only five participants in this follow-up investigation, no actual statistics were run to test for significance.  The results here are to be used in a basic comparison of possible directions these altered methods may be able to take future research.
}\DIFdelend \DIFaddbegin \subsection{Stimuli}
\DIFaddend 

The \DIFdelbegin \DIFdel{boxplot in Figure \ref{fig:F0_noiseXmic_boxplot} demonstrates the interaction of most interest, that of noise-type and microphone location, as identified by the ANOVAs performed on }\DIFdelend \DIFaddbegin \DIFadd{exact same stimuli from }\DIFaddend the primary experiment \DIFdelbegin \DIFdel{above. No change }\DIFdelend \DIFaddbegin \DIFadd{were used in this study. No modification }\DIFaddend was made to \DIFdelbegin \DIFdel{any of the presented mouth-recorded }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend speech signals, \DIFdelbegin \DIFdel{so, as is observed, one would not expect there to be any change to listeners' ability to recognize these signals (outside of expected inter-speaker variation)}\DIFdelend \DIFaddbegin \DIFadd{such as the one used for the `F0' investigation explained in Section \ref{F0-methods} above.
}

\subsection{Participants}

\DIFadd{There were four native speakers of English with self-reported normal hearing who participated in this follow up investigation}\DIFaddend .

\DIFdelbegin \DIFdel{It is interesting to note that the primary difference between the clean ear-recorded speech and the ``noisy'' }\DIFdelend \DIFaddbegin \subsection{Design}

\DIFadd{This investigation aimed to test if `training' the participants on }\DIFaddend ear-recorded speech \DIFdelbegin \DIFdel{appears to be the extent of the variation in }\DIFdelend \DIFaddbegin \DIFadd{prior to }\DIFaddend the \DIFdelbegin \DIFdel{upper quartile and whiskers.  Based on what is seen here, if this follow up study were to be extended carried out in a full study, one would expect there to be a dramatic difference between the noisy speech recorded at the mouth, and the speech recorded at the ear.  Further discussion concerning the comparison of these results with those of the primary study and the ``perceptual learning'' study can be found below in Section \ref{ch4:glob_discussion}.
%DIF <  %
%DIF <  \begin{wrapfigure}{L}{0.5\textwidth}
%DIF <  <<F0spkr-gender_mic-location_interaction, echo=FALSE, results='asis'>>=
%DIF <  boxplot(wer~speaker_gender*mic_location,col=c("white","lightgray"),F0data, names=c("Female\nEar-Recorded","Female\nMouth-Recorded","Male\nEar-Recorded","Male\nMouth-Recorded"),ylab="WER")
%DIF <  @
%DIF <  \caption{Interaction between speaker gender and microphone location.}
%DIF <  \label{fig:F0_spkr-genXmic-loc}
%DIF <  \end{wrapfigure}
%DIF <  %
}%DIFDELCMD < 

%DIFDELCMD < \subsection{``Perceptual Learning'' Methods}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{For this investigation}\DIFdelend \DIFaddbegin \DIFadd{experiment resulted in an improvement in recognition during the experiment.  The training used is a `read-along' short story - `Peter Rabbit', by Beatrix Potter.  To obtain a recording of the story}\DIFaddend , a new speaker was recorded from the ear canal (\DIFdelbegin \DIFdel{with the same }\DIFdelend \DIFaddbegin \DIFadd{using the same experimental }\DIFaddend set-up as \DIFdelbegin \DIFdel{all previous recordings)reciting the short story ``Peter Rabbit'', by Beatrix Potter.  
}\DIFdelend \DIFaddbegin \DIFadd{described in Sections \ref{chap2:methods:procedure} and \ref{chap3:methods:procedure}).  No microphone was needed to record the speech from the mouth, and there was no background noise emitted during the recording.  
}

\DIFaddend The recorded story, as was presented to the participants, had a total length of approximately 5 minutes and 13 seconds.  The recorded story underwent the same transformations as the ear-recorded stimuli in the primary study (ie. pre-emphasis, lowpass filtering\footnote{Lowpass filter of 0-2500Hz with a 500Hz slope.}, and pre-emphasis again).  \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{There were four native speakers of English with self-reported normal hearing who participated in this follow up investigation.  They were first }\DIFdelend \DIFaddbegin \DIFadd{The participants would be }\DIFaddend presented with a \DIFaddbegin \DIFadd{typed }\DIFaddend transcript of the story \DIFdelbegin \DIFdel{, and asked to listen to the audio and read }\DIFdelend \DIFaddbegin \DIFadd{with which to follow }\DIFaddend along.  This \DIFdelbegin \DIFdel{offers }\DIFdelend \DIFaddbegin \DIFadd{provided real-time recognition feedback and offered }\DIFaddend ample chance for \DIFdelbegin \DIFdel{``successful'' }\DIFdelend \DIFaddbegin \DIFadd{`successful' }\DIFaddend recognition of the degraded ear-recorded signal (cf. \cite{davis:05}).
\DIFdelbegin \DIFdel{After the reading }\DIFdelend \DIFaddbegin 

\subsection{Procedure}

\DIFadd{The participants entered the sound booth and were presented with a transcript of the short story and were asked to listen to the audio (ear-recorded narration) and read along.  After the `training' }\DIFaddend session, the participants conducted the task as was done in the other \DIFdelbegin \DIFdel{tasks mentioned in Sections \ref{hsp-main-procedure} and \ref{F0-methods}, with the exact same stimuli as in the primary experiment}\DIFdelend \DIFaddbegin \DIFadd{studies (cf. Sections \ref{chap3:methods:procedure} and \ref{F0-methods})}\DIFaddend .

\DIFdelbegin %DIFDELCMD < \subsection{``Perceptual Learning'' Results and Discussion}
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < \begin{wrapfigure}{L}{0.75\textwidth}
\DIFdelend %DIF >  \subsection{`Training' Results and Discussion}




% \caption{Using the data from the four participants who performed the training task in which they listened and read along to a story prior to the experiment.  Boxplot displaying the average word error rate (WER) averaged over each participant for every noise type. WER is the variable on the y-axis, and noise type is on the x-axis.}
% \label{fig:perc_noise_boxplot}
% \end{wrapfigure}

% \begin{wrapfigure}{L}{\textwidth}
% <<perc_boxplot_noiseXspkr, echo=FALSE, results='asis'>>=
% boxplot(wer~noise_type*speaker_gender,col=c("white","lightgray"),perc_data, names=c("Female\nNo Noise","Female\nBus","Female\nCafe","Female\nPedestrian","Female\nStreet","Female\nFactory","Male\nNo Noise","Male\nBus","Male\nCafe","Male\nPedestrian","Male\nStreet","Male\nFactory"),ylab="WER",las=2)
% @
% \caption{Using the data from the four participants who performed the training task in which they listened and read along to a story prior to the experiment.  Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the speaker gender. WER is the variable on the y-axis, and noise type by speaker gender is on the x-axis.}
% \label{fig:perc_noiseXspkr_boxplot}
% \end{wrapfigure}

\DIFdelbegin %DIFDELCMD < \begin{figure}[h!]
%DIFDELCMD < 

%DIFDELCMD < \includegraphics[width=\maxwidth]{figure/perc_boxplot_noiseXmic-1} 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Using the data from the four participants who performed the training task in which they listened and read along to a story prior to the experiment. Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location. WER is the variable on the y-axis, and noise type by mic location is on the x-axis.}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:perc_noiseXmic_boxplot}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{As stated in the discussion of the ``F0'' investigation in Section \ref{ch4:F0_discussion}, there are too few participants to run actual statistics on the results of this follow up investigation, and so implications here should be taken lightly and used to prepare further future research in this area. The only difference between this investigation and the primary study are that the participants in this task heard a 5 minute, 13 second ``training'' story beforehand, recorded from the story narrator's ear.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The boxplot in Figure \ref{fig:perc_noiseXmic_boxplot} displays the interaction of noise type and microphone location, which was a statistical interaction in the primary experiment.  It can be seen that the relationship between ear- and noisy mouth-recorded speech appear to remain the same; as expected the ear-recorded speech continues to be more easily recognized than speech recorded at the mouth in noise.  Further discussion continues below.
}\DIFdelend %DIF >  \begin{figure}[h!]
%DIF >  <<perc_boxplot_noiseXmic, echo=FALSE, results='asis'>>=
%DIF >  par(mar=c(6,4,0,0))
%DIF >  boxplot(wer~noise_type*mic_location,col=c("white","lightgray"),perc_data, names=c("Ear\nNo Noise","Ear\nBus","Ear\nCafe","Ear\nPedestrian","Ear\nStreet","Ear\nFactory","Mouth\nNo Noise","Mouth\nBus","Mouth\nCafe","Mouth\nPedestrian","Mouth\nStreet","Mouth\nFactory"),ylab="WER",las=2)
%DIF >  @
%DIF >  \caption{Using the data from the four participants who performed the training task in which they listened and read along to a story prior to the experiment. Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location. WER is the variable on the y-axis, and noise type by mic location is on the x-axis.}
%DIF >  \label{fig:perc_noiseXmic_boxplot}
%DIF >  \end{figure}
%DIF >  
%DIF >  As stated in the discussion of the `F0' investigation in Section \ref{chap3:F0_discussion}, there are too few participants to run actual statistics on the results of this follow up investigation, and so implications here should be taken lightly and used to prepare further future research in this area. The only difference between this investigation and the primary study are that the participants in this task heard a 5 minute, 13 second `training' story beforehand, recorded from the story narrator's ear.
%DIF >  
%DIF >  The boxplot in Figure \ref{fig:perc_noiseXmic_boxplot} displays the interaction of noise type and microphone location, which was a statistical interaction in the primary experiment.  It can be seen that the relationship between ear- and noisy mouth-recorded speech appear to remain the same; as expected the ear-recorded speech continues to be more easily recognized than speech recorded at the mouth in noise.  Further discussion continues below.



\subsection{Discussion of All Investigations}
\DIFdelbegin %DIFDELCMD < \label{ch4:glob_discussion}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \label{chap3:glob_discussion}
\DIFaddend 

Looking into the differences between the primary study and the two follow-up studies, \DIFdelbegin \DIFdel{Figure \ref{fig:ALLXperc_noiseXmic_mouth_boxplot} shows the difference between the mouth-recorded speech in each of these three studies}\DIFdelend \DIFaddbegin \DIFadd{Figures \ref{fig:female-all} and \ref{fig:male-all} split the results by gender into female-speaker and male-speaker data, respectively}\DIFaddend .  Given that no change was actually made to the audio of the mouth-recorded speech between these different experiments, there is expected to be no \DIFdelbegin \DIFdel{significant visual }\DIFdelend \DIFaddbegin \DIFadd{substantial observable }\DIFaddend difference between the three \DIFdelbegin \DIFdel{different sets of boxplots}\DIFdelend \DIFaddbegin \DIFadd{studies, in each condition}\DIFaddend , which is largely what is seen.  \DIFdelbegin \DIFdel{The participants in the ``perceptual learning'' task did perform remarkably well when recognizing clean speech from the mouth compared to the other studies}\DIFdelend \DIFaddbegin \DIFadd{There is, of course, variation among the WERs on mouth-recorded signals}\DIFaddend , but this \DIFdelbegin \DIFdel{would be expected to be washed out with a proper number of participants.
}%DIFDELCMD < 

%DIFDELCMD < \begin{figure}[h!]
%DIFDELCMD < 

%DIFDELCMD < \includegraphics[width=\maxwidth]{figure/boxplot_noiseXmicXall_mouth-1} 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location, for all three studies. Only data from mouth-recorded speech is shown. WER is the variable on the y-axis, and noise type by mic location is on the x-axis. P = `Primary' study; F = `F0' study; T = `Training' study}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:ALLXperc_noiseXmic_mouth_boxplot}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The minor differences that can be visually observed in the noisy speech conditions recorded at the mouth are }\DIFdelend \DIFaddbegin \DIFadd{is not }\DIFaddend expected to be \DIFdelbegin \DIFdel{washed out as well}\DIFdelend \DIFaddbegin \DIFadd{significant if tested statistically}\DIFaddend .  It is possible that the study involving the training task, exposing listeners to degraded speech, would help `train' their auditory system to be more perceptive overall, but this is not expected to result in significant benefit.




\DIFdelbegin \DIFdel{Moving on to the speech recorded at the ear in each study, these comparisons can be seen in Figure \ref{fig:ALLXperc_noiseXmic_ear_boxplot}.  By reintroducing noise into the signal, the results of }\DIFdelend \DIFaddbegin \begin{figure}[h!]

\includegraphics[width=\maxwidth]{figure/boxplot_noiseXmicXall_female-1} 

\caption{\DIFaddFL{Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location, for all three studies; this is data for only recognition on utterances by the female speaker. WER is the variable on the y-axis, and noise type by mic location is on the x-axis. (`P',gray,`Primary' study); (`F0',green,`F0' study); (`T',blue,`Training' study); (`E',lighter color,`Ear-recorded'); (`M',darker color,`Mouth-recorded'). `EH' = Ear-recorded with reintroduction of high frequencies (from mouth-recorded speech).}}
\label{fig:female-all}
\end{figure}

\begin{figure}[h!]

\includegraphics[width=\maxwidth]{figure/boxplot_noiseXmicXall_male-1} 

\caption{\DIFaddFL{Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location, for all three studies; this is data for only recognition on utterances by the male speaker. WER is the variable on the y-axis, and noise type by mic location is on the x-axis. (`P',gray,`Primary' study); (`F0',green,`F0' study); (`T',blue,`Training' study); (`E',lighter color,`Ear-recorded'); (`M',darker color,`Mouth-recorded'). `EH' = Ear-recorded with reintroduction of high frequencies (from mouth-recorded speech).}}
\label{fig:male-all}
\end{figure}


\DIFadd{The performance on ear-recorded speech in }\DIFaddend the `F0' study \DIFaddbegin \DIFadd{(green boxes; light=ear-recorded, dark=mouth-recorded) }\DIFaddend could have potentially \DIFdelbegin \DIFdel{gone in either direction}\DIFdelend \DIFaddbegin \DIFadd{improved or worsened, as upper frequencies were reintroduced, but additional noise was also introduced}\DIFaddend .  However, the \DIFdelbegin \DIFdel{recognition WER }\DIFdelend \DIFaddbegin \DIFadd{performance on ear-recorded speech }\DIFaddend from the `\DIFdelbegin \DIFdel{Training' study would }\DIFdelend \DIFaddbegin \DIFadd{training' study (blue boxes; light=ear-recorded, dark=mouth-recorded) was }\DIFaddend not be expected to be higher than that of the primary study \DIFaddbegin \DIFadd{(gray boxes; light=ear-recorded, dark=mouth-recorded)}\DIFaddend , because the stimuli in both studies are exactly the same.  

\DIFdelbegin \DIFdel{This is precisely what is seen; while there are some variances (and }\DIFdelend \DIFaddbegin \DIFadd{There are variances in the recognition of ear-recorded speech in the different conditions, and an }\DIFaddend increased variance between the individual \DIFdelbegin \DIFdel{noise types), }\DIFdelend \DIFaddbegin \DIFadd{levels of noise type.  This could in part be due to the addition of noise into the `F0' signals along with the higher frequency, but it would not be expected to affect }\DIFaddend the data from the `\DIFaddbegin \DIFadd{training' study as there was not a significant effect in the primary study (again, the stimuli in these two studies was exactly the same).  Variances aside, the `}\DIFaddend Training' study seems to result in essentially the same results the primary study.  A full-fledged study with more participants is certainly needed to be able to make any inferences about the benefits of this particular training procedure, but the results shown here do not indicate much improvement.  There are a number of potential factors that may have, or could in the future affect the ability of listeners to fully benefit from the training offered.  

While unlikely a major component, a single, different (male) voice was used for the training story in this follow-up study.  It is possible that the use of only one voice did not provide listeners with an adequate variety of vocal variations to make proper inferences about the distortion.  It is also important to consider the use of at least one male and at least one female voice during training, to avoid a potential gender effect.

Additionally, while for this task full attention was assumed, it is uncertain how much actual attention participants were devoting to listening to the training story.  Rather than a \DIFdelbegin \DIFdel{``}\DIFdelend \DIFaddbegin \DIFadd{`}\DIFaddend read-along\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{' }\DIFaddend training task, a more interactive task may capture more attention than passive listening and reading.  The interactive task could be structured as a forced decision task, providing multiple choice answers to an ear recorded sentence they hear.  Alternatively, it could be structured (as in the experiment itself) to force listeners to \DIFdelbegin \DIFdel{``fill in the blank'' }\DIFdelend \DIFaddbegin \DIFadd{`fill-in-the-blank' }\DIFaddend with what they thought was spoken, then provide listeners with the feedback in the form of the answer.  

\DIFdelbegin \DIFdel{In either of these tasks}\DIFdelend \DIFaddbegin \DIFadd{Regardless of the training task employed}\DIFaddend , the listener should be given the ability to replay the same sound multiple times\DIFaddbegin \DIFadd{, which was not done in the present training task}\DIFaddend .  The length of time in a \DIFdelbegin \DIFdel{``}\DIFdelend \DIFaddbegin \DIFadd{`}\DIFaddend read-along\DIFdelbegin \DIFdel{'' }\DIFdelend \DIFaddbegin \DIFadd{' }\DIFaddend training task, or the number of training sentences in an interactive training task, would also likely have an effect on sentence recognition during the experiment.  This should be determined more carefully than was done in the above experiment.

\DIFdelbegin %DIFDELCMD < \begin{figure}[h!]
%DIFDELCMD < 

%DIFDELCMD < \includegraphics[width=\maxwidth]{figure/boxplot_noiseXmicXall_ear-1} 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdel{Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location, for all three studies. Only data from ear-recorded speech is shown.  WER is the variable on the y-axis, and noise type by mic location is on the x-axis. P = `Primary' study; F = `F0' study; T = `Training' study}}
%DIFAUXCMD
%DIFDELCMD < \label{fig:ALLXperc_noiseXmic_ear_boxplot}
%DIFDELCMD < \end{figure}
%DIFDELCMD < %%%
\DIFdelend %DIF >  \begin{figure}[h!]
%DIF >  <<boxplot_noiseXmicXall_ear, echo=FALSE, results='asis'>>=
%DIF >  
%DIF >  alldata1 <- subset(alldata, mic_location!=1)
%DIF >  par(mar=c(6,4,0,0))
%DIF >  boxplot(wer~noise_type*droplevels(mic_location)*style,col=c("white","lightgray"),alldata1, las=2, names=c("No Noise.P","Bus.P","Cafe.P","Pedestrian.P","Street.P","Factory.P","No Noise.F","Bus.F","Cafe.F","Pedestrian.F","Street.F","Factory.F","No Noise.T","Bus.T","Cafe.T","Pedestrian.T","Street.T","Factory.T"),ylab="WER",las=2)
%DIF >  @
%DIF >  \caption{Boxplot displaying the average word error rate (WER) averaged over each participant for the interaction of every noise type by the mic location, for all three studies. Only data from ear-recorded speech is shown.  WER is the variable on the y-axis, and noise type by mic location is on the x-axis. P = `Primary' study; F = `F0' study; T = `Training' study}
%DIF >  \label{fig:ALLXperc_noiseXmic_ear_boxplot}
%DIF >  \end{figure}


The `F0' experiment, also displayed in \DIFdelbegin \DIFdel{Figure \ref{fig:ALLXperc_noiseXmic_ear_boxplot}}\DIFdelend \DIFaddbegin \DIFadd{Figures \ref{fig:female-all} and \ref{fig:male-all}}\DIFaddend , appears to \DIFdelbegin \DIFdel{genuinely have promise}\DIFdelend \DIFaddbegin \DIFadd{have only marginally more promise than the primary study or the training study}\DIFaddend .  It would be expected that the `\DIFdelbegin \DIFdel{non-noise' }\DIFdelend \DIFaddbegin \DIFadd{clean' ear-recorded }\DIFaddend condition would perform much better under this transformation, since \textit{clean} upper frequency information is being added back into the signal (ie. there is no additional noise being added, only information that was previously lost).  \DIFaddbegin \DIFadd{This is largely observed, and for the data for the male-speaker in this condition, recognition nearly matched that of the clean, mouth-recorded signal in the primary study.
}\DIFaddend 

In the conditions in which there is noise, it was uncertain whether there would be benefit - and if so, to what degree - of re-incorporating noisy upper frequencies.  \DIFdelbegin \DIFdel{Yet there appears }\DIFdelend \DIFaddbegin \DIFadd{There did not seem }\DIFaddend to be a \DIFdelbegin \DIFdel{noticeable performance increase.  For every individual noise type }\DIFdelend \DIFaddbegin \DIFadd{decrease in recognition accuracy for ear-recorded speech in this study, but the only substantial benefit occurred with the female-speaker data, for caf}\'{e} \DIFadd{and street background noises.  While for every other noise type (except the male speaker's caf}\'{e} \DIFadd{background)}\DIFaddend , the median WER \DIFdelbegin \DIFdel{in }\DIFdelend \DIFaddbegin \DIFadd{is lower for }\DIFaddend the `F0' study \DIFdelbegin \DIFdel{is lower than that in }\DIFdelend \DIFaddbegin \DIFadd{data than for that of }\DIFaddend the primary study\DIFaddbegin \DIFadd{, the difference between the two does not seem visually substantial}\DIFaddend .

There is often important (though, perhaps, not critical) speech information above the \DIFdelbegin \DIFdel{~2700 Hz }\DIFdelend \DIFaddbegin \DIFadd{approximately 2.7 kHz }\DIFaddend lowpass cutoff imposed on the stand-alone ear-recorded speech.  This would be expected, as a standard bandpassed signal for a telephone reaches up to \DIFdelbegin \DIFdel{3500 Hz}\DIFdelend \DIFaddbegin \DIFadd{3.5 kHz (compared with approx. 2.7 kHz)}\DIFaddend , and there are still those who have difficulty with its intelligibility.  The noisy upper frequencies which were added back into the ear-recorded signal contain this important information\DIFdelbegin \DIFdel{.  These preliminary results indicate that the human auditory system seems to be adept enough to parse this speech information from the higher frequencies out of the surrounding background noiseusing the ``clearer'' speech information in the lower frequencies.  And, importantly, that this benefits recognition}\DIFdelend \DIFaddbegin \DIFadd{, but it is also accompanied by unpredictable noise.  There is some promise that this benefited recognition, but the improvement seen here is only marginal}\DIFaddend .

Further investigation, of course, is still needed, as these preliminary WERs from the F0 study are from only five listeners.  Additionally, this method is still (albeit to a lesser degree) at the mercy of the ambient noise, and one would expect that as the SNR decreases (ie. as the noise level increases), this method will become less effective, and the auditory system poorer at extracting the high frequency speech information from the noise.  The range of SNRs at which this method becomes ineffective, as well statistically demonstrating its effectiveness in general, will need to be \DIFdelbegin \DIFdel{carried out by }\DIFdelend \DIFaddbegin \DIFadd{determined with }\DIFaddend future research.

\DIFdelbegin %DIFDELCMD < \subsection{Limitations}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \subsection{Future Research}\label{chap3:future-research}
\DIFaddend 

There are several limitations that were noted during the experiment.  The first \DIFdelbegin \DIFdel{was brought up by a participant during discussion while filling out a questionnaire, and, while minor , }\DIFdelend \DIFaddbegin \DIFadd{is the very low number of participants (only one) per counter-balanced group.  Despite this, the statistics were able to demonstrate a significant effect.  In similar future research, more participants should be used to adequately fill each counter-balanced group.  The author hypothesizes that the results of any such experiment will still be significant.  
}

\DIFadd{A participant indicated another limitation during discussion after the experiment had completed.  While it likely has only a minor effect, it }\DIFaddend has interesting ties to the literature.  This participant (including several others), \DIFdelbegin \DIFdel{mentioned the fact }\DIFdelend \DIFaddbegin \DIFadd{stated }\DIFaddend that they found the task particularly difficult due to the computer monitor in front of them.  The participant reported that it was difficult to focus on recognition of the stimulus due to the \DIFdelbegin \DIFdel{glow and }\DIFdelend competing visual stimulus of the computer monitor.

This is in line with what was found in \cite{francis:09} and \cite{francis:10} concerning working memory and perception; in this instance staring at the computer monitor and resulting visual stimulation proved to be a distraction and overloaded the working memory of the participant, which was already overloaded by trying to interpret the noisy and degraded speech.  \DIFdelbegin \DIFdel{Remarkably }\DIFdelend \DIFaddbegin \DIFadd{It was remarkable that }\DIFaddend several participants commented that the computer monitor proved to be a distraction.  This diversion of working memory may have had a detrimental effect on recognition (cf. \cite{caplan:99})\DIFaddbegin \DIFadd{, though it is unlikely that this aspect of the experiment can be avoided}\DIFaddend .

\DIFdelbegin \DIFdel{Another }\DIFdelend \DIFaddbegin \DIFadd{An additional }\DIFaddend limitation, realized partway through the experiment, was that not all of the sound files had a normalized amplitude.  \DIFaddbegin \DIFadd{This is an experimenter error; all stimuli underwent an amplitude normalization process, and correct normalization was not adequately verified. }\DIFaddend There were some \DIFaddbegin \DIFadd{utterance files in }\DIFaddend which the amplitude of the sound was \DIFdelbegin \DIFdel{below what it should have been}\DIFdelend \DIFaddbegin \DIFadd{much lower than the norm}\DIFaddend , and some in which this amplitude was \DIFdelbegin \DIFdel{greater than it should have been}\DIFdelend \DIFaddbegin \DIFadd{much greater than the norm}\DIFaddend .  This occurred seemingly randomly throughout all conditions, and likely resulted in more variance towards the higher WERs and poorer performance on these sentences with \DIFdelbegin \DIFdel{``abnormal'' amplitudes.  }%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Notably, the number of participants per counter-balanced group is rather low.  There are 24 total participants, but only one in each counter-balanced group.  This means that only one person heard each individual sentence in a given condition combination}\footnote{\DIFdel{For example, only one person heard ``A cramp is no small danger on a swim'' spoken by the male speaker, recorded at the mouth with bus background noise; this is true of all sentence/condition combinations.}}%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{.  If more participants were able to be run in each counter-balanced group, this would present a more accurate representation of performance on a given item, rather than having only one WER for each sentence-condition combination}\DIFdelend \DIFaddbegin \DIFadd{`abnormal' amplitudes.  Furthermore, the difference in SNR between the two speakers introduced a confound within the `speaker gender' factor.  Ensuring a relatively equal SNR between all speakers' stimuli in the future will avoid this issue}\DIFaddend .

Additionally, as stated earlier in Sections \DIFdelbegin \textit{\textbf{%DIFDELCMD < [%%%
\DIFdel{First Chapter, Limitations}%DIFDELCMD < ]%%%
}%DIFAUXCMD
}%DIFAUXCMD
\DIFdel{\ref{ch2:limitations} and \ref{chap4:methods:stimuli}}\DIFdelend \DIFaddbegin \DIFadd{\ref{chap2:limitations} and \ref{chap3:methods:stimuli}}\DIFaddend , the method of recording these stimuli to achieve a higher SNR ratio\footnote{Recall, the microphone in front of the mouth was pointed towards the loudspeaker \DIFaddbegin \DIFadd{for the stimuli in this study}\DIFaddend } may have artificially increased some of the differences seen between the noisy speech recorded at the mouth and the same speech recorded at the ear.  Since there still appears to be some noise that reaches the microphone at the ear\DIFdelbegin \DIFdel{(there is a difference between the noisy conditions at the ear and the non-noise condition at the ear)}\DIFdelend \DIFaddbegin \footnote{\DIFadd{There is a statistical difference between the noisy conditions at the ear and the non-noise condition at the ear for the female speaker. It is possible this difference resulted from a leak in the seal of the earmuff for this speaker (cf. Section \ref{chap3:discussion}).}}\DIFaddend , this may have had a detrimental effect on listener's performance on these sentences.  Future research could avoid this by increasing the ambient noise using a capable loudspeaker with appropriate hearing protection for both participant and researcher, and in \DIFdelbegin \DIFdel{an environment }\DIFdelend \DIFaddbegin \DIFadd{a location }\DIFaddend where the surrounding environment can be insulated from this level of noise (cf. Section \DIFdelbegin \textit{\textbf{%DIFDELCMD < [%%%
\DIFdel{First Chapter, Limitations}%DIFDELCMD < ]%%%
}%DIFAUXCMD
}%DIFAUXCMD
\DIFdel{\ref{ch2:limitations} }\DIFdelend \DIFaddbegin \DIFadd{\ref{chap2:limitations} }\DIFaddend for more details).


% \section{Survey Results}

% # <<survey_results_load, echo=FALSE, results='asis'>>=
% # exp2_data1 <- read.csv("/home/sam/Dissertation/exp2_data.csv")
% # exp2_data2 <- read.csv("/home/sam/Dissertation/exp2_inelligible_data.csv")
% # 
% # exp2_data <- rbind(exp2_data1,exp2_data2)
% # 
% # exp2_data$difficult[exp2_data$difficult == "y"] <- "yes"
% # exp2_data$difficult[exp2_data$difficult == "s"] <- "somewhat"
% # exp2_data$difficult[exp2_data$difficult == "n"] <- "no"
% # 
% # exp2_data$nois_muff[exp2_data$nois_muff == "n"] <- "noisy"
% # exp2_data$nois_muff[exp2_data$nois_muff == "b"] <- "both"
% # exp2_data$nois_muff[exp2_data$nois_muff == "m"] <- "muffled"
% # 
% # exp2_data$muff_annoy[exp2_data$muff_annoy == "y"] <- "yes"
% # exp2_data$muff_annoy[exp2_data$muff_annoy == "s"] <- "somewhat"
% # exp2_data$muff_annoy[exp2_data$muff_annoy == "n"] <- "no"
% # 
% # ggplot(exp2_data$nois_muff, aes(x=variable)) + geom_bar()
% # @

% The 24 participants in the primary study were asked 3 questions, (a) ``Overall did you find the task difficult?'', (b) ``Which did you find more difficult, the noisy speech or the `muffled' speech?'', and (c) ``Would you find it uncomfortable/annoying to listen to the muffled speech for an extended period?''.  Qualitative answers were given, which were then coded into different categories by a researcher.  Regarding question (a), 18 participants found the task to be difficult, while 8 found it to be \textit{somewhat} difficult.  No one responded that the task was easy.  
% 
% For question (b), the `muffled' speech refers to the ear-recorded speech, which the participants were unaware was recorded from the ear.  There were 14 responses which deemed the noisy speech more difficult to understand and 10 responses that deemed the muffled speech more difficult to understand.  Two respondants reported that both were equally difficult.  This is particularly interesting, because while subjectively responses were relatively split between the two recording locations (noisy speech from the mouth, and `muffled' speech from the ear), every single participant had a lower WER for the ear-recorded speech compared to their WER for the noisy, mouth-recorded speech.  This means that their subjective judgement of overall intelligibility did not match their sentence-for-sentence recognition performance.
% 
% Question (c), however, shows a one-sided agreement similar to question (a); 18 participants reported that they would find it annoying or uncomfortable to listen to the `muffled' ear-recorded speech for an extended period of time.  There were 5 respondants who reported it would only be \textit{somewhat} annoying/uncomfortable, and 3 respondants who said it would not be uncomfortable to listen to the `muffled' speech.  The 8 participants who answered ``somewhat'' or ``no'' for question (c) do not necessarily correspond with the 8 participants who answered ``somewhat'' in question (a).
% 
% This same survey was completed by the four participants in the ``perceptual-learning'' investigation.  Question (a) resulted in 3 ``yes'' answers and 1 ``somewhat'' answer.  Question (b) yielded 3 participants who believed the noisy speech was more difficult to understand, and 1 participant who thought the `muffled' speech was more difficult.  All four participants reported that they would find it annoying or uncomfortable to listen to the muffled speech for longer periods of time.  These answers are given despite the training task they performed, listening and reading a story in which the narration was recorded from the speaker's ear.  Similar to the preliminary results seen in Figure \ref{}, the training seems to have had no effect on listener's ability to understand, or comfort with, speech recorded from the ear.


\section{Conclusion}

In summary, speech recorded at the ear, via the process described in Chapter \DIFdelbegin \DIFdel{2}\DIFdelend \ref{chapter2} does appear to be intelligible by human listeners, despite being severely low-pass filtered.  More-so, it is more intelligible than simultaneously recorded speech at the mouth in noise, although mouth-recorded speech without background noise is still obviously the easiest to understand.  \DIFdelbegin \DIFdel{A speaker gender interaction was found to be significant}\DIFdelend \DIFaddbegin \DIFadd{Speaker gender significantly interacted with microphone location}\DIFaddend , but this is likely due to differences in the SNR for each speaker, rather than their gender.  

The additional follow-up study using training to help listeners become more accustomed to the ear-recorded speech did not demonstrate much performance over training.  \DIFdelbegin \DIFdel{However, the additional follow up }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend study which recombined the high frequency information from the mouth-recorded speech with the \DIFdelbegin \DIFdel{low-pass filtered }\DIFdelend \DIFaddbegin \DIFadd{lowpass-filtered }\DIFaddend ear-recorded speech indicates\DIFdelbegin \DIFdel{that this transformation might yield greater human recognition of speech than the }\DIFdelend \DIFaddbegin \DIFadd{, at best, marginal improvement over lowpass-filtered }\DIFaddend ear-recorded signals by themselves.

It is important to emphasize that the results from both the follow-up studies \DIFdelbegin \DIFdel{are not statistically significant}\DIFdelend \DIFaddbegin \DIFadd{were not statistically tested}\DIFaddend , simply because they did not contain enough participants to run statistics.  Future research should expand on these preliminary results, conducting more thorough and statistical experimentation\footnote{\DIFdelbegin \textbf{\DIFdel{Stimuli, experimental code, and data from this experiment can be found at URL.COM}}%DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{Speech data from this experiment can be found at http://www.openslr.org/}\DIFaddend }.  There are certainly more \DIFdelbegin \DIFdel{stimuli }\DIFdelend \DIFaddbegin \DIFadd{stimulus }\DIFaddend modifications to explore that exploit the auditory system's innate ability to release it from the masking of ambient noise.

% \bibliographystyle{apa}
% \bibliography{DissRefs.bib}
% \end{document}
