% %\Sexpr{set_parent(â€˜/Users/mwilli/Documents/Spring_2017/Dissertation_Document/Dissertation_Working_Directory_Draft/Dissertation_Main.Rnw')}
% 
% <<chunk_options, echo=FALSE>>=
% # This is where we set basic knitr options.
% opts_chunk$set(echo=TRUE, message=FALSE, warning = FALSE, cache = FALSE, comment="")
% options(width=75) # This sets how wide the R printout can be.
% @
% 
%  <<setup-child, include=FALSE>>=
%  set_parent('/home/sam/Dissertation/Dissertation_Template/Dissertation_Main_Template/Dissertation_Main.Rnw')
% @
% 
% <<load_libraries, echo=FALSE>>=
% library(tidyr)
% library(dplyr)
% library(ggplot2)
% library(lme4)
% library(lsmeans)
% library(car)
% library(pbkrtest)
% library(xtable)
% library(cowplot)
% library(plyr)
% @

\chapter{Overall Discussion\label{chapter5}}


The primary goal of this project is to record a small corpus of ear-recorded speech, and test the ability of humans and ASR systems to accurately recognize the speech content.  More specifically, the project aims to determine a) if ear-recorded speech, using an earplug and ear muffs, is able to significantly filter out background noise, b) if ear-recorded speech contains enough information to be intelligible, c) if human listeners find ear-recorded speech more intelligible than the exact same signal recorded at the mouth in a noisy environment, and d) if an ASR system finds ear-recorded speech more intelligible than speech recorded at the mouth in a noisy environment.

The following three sections briefly review the task and results of the three experiments discussed in this report: a) the data collection experiment which gathered the ear-recorded speech, b) the human speech perception experiment testing human ability to perceive ear-recorded and noisy mouth-recorded speech, and c) the ASR experiment testing the ability of an ASR system to recognize ear-recorded speech compared with noisy mouth-recorded speech.  Limitations of the conducted studies and directions for future research will be discussed in Section \ref{chap5:future-research}, and the over-arching findings of this report will be briefly summarized in Section \ref{chap5:conclusions}.


\section{Summary of Ear-Recorded Speech}\label{sec:chap2-review}

Ear-recorded speech was collected and analyzed from 20 participants.  The microphone was placed into a silicone earplug and into the participants' right ear (facing the tympanic membrane), and noise reduction earmuff were placed over top of their ears.  A rod extending out from the side of the earmuffs was used to place a microphone recording the speech as it came from their mouth.  A loudspeaker was placed off to the side, which emitted background noise.  The participants read aloud stimulus sentences, and were recorded, while various kinds of background noises (bus, caf\'{e}, pedestrian area, street, factory) were produced by the loudspeaker.  For more details concerning the experimental methods, refer to Chapter \ref{chapter2}, Section \ref{expt1}.

These recordings were then observed.  Overall, the ear-recorded speech was highly low-pass filtered, and, subjectively, it had a very `muffled' quality.  To test the similarity between the mouth-recorded signals and their ear-recorded counterparts, the maximum value of the cross-correlation matrix was obtained and normalized (ie. to obtain values between 0 and 1, where 1 indicates identical signals), and this value was averaged over all clean-speech utterances by all speakers.  Only utterances with no background noise were included in this calculation, in order to only test the similarity of the speech and not the noise.  This test yielded a maximum normalized cross-correlation value of 0.358 as an average across all included utterances.

To increase the amplitude of the speech information in the higher frequencies, the author pre-emphasized the signal.  As there was no speech information found above approximately 2.7 kHz, and since the pre-emphasis resulted in the presence of white noise in the higher frequencies, the signal was then low-pass filtered at 2.5 kHz with a slope of 500 Hz.  The lower frequencies in this signal were still much more prominent than the upper frequencies, and so pre-emphasis was applied a second time.  The average maximum normalized cross-correlation metric was obtained using the newly transformed ear-recorded speech, and each signal was compared to its simultaneously mouth-recorded counterpart.  As before, only clean speech, with no background noise, was included in this comparison.  This gave a nearly doubled cross-correlation value of 0.674.

% The spectra and spectrograms for the sentence ``He said the same phrase thirty times'', spoken by a male participant, are provided for better visualization of speech intelligibility and noise reduction.  A comparison between speech recorded at the mouth and at the ear, with transformations, and with no background noise can be seen in Figures \ref{x} and \ref{y}); the same utterance is displayed, spoken at the mouth with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{} and \ref{}) and at the ear (after transformations) with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{}, and \ref{}).  Each mouth- and ear-recorded pair is the exact same utterance; they were recorded simultaneously (eg. the already mentioned Figures \ref{x} at the ear and \ref{y} at the mouth).

%The figures containing noisy speech recorded at the mouth (ie. Figs. \ref{}, \ref{}, and \ref{}) demonstrate 
One of the limitations of the collected data was that the SNR of the signal recorded by the mouth microphone was very high for all noise categories.  The noise did not drown out the speech in the high noise (80 dB) condition as originally intended.  The SNR needed to be high to compare the recognition performance of humans and computers on the ear-recorded speech, compared with the noisy mouth speech.  If the noise wasn't sufficiently high enough to interfere with the recognition of speech, then the mouth-recorded speech would be too easy to understand, and an adequate comparison between ear-recorded speech and noisy mouth-recorded speech could not be made.

Despite the high SNRs in the mouth-recorded signals, noise is still present.  Considering the ear-recorded signals that were also recorded in the noisy environment %(ie. Figs. \ref{}, \ref{}, and \ref{}), 
the passive noise reduction that is provided by the head, earplug, and earmuffs appears to have successfully eliminated a substantial proportion of the noise from the signal.

These recordings demonstrate that using the ear canal as a location of recording speech in noise can offer a large reduction in the noise that enters the signal.  Furthermore, ear-recorded signals can provide speech comparable to mouth-recorded speech up to approximately 2.7 kHz.  This is relatively close to the 3.5 kHz cutoff generally used for telephonic communication, which does provide intelligible speech for both humans and computers. The author hypothesized that speech recorded from the ear and cut off at 2.7 kHz would also be intelligible for humans and for computers.  The following sections will review the results of the experiments in Chapters \ref{chapter3} and \ref{chapter4}, which aimed to test this hypothesis.


\section{Summary of Human Perception of Ear-Recorded Speech}\label{sec:chap3-review}

Due to the high SNR of the speech recorded in the data collection experiment, described in Chapter \ref{chapter2} and Section \ref{sec:chap2-review} above, two additional speakers were recorded in the same manner, ie. with a microphone placed in an earplug and in their ear canal, with earmuffs placed over their ears.  Another directional microphone was placed in front of their mouth, except instead of pointing at their mouth, the microphone pointed at the loudspeaker in the room, which produced the background noise. The ear-recorded speech was pre-emphasized and low-pass filtered in the same manner as the recordings conducted in Chapter \ref{chapter2}. For further details about the collection of these additional recordings, refer to Chapter \ref{chapter3}, Section \ref{expt2}.

The speech recorded from these two speakers was used as stimuli for the human perception experiment.  There were 24 participants, all native speakers of English, who listened to the provided stimuli.  There were 24 conditions with 3 factors, gender of the speaker of the stimulus (male,female), recording location of the stimulus (in front of mouth, in ear canal) and noise type (clean, bus, caf\'{e}, pedestrian area, street, factory).  The stimuli were placed in counter-balanced groups, where each sentence stimulus occurred in each of the 24 conditions.  Each of the 24 participants received one of the 24 counter-balanced stimulus groups, meaning that each distinct sentence-condition pairing was only heard by one participant listener.
%Each participant listener heard each stimulus utterance once, and heard three utterances per condition.  This yielded 72 stimuli (72 distinct utterances), with each of the 24 conditions repeated three times.  Each listener could only hear a given utterance once before they would be given an advantage to understanding it.  To avoid this, counter-balanced lists were developed to place each stimulus in each of the 24 conditions, requiring

The participant listeners heard an utterance, and typed what they heard into a response box.  They were instructed that what was heard might not be a complete sentence, and to only type what they thought they heard.  These responses were recorded for each participant, and WER was calculated between the actual utterance transcription and the proposed transcription provided by the participant listeners.

Two, 3-way ANOVAs was conducted on the data, with dependent variable WER.  Factors for the first included `speaker [of the stimulus]'s gender' (female, male), `recording [mic] location' (mouth, ear), and `noise type' (clean, bus, caf\'{e}, pedestrian, street, factory).  The second ANOVA removed the level `clean' (no noise) from the factor of noise, because it differed substantially from the other levels in that factor.  Both ANOVA found significant interactions of speaker gender \textbf{x} recording `mic' location and noise type \textbf{x} microphone location.  Factors were split to test simple effects.  Of note, there was a statistical difference for all simple effects of microphone location, indicating that listeners more easily recognize ear-recorded speech than [sufficiently] noisy mouth-recorded speech.  Additionally, when the `clean' level was removed from the ANOVA, and simple effects of noise were tested, there was a statistical difference of noise for mouth-recorded speech, but no statistical difference for ear-recorded speech.  This implies that the noise level was sufficiently dampened in the ear-recorded speech that the various noise types had no effect.  Considering the ear-recorded speech at every level of speaker gender and noise, the WER hovered mainly between 20-30\%.

Two additional investigative studies were performed; the first added high-pass filtered mouth recorded speech to the low-pass filtered ear-recorded stimuli, and the second added a brief training session - listening to the narration of a story recorded from the ear canal - before proceeding to the task.  There were not enough participants in either of these studies to run statistics, so all results are speculative.  These were compared to the primary study (above) to determine if there might be any improvement in the recognition of ear-recorded speech by utilizing these methods.  No substantial improvement was observed from the participants who were exposed to a pre-task training session.  Very minor improvement was observed for the high/low frequency combination study; there were a few conditions that did see a stark improvement, including the `clean' noise condition.

In summary, the use of ear-recorded speech as a noise-robust method of communication seems promising, as it is more easily recognizable than speech spoken from the mouth in noise.  However, even a WER of 20\%, ie. the ability to correctly understand only 4 out of every 5 words, is not sufficient for most situations requiring human communication.  The two methods employed do not seem to offer much improvement, though these - and other possibilities for improving recognition - should be fully tested in future research.



\section{Summary of Automatic Speech Recognition of Ear Recorded Speech}\label{sec:chap4-review}

The author used the Kaldi ASR system (\cite{povey:11}) and the acoustic and language models trained on the data from the LibriSpeech corpus (\cite{panayotov:15}) to test the ability of a standard ASR system to recognize both ear-recorded speech and noisy mouth-recorded speech.  For testing data, the ear- and mouth-recorded speech that was used was obtained in the data collection experiment - presented in Chapter \ref{chapter2} and reviewed in Section \ref{sec:chap2-review}.  The ear-recorded speech was transformed with pre-emphasis, low-pass filtering, and a second application of pre-emphasis (described in Chapter \ref{chapter2} and in Section \ref{sec:chap2-review}). For more details about these methods, refer to Chapter \ref{chapter4}, Section \ref{expt3}.

The performance of the ASR system on clean mouth-recorded speech (9.38\% WER) and clean ear-recorded speech (79.30\% WER) demonstrated a tremendous divergence, with mouth-recorded speech substantially out-performing ear-recorded speech.  As the noise level increased, this divergence shrunk, but even at the highest noise level (80 dB), the ear-recorded speech out-performed the mouth recorded speech in only one noise type (`bus').  Furthermore, these WERs are prior to any application of noise-removal techniques beyond standard, built-in procedures that are used during Kaldi feature extraction.  If additional noise-removal techniques were to be applied, such as Advanced Front End (AFE), there would be an even greater gap between ASR performance on ear- and mouth-recorded speech.

It was very possible that the acoustic model - trained on 960 hours of mouth-recorded speech - was searching for speech information in the higher frequencies that simply did not exist in the low-frequency ear-recorded signal.  Another model was trained on a much smaller subset of the 960-hour corpus (75 hours) of mouth-recorded speech, which was low-passed to the same frequency range as ear-recorded speech.  Performance worsened even further when using this model.  It may be that low-passed mouth-recorded speech cannot substitute for ear-recorded speech when training acoustic models for ASR.  This is telling, that humans are able to understand the ear-recorded speech with a reasonable degree of accuracy, and yet the ASR system's peformance suffers greatly.  It would seem that humans are using speech cues in the lower frequencies that the ASR system is not, and/or the ASR system is using speech cues in the higher frequencies that humans do not need.

A final test was performed using the technique of adding the high-frequency portion of the mouth-recorded speech to the low-frequency ear-recorded speech to reintroduce this upper frequency information that the ASR system may be looking for.  This same technique was also used in Chapter \ref{chapter3} for human speech perception.  For these tests, the original acoustic model trained on 960 hours of speech was used. Performance of this `hybrid' speech data increased by as much as 14\% WER in the 80 dB noise condition, and in the 60 dB noise condition, by as much as 30\% WER.  All noise types in the 80 dB noise conditions - except one (`factory') - saw an improvement in WER from the mouth-recorded noisy speech to the hybrid mouth- and ear-recorded combination.  Nevertheless, for the much higher-SNR 70 dB and 60 dB noise conditions, performance worsened when using the `hybrid' speech over the `plain' noisy mouth-recorded speech.  This hints that the ear-recorded speech itself might be subtley different from the mouth-recorded speech in the same frequency range, and that for substantial improvements to be seen, an acoustic model needs to be trained with ear-recorded speech.

From these results, it appears that ear-recorded speech only offers a benefit over speech recorded at the mouth in very noisy conditions.  The hybrid speech offers a slight improvement, but the capability for further benefit is unknown.  Despite the poor performance of the model trained on low-pass filtered mouth recorded speech, the greatest improvement to recognition will likely come from retraining or adapting an acoustic model specifically using ear-recorded speech data.


\section{Future Directions}\label{chap5:future-research}

One of the primary limitations discussed throughout this report is that the SNR for the speech recorded in the original data collection experiment (cf. Chapter \ref{chapter2}) was much higher than originally intended or hoped for, by slightly more than +20 dB SNR for each condition.  This yielded noisy speech which was largely greater than +10 dB SNR; current ASR technology is rather adept at recognizing signals with these levels of noise (cf. \cite{braun:16}).  Due to this, it was difficult to see how much improvement could be gained over very noisy, low-SNR signals.  Two possibilities stand out as potentially workable solutions.  The first, would be to simply increase the ambient background noise level, and the second involves using an omnidirectional microphone to record the speech from the mouth (rather than a directional microphone).  

The former introduces potential risk to those exposed to the noise, depending on the level of noise and the amount of hearing protection offered.  Additionally, for very high noise conditions, it would be difficult to limit the noise exposure to the surrounding area.  The latter solution allows the speaker to talk normally and for the ambient noise level to remain relatively low.  However, this does not necessarily result in a fair comparison between ear-recorded and mouth-recorded speech, as in any real-life application, a directional microphone would be utilized with the specific intention of reducing the noise picked up by the microphone.  If adequate hearing protection is offered, and the noise contained, the former solution offers the most realistic comparison between the two microphone locations.

Regarding human speech perception, ear-recorded speech seems promising.  There is a significant difference between the ability of human listeners to recognize ear-recorded speech and mouth-recorded speech in noise.  Ear-recorded speech does not provide the same recognition performance as speech recorded at the mouth without background noise.  However, two methods were proposed to further improve the performance of ear-recorded speech.  The first introduced a training session prior to the actual experiment in which participants listened to the narration of a short story, recorded from the ear canal.  The second spliced the low-frequency ear-recorded speech with the higher frequencies of the mouth-recorded speech.  Neither of these tested methods were performed with enough participants (due to resource limitations) to run adequate statistics (due to requiring counter-balanced groups).  

The non-statistical, preliminary results for the former (training) method showed little or no improvement over the standard ear-recorded speech, while the non-statistical, preliminary results for the latter (spliced) method provided indication that this could potentially yield improvement.  Future research should test these methods statistically.  The former (training) method may need altered methodology in order to determine whether a form of training could result in an improvement in future recognition of ear-recorded speech.  A training task that requires interaction and provides feedback may promote active attention during the training task, unlike passively listening to a story as used in this study.  In the method using spliced stimuli, further research should pursue the possibility of using the clean, lower frequencies to actively clean any noise from the higher mouth-recorded frequencies.  

The ASR recognition task initially indicated that ear-recorded speech offered essentially no performance improvement over noisy mouth-recorded speech. This utilized a DNN ASR system trained on a 960-hour corpus of speech. A small GMM-HMM model was trained on 75 hours of speech - a subset of the 960-hour corpus - which was low-pass filtered to mimic the low-frequency ear-recorded speech.  When tested on this system, recognition accuracy on ear-recorded speech became worse.  It could very well be possible that the amount of speech used to train this additional model was not sufficient, or that performance would have improved by training a DNN model, though this would likely not have resulted in adequate improvement alone.  low-pass filtered mouth-recorded speech may also not be an appropriate substitute for ear-recorded speech, and that by training a model using speech recorded from the ear would provide the needed increase in performance.

The same method that was used in the human perception experiment - adding high-frequency (noisy) mouth information to low-frequency ear-recorded speech - was also used for the ASR task.  This demonstrated moderate improvements to performance on the hybrid ear recorded speech, but it only dropped below the WER of mouth-recorded speech in the 80 dB noise condition and only in 4/5 noise types.  Needless to say this is still not acceptable for use in any real-life ASR application.  It is apparent that the level of noise in the high-frequency mouth-recorded component has an effect on performance; if existing techniques for noise removal were applied to the mouth-recorded speech before combination with the ear speech, this might partially mitigate the negative effect of noise observed in Table \ref{tab:hybrid-wers}.  Further gains might be obtained by actively using the clean lower-frequency ear-recorded speech to actively clean the higher frequency information in the mouth recorded signal before combining the two for recognition.   
Additional research should also pursue other methods of improving the human or computer recognition of ear-recorded speech. 


% \section{Theoretical Implications}
% 
% If Chapter \ref{chapter3} demonstrated that the human auditory system is robust in its ability to accurately identify speech in a degraded signal, Chapter \ref{chapter4} has shown that automatic speech recognition systems remain fragile.  It is not unreasonable to assume that the human listeners in the speech perception experiment had never heard speech recorded from the ear, yet were still able to understand the signal with a relatively high degree of accuracy and adapt to the task at hand.  ASR, on the other hand, remains relatively `rigid' in its domain-specific ability to recognize speech unlesstrained on the specific task.
% 
% The robustness and flexibility of human speech perception could be due in part to the redundancy of cues it uses to represent various speech features (\cite{winter:14}).  If one feature or set of features is rendered unitelligible (eg. voice onset time (VOT) for voicing contrast of stop sounds), others cues can be used (eg. duration of previous vowel) to still achieve recognition.  Furthermore, humans are able to `turn-off' reliance on features that are unavailable or unreliable, eg. those above 2.7 kHz for ear-recorded speech.  Many proposed ASR system modifications have incorporated human auditory functions (e.g. \cite{kim:99,fazel:12,moritz:15}), and continuing to model aspects of the human auditory system in ASR processing is likely to bring ASR performance closer to the recognition accuracy of human listeners, regardless of the speech domain.  The information necessary for recognition exists in the ear-recorded speech signals, as the human auditory system has shown its ability to parse it with a relatively high degree of accuracy (cf. Chapter \ref{chapter2}).  With domain specific training, ASR recognition performance on ear-recorded speech will likely improve.
% To truly achieve human-level parity, however, ASR will need to exhibit the the same flexibility and robustness of the auditory system.


\section{Conclusions}\label{chap5:conclusions}

The passage of speech through the head and into the ear canal has been shown to have a consistent and predictable low-pass filtering effect on the signal.  Nevertheless, when recorded from inside the canal it is seen to contain meaningful speech information, such as the lower formants.  When coupled with an earplug and 30 NRR earmuffs, recording in this location also significantly reduces any background noise present in the surrounding environment.  This ear-recorded speech is able to offer significant intelligibility benefits to humans over speech recorded from the mouth, but with noise in the background.  Whether or not there are potential benefits offered by speech recorded at the ear to ASR systems is not yet certain, though there appears to be some recognition improvement over speech in very noisy backgrounds. While not yet suitable for real-life applications, much is yet to be explored regarding the possibilities and potential of an ear-recorded speech signal as a method of noise-robust human-to-human and human-to-computer communication.





