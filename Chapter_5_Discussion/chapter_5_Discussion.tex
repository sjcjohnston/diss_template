\documentclass[dissertation,copyright]{uathesis}
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R


%\documentclass[dissertation,CC-BY]{uathesis}
%\documentclass[dissertation,CC-BY-SA]{uathesis}
%documentclass[dissertation,CC-BY-ND]{uathesis}
%\documentclass[thesis]{uathesis}
%\documentclass[document]{uathesis}

% Package Usage
% These are the packages that we need
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{natbib}			% natbib is available on most systems, and is
					% terribly handy.
					
%May need to remove! Trying to fix nocite{*} biblography problem:
					% If you want to use a different Bibliography package, 
					% you should be able to, just change this
					% and the \bibliographystyle command below.  Be warned
					% that you may need to do a little hacking to get
					% the REFERENCES item to show up in your TOC.

% Compatibility with the AASTEX package 
% of the American Astronomical Society.
%\usepackage{deluxetable}		% Allows use of AASTEX deluxe tables
%\usepackage{aastex_hack}		% Allows other AASTEX functionality.

% These are other packages that you might find useful.
% For controlling the fonts, see
% http://www.math.uiuc.edu/~hartke/computer/latex/survey/survey.html
% The following is a nice font set:
%\usepackage{mathtime}			% Times for letters; Belleek math.
%
\usepackage{wrapfig}

\newenvironment{lydiawrapfigure}
 {%
%  \setlength{\intextsep}{0pt}% <--- Wrong!
  \setlength{\columnsep}{15pt}%
  \wrapfloat{figure}%
 }
 {\endwrapfloat}
 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tipa}
\usepackage{color,soul}
\usepackage{url}
\usepackage{blindtext}
\usepackage[inline]{enumitem}
\usepackage{breakurl}
\usepackage{mathtools}
\usepackage{amsmath}
% \usepackage[normalem]{ulem}
% \usepackage{xyling,comment}			% AMS Math (advanced math typesetting)
% includecomment{delall}
% %\excludecomment{delall}
% 
% %default: don't show edits
% 
% \newcommand{\add}[1]{#1}
% \newcommand{\del}[1]{}
% \newcommand{\delni}[1]{}
% \newcommand{\addtab}{}
% 
% %block to show edits controlled by include/exclude-comment above
% \begin{delall}
% %added stuff
% \renewcommand{\add}[1]{\textcolor{blue}{#1}}
% %added table
% \renewcommand{\addtab}{\color{blue}}
% %deleted stuff
% \renewcommand{\del}[1]{\textcolor{red}{\sout{#1}}}
% \renewcommand{\delni}[1]{\noindent\textcolor{red}{\sout{#1}}}
% \end{delall}

%\usepackage{lscape}			% Used for making fitting large tables in by putting them landscape
%\usepackage{refs}			
%
% If you are using hyper-ref (recommended), this command must go after all 
% other package inclusions (from the hyperref package documentation).
% The purpose of hyperref is to make the PDF created extensively
% cross-referenced.

%Also works! Change dvips to driverfallback=dvips.
\usepackage[driverfallback=dvips,bookmarks,colorlinks=true,urlcolor=black,linkcolor=black,citecolor=black]{hyperref}


%Works!
%\usepackage[pdftex,bookmarks,colorlinks=true,urlcolor=black,linkcolor=black,citecolor=black]{hyperref}
%HERE IS THE THING THAT NEEDS TO CHANGE TO GET LATEX TO WORK WITH RSTUDIO. USE pdftex instead of dvips.

% Set up some values.
\completetitle{Working Title: An approach to automatic and human speech recognition using ear-recorded speech.}
\fullname{Samuel John Charles Johnston}			% Grad college wants your full name here.
\degreename{Doctor of Philosophy}	% Title of your degree.



\begin{document}
%set_parent(â€˜/Users/mwilli/Documents/Spring_2017/Dissertation_Document/Dissertation_Working_Directory_Draft/Dissertation_Main.Rnw')



 



\chapter{Overall Discussion\label{chapter5}}


The primary goal of this project is to record a small corpus of ear-recorded speech, and test the ability of humans and ASR systems to accurately recognize the speech content.  More specifically, the project aims to determine a) if ear-recorded speech, using an earplug and ear muffs, is able to significantly filter out background noise, b) if ear-recorded speech contains enough information to be intelligible, c) if human listeners find ear-recorded speech more intelligible than the exact same signal recorded at the mouth in a noisy environment, and d) if an ASR system finds ear-recorded speech more intelligible than speech recorded at the mouth in a noisy environment.

The following three sections will briefly review the results of the three experiments discussed in this report: a) the data collection experiment which gathered the ear-recorded speech, b) the human speech perception experiment testing ear-recorded and noisy mouth-recorded speech, and c) the ASR experiement testing the ability of an ASR system to recognize ear-recorded speech compared with noisy mouth-recorded speech.


\section{Review of Ear-Recorded Speech}\label{sec:chap2-review}

Ear-recorded speech was collected and analyzed from 20 participants.  The microphone was placed into a silicone earplug and into the participants' right ear (facing the tympanic membrane), and noise reduction earmuff were placed over top of their ears.  A rod extending out from the side of the earmuffs was used to place a microphone recording the speech as it came from their mouth.  A loudspeaker was placed off to the side, which emitted background noise.  The participants read aloud stimulus sentences, and were recorded, while various kinds of background noises (bus, caf\'{e}, pedestrian area, street, factory) were produced by the loudspeaker.  For more details concerning the experimental methods, refer to Chapter 2\ref{chapter2}, Section \ref{expt1}.

These recordings were then observed.  Overall, the ear-recorded speech was highly lowpass filtered, and, subjectively, it had a very `muffled' quality, which can be visualized with the spectra and spectrograms in Figs. \ref{} and \ref{}, each from a different utterance spoken by a different participant.  To test the similarity between the mouth-recorded signals and their ear-recorded counterparts, the maximum value of the cross-correlation matrix was obtained and normalized \textbf{(ie. to obtain values between 0 and 1, where 1 indicates identical signals)}, and this value was averaged over all clean-speech utterances by all speakers.  Only utterances with no background noise were included in this calculation, in order to only test the similarity of the speech and not the noise.  This test yielded a maximum normalized cross-correlation value of 0.358 as an average across all included utterances.

To increase the amplitude of the speech information in the higher frequencies, the author pre-emphasized the signal (Fig. \ref{x}).  As there was no speech information found above approximately 2.7 kHz, and since the pre-emphasis resulted in the presence of white noise in the higher frequencies, the signal was then low-pass filtered at 2500 Hz with a slope of 500 Hz (Fig. \ref{x}).  The lower frequencies in this signal were still much more prominent than the upper frequencies, and so pre-emphasis was applied a second time (Fig. \ref{}).  The average maximum normalized cross-correlation metric was obtained using the newly transformed ear-recorded speech, and each signal was compared to its simultaneously mouth-recorded counterpart.  As before, only clean speech, with no background noise, was included in this comparison.  This gave a nearly doubled cross-correlation value of 0.674.

The spectra and spectrograms for the sentence ``He said the same phrase thirty times'', spoken by a male participant, are provided for better visualization of speech intelligibility and noise reduction.  A comparison between speech recorded at the mouth and at the ear, with transformations, and with no background noise can be seen in Figures \ref{x} and \ref{y}); the same utterance is displayed, spoken at the mouth with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{} and \ref{}) and at the ear (after transformations) with bus background noise (60, 70, and 80dB; Figs. \ref{}, \ref{}, and \ref{}).  Each mouth- and ear-recorded pair is the exact same utterance; they were recorded simultaneously (e.g. the already mentioned Figures \ref{x} at the ear and \ref{y} at the mouth).

The figures containing noisy speech recorded at the mouth (ie. Figs. \ref{}, \ref{}, and \ref{}) demonstrate one of the limitations of the collected data - that the SNR of the signal recorded by the mouth microphone was very high for all noise categories.  The noise did not drown out the speech in the high noise (80 dB) condition as originally intended.  The SNR needed to be high to compare the recognition performance of humans and computers on the ear-recorded speech, compared with the noisy mouth speech.  If the noise wasn't sufficiently high enough to interfere with the recognition of speech, then the mouth-recorded speech would be too easy to understand, and an adequate comparison between ear-recorded speech and noisy mouth-recorded speech could not be made.

Despite the high SNRs in the mouth-recorded signals, noise is still present.  When, by comparison, one looks at the ear-recorded signals that were also recorded in the noisy environment (ie. Figs. \ref{}, \ref{}, and \ref{}), the passive noise reduction that is provided by the head, earplug, and earmuffs appears to have successfully eliminated a substantial proportion of the noise from the signal.

These recordings demonstrate that using the ear canal as a location of recording speech in noise can offer a large reduction in the noise that enters the signal.  Furthermore, ear-recorded signals can provide speech comparable to mouth-recorded speech up to approximately 2.7 kHz.  This is relatively close to the 3.5 kHz cutoff generally used for telephonic communication, which does provide intelligible speech for both humans and computers. The author hypothesized that speech recorded from the ear and cut off at 2.7 kHz would also be intelligible for humans and for computers.  The following sections will review the results of the experiments in Chapters 3\ref{chapter3} and 4\ref{chapter4}, which aimed to test this hypothesis.


\section{Review of Human Perception of Ear-Recorded Speech}\label{sec:chap3-review}

\textbf{Will be included with revised statistics}


\section{Review of Automatic Speech Recognition of Ear Recorded Speech}\label{sec:chap4-review}

The author used the Kaldi ASR system (\cite{povey:10}) and the acoustic and language models trained on the data from the LibriSpeech corpus (\cite{panayatov:15}) to test the ability of a standard ASR system to recognize both ear-recorded speech and noisy mouth-recorded speech.  For testing data, the ear- and mouth-recorded speech was used which was obtained in the data collection experiment - presented in Chapter 2\ref{chapter2} and reviewed in Section \ref{sec:chap2-review}.  The ear-recorded speech was transformed with pre-emphasis, lowpass filtering, and a second application of pre-emphasis (described in Chapter 2\ref{chapter2} and in Section \ref{sec:chap2-review}). 

\begin{table}[h]
\begin{center}
\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |} \hline
      & \multicolumn{2}{|c|}{Bus} & \multicolumn{2}{|c|}{Cafe} & \multicolumn{2}{|c|}{Ped.} & \multicolumn{2}{|c|}{Street} & \multicolumn{2}{|c|}{Factory} \\ \hline
      & Mouth & Ear & Mouth & Ear & Mouth & Ear & Mouth & Ear & Mouth & Ear \\ \hline\hline
60 dB & 21.51 & ?? & 20.33 & ?? & 18.64 & ?? & 19.05 & ?? & 17.93 & ??  \\ \hline
70 dB & 41.74 & ?? & 32.93 & ?? & 32.02 & ?? & 36.49 & ?? & 29.96 & ??  \\ \hline
80 dB & 88.20 & ?? & 73.39 & ?? & 75.64 & ?? & 85.00 & ?? & 71.43 & ??  \\ \hline
\end{tabular}
\end{center}
\caption{These values are obtained the highest-performing (4-gram) language model, utilizing the 960-hour LibriSpeech DNN acoustic model - both trained using the LibriSpeech corpus.  Each row is a different noise level, and each column a different noise type (excluding the `clean' noise type).  All values are given as WER.}\label{tab:disc-all-wers}
\end{table}

\textbf{Will be included with revised analyses}

\section{Conclusions}



\section{Future Direction}


\bibliographystyle{apa}
\bibliography{DissRefs.bib}
\end{document}
